<!DOCTYPE HTML>
<html>

<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Arxiv Sanity Preserver</title>

<!-- MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML">
</script>

<!-- CSS -->
<link rel="stylesheet" type="text/css" href="/static/style.css">

<!-- Favicon -->
<link rel="shortcut icon" type="image/png" href="/static/favicon.png" />

<!-- JS -->
<script src="/static/jquery-1.8.3.min.js"></script>
<script src="/static/d3.min.js"></script>
<script src="/static/as-common.js"></script>

<!-- Google Analytics JS -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-3698471-25', 'auto');
  ga('send', 'pageview');

</script>

<script>

// passed in from flask as json
var tweets = [];
var papers = [{"abstract": "In the last several years, remote sensing technology has opened up the\npossibility of performing large scale building detection from satellite\nimagery. Our work is some of the first to create population density maps from\nbuilding detection on a large scale. The scale of our work on population\ndensity estimation via high resolution satellite images raises many issues,\nthat we will address in this paper. The first was data acquisition. Labeling\nbuildings from satellite images is a hard problem, one where we found our\nlabelers to only be about 85% accurate at. There is a tradeoff of quantity vs.\nquality of labels, so we designed two separate policies for labels meant for\ntraining sets and those meant for test sets, since our requirements of the two\nset types are quite different. We also trained weakly supervised footprint\ndetection models with the classification labels, and semi-supervised approaches\nwith a small number of pixel-level labels, which are very expensive to procure.", "authors": ["Amy Zhang", "Xianming Liu", "Andreas Gros", "Tobias Tiecke"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1707.08952v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08952v1", "num_discussion": 0, "originally_published_time": "7/27/2017", "pid": "1707.08952v1", "published_time": "7/27/2017", "rawpid": "1707.08952", "tags": ["cs.CV"], "title": "Building Detection from Satellite Images on a Global Scale"}, {"abstract": "In this paper, we present a methodology for off-line handwritten character\nrecognition. The proposed methodology relies on a new feature extraction\ntechnique based on structural characteristics, histograms and profiles. As\nnovelty, we propose the extraction of new eight histograms and four profiles\nfrom the $32\\times 32$ matrices that represent the characters, creating\n256-dimension feature vectors. These feature vectors are then employed in a\nclassification step that uses a $k$-means algorithm. We performed experiments\nusing the NIST database to evaluate our proposal. Namely, the recognition\nsystem was trained using 1000 samples and 64 classes for each symbol and was\ntested on 500 samples for each symbol. We obtain promising accuracy results\nthat vary from 81.74\\% to 93.75\\%, depending on the difficulty of the character\ncategory, showing better accuracy results than other methods from the state of\nthe art also based on structural characteristics.", "authors": ["Jos\u00e9 Manuel Casas", "Nick Inassaridze", "Manuel Ladra", "Susana Ladra"], "category": "cs.CV", "comment": "9 pages", "img": "/static/thumbs/1707.08951v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08951v1", "num_discussion": 0, "originally_published_time": "7/27/2017", "pid": "1707.08951v1", "published_time": "7/27/2017", "rawpid": "1707.08951", "tags": ["cs.CV"], "title": "Handwritten character recognition using some (anti)-diagonal structural\n  features"}, {"abstract": "Deep neural network-based classifiers are known to be vulnerable to\nadversarial examples that can fool them into misclassifying their input through\nthe addition of small-magnitude perturbations. However, recent studies have\ndemonstrated that such adversarial examples are not very effective in the\nphysical world--they either completely fail to cause misclassification or only\nwork in restricted cases where a relatively complex image is perturbed and\nprinted on paper. In this paper we propose a new attack algorithm--Robust\nPhysical Perturbations (RP2)-- that generates perturbations by taking images\nunder different conditions into account. Our algorithm can create\nspatially-constrained perturbations that mimic vandalism or art to reduce the\nlikelihood of detection by a casual observer. We show that adversarial examples\ngenerated by RP2 achieve high success rates under various conditions for real\nroad sign recognition by using an evaluation methodology that captures physical\nworld conditions. We physically realized and evaluated two attacks, one that\ncauses a Stop sign to be misclassified as a Speed Limit sign in 100% of the\ntesting conditions, and one that causes a Right Turn sign to be misclassified\nas either a Stop or Added Lane sign in 100% of the testing conditions.", "authors": ["Ivan Evtimov", "Kevin Eykholt", "Earlence Fernandes", "Tadayoshi Kohno", "Bo Li", "Atul Prakash", "Amir Rahmati", "Dawn Song"], "category": "cs.CR", "comment": "", "img": "/static/thumbs/1707.08945v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08945v1", "num_discussion": 0, "originally_published_time": "7/27/2017", "pid": "1707.08945v1", "published_time": "7/27/2017", "rawpid": "1707.08945", "tags": ["cs.CR", "cs.LG"], "title": "Robust Physical-World Attacks on Machine Learning Models"}, {"abstract": "Compared with raw images, the more common JPEG images are less useful for\nmachine vision algorithms and professional photographers because JPEG-sRGB does\nnot preserve a linear relation between pixel values and the light measured from\nthe scene. A camera is said to be radiometrically calibrated if there is a\ncomputational model which can predict how the raw linear sensor image is mapped\nto the corresponding rendered image (e.g. JPEGs) and vice versa. This paper\nbegins with the observation that the rank order of pixel values are mostly\npreserved post colour correction. We show that this observation is the key to\nsolving for the whole camera pipeline (colour correction, tone and gamut\nmapping). Our rank-based calibration method is simpler than the prior art and\nso is parametrised by fewer variables which, concomitantly, can be solved for\nusing less calibration data. Another advantage is that we can derive the camera\npipeline from a single pair of raw-JPEG images. Experiments demonstrate that\nour method delivers state-of-the-art results (especially for the most\ninteresting case of JPEG to raw).", "authors": ["Han Gong", "Graham D. Finlayson", "Maryam M. Darrodi"], "category": "cs.CV", "comment": "accepted by BMVC 2017", "img": "/static/thumbs/1707.08943v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08943v1", "num_discussion": 0, "originally_published_time": "7/27/2017", "pid": "1707.08943v1", "published_time": "7/27/2017", "rawpid": "1707.08943", "tags": ["cs.CV"], "title": "Concise Radiometric Calibration Using The Power of Ranking"}, {"abstract": "The field of connectomics has recently produced neuron wiring diagrams from\nrelatively large brain regions from multiple animals. Most of these neural\nreconstructions were computed from isotropic (e.g., FIBSEM) or near isotropic\n(e.g., SBEM) data. In spite of the remarkable progress on algorithms in recent\nyears, automatic dense reconstruction from anisotropic data remains a challenge\nfor the connectomics community. One significant hurdle in the segmentation of\nanisotropic data is the difficulty in generating a suitable initial\nover-segmentation. In this study, we present a segmentation method for\nanisotropic EM data that agglomerates a 3D over-segmentation computed from the\n3D affinity prediction. A 3D U-net is trained to predict 3D affinities by the\nMALIS approach. Experiments on multiple datasets demonstrates the strength and\nrobustness of the proposed method for anisotropic EM segmentation.", "authors": ["Toufiq Parag", "Fabian Tschopp", "William Grisaitis", "Srinivas C Turaga", "Xuewen Zhang", "Brian Matejek", "Lee Kamentsky", "Jeff W. Lichtman", "Hanspeter Pfister"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1707.08935v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08935v1", "num_discussion": 0, "originally_published_time": "7/27/2017", "pid": "1707.08935v1", "published_time": "7/27/2017", "rawpid": "1707.08935", "tags": ["cs.CV"], "title": "Anisotropic EM Segmentation by 3D Affinity Learning and Agglomeration"}, {"abstract": "We give the motivation for scoring clustering algorithms and a metric $M : A\n\\rightarrow \\mathbb{N}$ from the set of clustering algorithms to the natural\nnumbers which we realize as \\begin{equation} M(A) = \\sum_i \\alpha_i |f_i -\n\\beta_i|^{w_i} \\end{equation} where $\\alpha_i,\\beta_i,w_i$ are parameters used\nfor scoring the feature $f_i$, which is computed empirically.. We give a method\nby which one can score features such as stability, noise sensitivity, etc and\nderive the necessary parameters. We conclude by giving a sample set of scores.", "authors": ["Clark Alexander", "Sofya Akhmametyeva"], "category": "cs.DM", "comment": "15 pages", "img": "/static/thumbs/1707.08912v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08912v1", "num_discussion": 0, "originally_published_time": "7/27/2017", "pid": "1707.08912v1", "published_time": "7/27/2017", "rawpid": "1707.08912", "tags": ["cs.DM", "cs.AI", "cs.CG"], "title": "A Family of Metrics for Clustering Algorithms"}, {"abstract": "Parsing sentences to linguistically-expressive semantic representations is a\nkey goal of Natural Language Processing. Yet statistical parsing has focused\nalmost exclusively on bilexical dependencies or domain-specific logical forms.\nWe propose a neural encoder-decoder transition-based parser which is the first\nfull-coverage semantic graph parser for Minimal Recursion Semantics (MRS). The\nmodel architecture uses stack-based embedding features, predicting graphs\njointly with unlexicalized predicates and their token alignments. Our parser is\nmore accurate than attention-based baselines on MRS, and on an additional\nAbstract Meaning Representation (AMR) benchmark, and GPU batch processing makes\nit an order of magnitude faster than a high-precision grammar-based parser.\nFurther, the 86.69% Smatch score of our MRS parser is higher than the\nupper-bound on AMR parsing, making MRS an attractive choice as a semantic\nrepresentation.", "authors": ["Jan Buys", "Phil Blunsom"], "category": "cs.CL", "comment": "12 pages; ACL 2017", "img": "/static/thumbs/1704.07092v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.07092v2", "num_discussion": 0, "originally_published_time": "4/24/2017", "pid": "1704.07092v2", "published_time": "7/27/2017", "rawpid": "1704.07092", "tags": ["cs.CL"], "title": "Robust Incremental Neural Semantic Graph Parsing"}, {"abstract": "The Particle Swarm Optimization Policy (PSO-P) has been recently introduced\nand proven to produce remarkable results on interacting with academic\nreinforcement learning benchmarks in an off-policy, batch-based setting. To\nfurther investigate the properties and feasibility on real-world applications,\nthis paper investigates PSO-P on the so-called Industrial Benchmark (IB), a\nnovel reinforcement learning (RL) benchmark that aims at being realistic by\nincluding a variety of aspects found in industrial applications, like\ncontinuous state and action spaces, a high dimensional, partially observable\nstate space, delayed effects, and complex stochasticity. The experimental\nresults of PSO-P on IB are compared to results of closed-form control policies\nderived from the model-based Recurrent Control Neural Network (RCNN) and the\nmodel-free Neural Fitted Q-Iteration (NFQ). Experiments show that PSO-P is not\nonly of interest for academic benchmarks, but also for real-world industrial\napplications, since it also yielded the best performing policy in our IB\nsetting. Compared to other well established RL techniques, PSO-P produced\noutstanding results in performance and robustness, requiring only a relatively\nlow amount of effort in finding adequate parameters or making complex design\ndecisions.", "authors": ["Daniel Hein", "Steffen Udluft", "Michel Tokic", "Alexander Hentschel", "Thomas A. Runkler", "Volkmar Sterzing"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1705.07262v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1705.07262v2", "num_discussion": 0, "originally_published_time": "5/20/2017", "pid": "1705.07262v2", "published_time": "7/27/2017", "rawpid": "1705.07262", "tags": ["cs.LG", "cs.AI", "cs.NE", "cs.SY"], "title": "Batch Reinforcement Learning on the Industrial Benchmark: First\n  Experiences"}, {"abstract": "Sparsity learning with known grouping structures has received considerable\nattention due to wide modern applications in high-dimensional data analysis.\nAlthough advantages of using group information have been well-studied by\nshrinkage-based approaches, benefits of group sparsity have not been\nwell-documented for greedy-type methods, which much limits our understanding\nand use of this important class of methods. In this paper, generalizing from a\npopular forward-backward greedy approach, we propose a new interactive greedy\nalgorithm for group sparsity learning and prove that the proposed greedy-type\nalgorithm attains the desired benefits of group sparsity under high dimensional\nsettings. An estimation error bound refining other existing methods and a\nguarantee for group support recovery are also established simultaneously. In\naddition, an interactive feature is incorporated to allow extra algorithm\nflexibility without compromise in theoretical properties. The promising use of\nour proposal is demonstrated through numerical evaluations including a real\nindustrial application in human activity recognition.", "authors": ["Wei Qian", "Wending Li", "Yasuhiro Sogawa", "Ryohei Fujimaki", "Xitong Yang", "Ji Liu"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1707.02963v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.02963v2", "num_discussion": 0, "originally_published_time": "7/10/2017", "pid": "1707.02963v2", "published_time": "7/27/2017", "rawpid": "1707.02963", "tags": ["stat.ML"], "title": "An Interactive Greedy Approach to Group Sparsity in High Dimension"}, {"abstract": "We propose a new type of self-aware systems inspired by ideas from\nhigher-order theories of consciousness. First, we discussed the crucial\ndistinction between introspection and reflexion. Then, we focus on\ncomputational reflexion as a mechanism by which a computer program can inspect\nits own code at every stage of the computation. Finally, we provide a formal\ndefinition and a proof-of-concept implementation of computational reflexion,\nviewed as an enriched form of program interpretation and a way to dynamically\n\"augment\" a computational process.", "authors": ["Alessandro Valitutti", "Giuseppe Trautteur"], "category": "cs.AI", "comment": "12 pages plus bibliography, appendices with code description, code of\n  the proof-of-concept impleme...", "img": "/static/thumbs/1707.08901v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08901v1", "num_discussion": 0, "originally_published_time": "7/27/2017", "pid": "1707.08901v1", "published_time": "7/27/2017", "rawpid": "1707.08901", "tags": ["cs.AI", "F.1.1; F.3.3; I.2.0; I.2.2"], "title": "Providing Self-Aware Systems with Reflexivity"}, {"abstract": "Lifted inference algorithms commonly exploit symmetries in a probabilistic\ngraphical model (PGM) for efficient inference. However, existing algorithms for\nBoolean-valued domains can identify only those pairs of states as symmetric, in\nwhich the number of ones and zeros match exactly (count symmetries). Moreover,\nalgorithms for lifted inference in multi-valued domains also compute a\nmulti-valued extension of count symmetries only. These algorithms miss many\nsymmetries in a domain. In this paper, we present first algorithms to compute\nnon-count symmetries in both Boolean-valued and multi-valued domains. Our\nmethods can also find symmetries between multi-valued variables that have\ndifferent domain cardinalities. The key insight in the algorithms is that they\nchange the unit of symmetry computation from a variable to a variable-value\n(VV) pair. Our experiments find that exploiting these symmetries in MCMC can\nobtain substantial computational gains over existing algorithms.", "authors": ["Ankit Anand", "Ritesh Noothigattu", "Parag Singla", "Mausam"], "category": "cs.AI", "comment": "9 pages, 5 figures", "img": "/static/thumbs/1707.08879v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08879v1", "num_discussion": 0, "originally_published_time": "7/27/2017", "pid": "1707.08879v1", "published_time": "7/27/2017", "rawpid": "1707.08879", "tags": ["cs.AI"], "title": "Non-Count Symmetries in Boolean \u0026 Multi-Valued Prob. Graphical Models"}, {"abstract": "Deep residual learning (ResNet) is a new method for training very deep neural\nnetworks using identity map-ping for shortcut connections. ResNet has won the\nImageNet ILSVRC 2015 classification task, and achieved state-of-the-art\nperformances in many computer vision tasks. However, the effect of residual\nlearning on noisy natural language processing tasks is still not well\nunderstood. In this paper, we design a novel convolutional neural network (CNN)\nwith residual learning, and investigate its impacts on the task of distantly\nsupervised noisy relation extraction. In contradictory to popular beliefs that\nResNet only works well for very deep networks, we found that even with 9 layers\nof CNNs, using identity mapping could significantly improve the performance for\ndistantly-supervised relation extraction.", "authors": ["Yi Yao Huang", "William Yang Wang"], "category": "cs.CL", "comment": "Accepted by EMNLP 2017", "img": "/static/thumbs/1707.08866v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08866v1", "num_discussion": 0, "originally_published_time": "7/27/2017", "pid": "1707.08866v1", "published_time": "7/27/2017", "rawpid": "1707.08866", "tags": ["cs.CL", "cs.AI"], "title": "Deep Residual Learning for Weakly-Supervised Relation Extraction"}, {"abstract": "Sensor setups consisting of a combination of 3D range scanner lasers and\nstereo vision systems are becoming a popular choice for on-board perception\nsystems in vehicles; however, the combined use of both sources of information\nimplies a tedious calibration process. We present a method for extrinsic\ncalibration of lidar-stereo camera pairs without user intervention. Our\ncalibration approach is aimed to cope with the constraints commonly found in\nautomotive setups, such as low-resolution and specific sensor poses. To\ndemonstrate the performance of our method, we also introduce a novel approach\nfor the quantitative assessment of the calibration results, based on a\nsimulation environment. Tests using real devices have been conducted as well,\nproving the usability of the system and the improvement over the existing\napproaches. Code is available at http://wiki.ros.org/velo2cam_calibration", "authors": ["Carlos Guindel", "Jorge Beltr\u00e1n", "David Mart\u00edn", "Fernando Garc\u00eda"], "category": "cs.CV", "comment": "Accepted to IEEE International Conference on Intelligent\n  Transportation Systems 2017 (ITSC)", "img": "/static/thumbs/1705.04085v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1705.04085v3", "num_discussion": 0, "originally_published_time": "5/11/2017", "pid": "1705.04085v3", "published_time": "7/27/2017", "rawpid": "1705.04085", "tags": ["cs.CV", "cs.RO", "68T45", "I.4.8; I.2.9; I.4.1"], "title": "Automatic Extrinsic Calibration for Lidar-Stereo Vehicle Sensor Setups"}, {"abstract": "We consider the problem of face swapping in images, where an input identity\nis transformed into a target identity while preserving pose, facial expression,\nand lighting. To perform this mapping, we use convolutional neural networks\ntrained to capture the appearance of the target identity from an unstructured\ncollection of his/her photographs.This approach is enabled by framing the face\nswapping problem in terms of style transfer, where the goal is to render an\nimage in the style of another one. Building on recent advances in this area, we\ndevise a new loss function that enables the network to produce highly\nphotorealistic results. By combining neural networks with simple pre- and\npost-processing steps, we aim at making face swap work in real-time with no\ninput from the user.", "authors": ["Iryna Korshunova", "Wenzhe Shi", "Joni Dambre", "Lucas Theis"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1611.09577v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1611.09577v2", "num_discussion": 0, "originally_published_time": "11/29/2016", "pid": "1611.09577v2", "published_time": "7/27/2017", "rawpid": "1611.09577", "tags": ["cs.CV"], "title": "Fast Face-swap Using Convolutional Neural Networks"}, {"abstract": "Explaining underlying causes or effects about events is a challenging but\nvaluable task. We define a novel problem of generating explanations of a time\nseries event by (1) searching cause and effect relationships of the time series\nwith textual data and (2) constructing a connecting chain between them to\ngenerate an explanation. To detect causal features from text, we propose a\nnovel method based on the Granger causality of time series between features\nextracted from text such as N-grams, topics, sentiments, and their composition.\nThe generation of the sequence of causal entities requires a commonsense\ncausative knowledge base with efficient reasoning. To ensure good\ninterpretability and appropriate lexical usage we combine symbolic and neural\nrepresentations, using a neural reasoning algorithm trained on commonsense\ncausal tuples to predict the next cause step. Our quantitative and human\nanalysis show empirical evidence that our method successfully extracts\nmeaningful causality relationships between time series with textual features\nand generates appropriate explanation between them.", "authors": ["Dongyeop Kang", "Varun Gangal", "Ang Lu", "Zheng Chen", "Eduard Hovy"], "category": "cs.CL", "comment": "Accepted at EMNLP 2017", "img": "/static/thumbs/1707.08852v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08852v1", "num_discussion": 0, "originally_published_time": "7/27/2017", "pid": "1707.08852v1", "published_time": "7/27/2017", "rawpid": "1707.08852", "tags": ["cs.CL"], "title": "Detecting and Explaining Causes From Text For a Time Series Event"}, {"abstract": "We adopt the perspective of an aggregator, which seeks to coordinate its\npurchase of demand reductions from a fixed group of residential electricity\ncustomers, with its sale of the aggregate demand reduction in a two-settlement\nwholesale energy market. The aggregator procures reductions in demand by\noffering its customers a uniform price for reductions in consumption relative\nto their predetermined baselines. Prior to its realization of the aggregate\ndemand reduction, the aggregator must also determine how much energy to sell\ninto the two-settlement energy market. In the day-ahead market, the aggregator\ncommits to a forward contract, which calls for the delivery of energy in the\nreal-time market. The underlying aggregate demand curve, which relates the\naggregate demand reduction to the aggregator\u0027s offered price, is assumed to be\naffine and subject to unobservable, random shocks. Assuming that both the\nparameters of the demand curve and the distribution of the random shocks are\ninitially unknown to the aggregator, we investigate the extent to which the\naggregator might dynamically adapt its offered prices and forward contracts to\nmaximize its expected profit over a time window of $T$ days. Specifically, we\ndesign a dynamic pricing and contract offering policy that resolves the\naggregator\u0027s need to learn the unknown demand model with its desire to maximize\nits cumulative expected profit over time. The proposed pricing policy is proven\nto be asymptotically optimal --- exhibiting a regret over $T$ days that is no\ngreater than $O(\\sqrt{T})$.", "authors": ["Kia Khezeli", "Eilyan Bitar"], "category": "cs.SY", "comment": "", "img": "/static/thumbs/1707.07342v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.07342v2", "num_discussion": 0, "originally_published_time": "7/23/2017", "pid": "1707.07342v2", "published_time": "7/27/2017", "rawpid": "1707.07342", "tags": ["cs.SY", "cs.LG"], "title": "An Online Learning Approach to Buying and Selling Demand Response"}, {"abstract": "The state-of-the-art solutions to the vocabulary mismatch in information\nretrieval (IR) mainly aim at leveraging either the relational semantics\nprovided by external resources or the distributional semantics, recently\ninvestigated by deep neural approaches. Guided by the intuition that the\nrelational semantics might improve the effectiveness of deep neural approaches,\nwe propose the Deep Semantic Resource Inference Model (DSRIM) that relies on:\n1) a representation of raw-data that models the relational semantics of text by\njointly considering objects and relations expressed in a knowledge resource,\nand 2) an end-to-end neural architecture that learns the query-document\nrelevance by leveraging the distributional and relational semantics of\ndocuments and queries. The experimental evaluation carried out on two TREC\ndatasets from TREC Terabyte and TREC CDS tracks relying respectively on WordNet\nand MeSH resources, indicates that our model outperforms state-of-the-art\nsemantic and deep neural IR models.", "authors": ["Gia-Hung Nguyen", "Laure Soulier", "Lynda Tamine", "Nathalie Bricon-Souf"], "category": "cs.IR", "comment": "", "img": "/static/thumbs/1706.04922v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1706.04922v2", "num_discussion": 0, "originally_published_time": "6/15/2017", "pid": "1706.04922v2", "published_time": "7/27/2017", "rawpid": "1706.04922", "tags": ["cs.IR", "cs.CL"], "title": "DSRIM: A Deep Neural Information Retrieval Model Enhanced by a Knowledge\n  Resource Driven Representation of Documents"}, {"abstract": "Detecting and recognizing text in natural scene images is a challenging, yet\nnot completely solved task. In re- cent years several new systems that try to\nsolve at least one of the two sub-tasks (text detection and text recognition)\nhave been proposed. In this paper we present STN-OCR, a step towards\nsemi-supervised neural networks for scene text recognition, that can be\noptimized end-to-end. In contrast to most existing works that consist of\nmultiple deep neural networks and several pre-processing steps we propose to\nuse a single deep neural network that learns to detect and recognize text from\nnatural images in a semi-supervised way. STN-OCR is a network that integrates\nand jointly learns a spatial transformer network, that can learn to detect text\nregions in an image, and a text recognition network that takes the identified\ntext regions and recognizes their textual content. We investigate how our model\nbehaves on a range of different tasks (detection and recognition of characters,\nand lines of text). Experimental results on public benchmark datasets show the\nability of our model to handle a variety of different tasks, without\nsubstantial changes in its overall network structure.", "authors": ["Christian Bartz", "Haojin Yang", "Christoph Meinel"], "category": "cs.CV", "comment": "9 pages, 6 figures", "img": "/static/thumbs/1707.08831v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08831v1", "num_discussion": 0, "originally_published_time": "7/27/2017", "pid": "1707.08831v1", "published_time": "7/27/2017", "rawpid": "1707.08831", "tags": ["cs.CV"], "title": "STN-OCR: A single Neural Network for Text Detection and Text Recognition"}, {"abstract": "Mild cognitive impairment is the early stage of several neurodegenerative\ndiseases, such as Alzheimer\u0027s. In this work, we address the use of lifelogging\nas a tool to obtain pictures from a patient\u0027s daily life from an egocentric\npoint of view. We propose to use them in combination with serious games as a\nway to provide a non-pharmacological treatment to improve their quality of\nlife. To do so, we introduce a novel computer vision technique that classifies\nrich and non rich egocentric images and uses them in serious games. We present\nresults over a dataset composed by 10,997 images, recorded by 7 different\nusers, achieving 79% of F1-score. Our model presents the first method used for\nautomatic egocentric images selection applicable to serious games.", "authors": ["Gabriel Oliveira-Barra", "Marc Bola\u00f1os", "Estefania Talavera", "Adri\u00e1n Due\u00f1as", "Olga Gelonch", "Maite Garolera"], "category": "cs.CV", "comment": "11 pages", "img": "/static/thumbs/1707.08821v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08821v1", "num_discussion": 0, "originally_published_time": "7/27/2017", "pid": "1707.08821v1", "published_time": "7/27/2017", "rawpid": "1707.08821", "tags": ["cs.CV"], "title": "Serious Games Application for Memory Training Using Egocentric Images"}, {"abstract": "This paper is devoted to the study of the max K-armed bandit problem, which\nconsists in sequentially allocating resources in order to detect extreme\nvalues. Our contribution is twofold. We first significantly refine the analysis\nof the ExtremeHunter algorithm carried out in Carpentier and Valko (2014), and\nnext propose an alternative approach, showing that, remarkably, Extreme Bandits\ncan be reduced to a classical version of the bandit problem to a certain\nextent. Beyond the formal analysis, these two approaches are compared through\nnumerical experiments.", "authors": ["Mastane Achab", "Stephan Cl\u00e9men\u00e7on", "Aur\u00e9lien Garivier", "Anne Sabourin", "Claire Vernade"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1707.08820v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08820v1", "num_discussion": 0, "originally_published_time": "7/27/2017", "pid": "1707.08820v1", "published_time": "7/27/2017", "rawpid": "1707.08820", "tags": ["stat.ML", "cs.LG"], "title": "Max K-armed bandit: On the ExtremeHunter algorithm and beyond"}, {"abstract": "The original ImageNet dataset is a popular large-scale benchmark for training\nDeep Neural Networks. Since the cost of performing experiments (e.g, algorithm\ndesign, architecture search, and hyperparameter tuning) on the original dataset\nmight be prohibitive, we propose to consider a downsampled version of ImageNet.\nIn contrast to the CIFAR datasets and earlier downsampled versions of ImageNet,\nour proposed ImageNet32 (and its variants ImageNet64 and ImageNet16) contains\nexactly the same number of classes and images as ImageNet, with the only\ndifference that the images are downsampled to 32$\\times$32 pixels per image\n(64$\\times$64 and 16$\\times$16 pixels for the variants, respectively).\nExperiments on these downsampled variants are dramatically faster than on the\noriginal ImageNet and the characteristics of the downsampled datasets with\nrespect to optimal hyperparameters appear to remain similar. The proposed\ndatasets and scripts to reproduce our results are available at\nhttps://image-net.org/download-images and\nhttps://github.com/PatrykChrabaszcz/Imagenet32_Scripts", "authors": ["Patryk Chrabaszcz", "Ilya Loshchilov", "Frank Hutter"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1707.08819v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08819v1", "num_discussion": 0, "originally_published_time": "7/27/2017", "pid": "1707.08819v1", "published_time": "7/27/2017", "rawpid": "1707.08819", "tags": ["cs.CV", "cs.LG"], "title": "A Downsampled Variant of ImageNet as an Alternative to the CIFAR\n  datasets"}, {"abstract": "We propose a general and model-free approach for Reinforcement Learning (RL)\non real robotics with sparse rewards. We build upon the Deep Deterministic\nPolicy Gradient (DDPG) algorithm to use demonstrations. Both demonstrations and\nactual interactions are used to fill a replay buffer and the sampling ratio\nbetween demonstrations and transitions is automatically tuned via a prioritized\nreplay mechanism. Typically, carefully engineered shaping rewards are required\nto enable the agents to efficiently explore on high dimensional control\nproblems such as robotics. They are also required for model-based acceleration\nmethods relying on local solvers such as iLQG (e.g. Guided Policy Search and\nNormalized Advantage Function). The demonstrations replace the need for\ncarefully engineered rewards, and reduce the exploration problem encountered by\nclassical RL approaches in these domains. Demonstrations are collected by a\nrobot kinesthetically force-controlled by a human demonstrator. Results on four\nsimulated insertion tasks show that DDPG from demonstrations out-performs DDPG,\nand does not require engineered rewards. Finally, we demonstrate the method on\na real robotics task consisting of inserting a clip (flexible object) into a\nrigid object.", "authors": ["Matej Ve\u010der\u00edk", "Todd Hester", "Jonathan Scholz", "Fumin Wang", "Olivier Pietquin", "Bilal Piot", "Nicolas Heess", "Thomas Roth\u00f6rl", "Thomas Lampe", "Martin Riedmiller"], "category": "cs.AI", "comment": "", "img": "/static/thumbs/1707.08817v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08817v1", "num_discussion": 0, "originally_published_time": "7/27/2017", "pid": "1707.08817v1", "published_time": "7/27/2017", "rawpid": "1707.08817", "tags": ["cs.AI"], "title": "Leveraging Demonstrations for Deep Reinforcement Learning on Robotics\n  Problems with Sparse Rewards"}, {"abstract": "Automatically constructing a food diary that tracks the ingredients consumed\ncan help people follow a healthy diet. We tackle the problem of food\ningredients recognition as a multi-label learning problem. We propose a method\nfor adapting a highly performing state of the art CNN in order to act as a\nmulti-label predictor for learning recipes in terms of their list of\ningredients. We prove that our model is able to, given a picture, predict its\nlist of ingredients, even if the recipe corresponding to the picture has never\nbeen seen by the model. We make public two new datasets suitable for this\npurpose. Furthermore, we prove that a model trained with a high variability of\nrecipes and ingredients is able to generalize better on new data, and visualize\nhow it specializes each of its neurons to different ingredients.", "authors": ["Marc Bola\u00f1os", "Aina Ferr\u00e0", "Petia Radeva"], "category": "cs.CV", "comment": "8 pages", "img": "/static/thumbs/1707.08816v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08816v1", "num_discussion": 0, "originally_published_time": "7/27/2017", "pid": "1707.08816v1", "published_time": "7/27/2017", "rawpid": "1707.08816", "tags": ["cs.CV"], "title": "Food Ingredients Recognition through Multi-label Learning"}, {"abstract": "Convolutional Neural Network (CNN) models have become the state-of-the-art\nfor most computer vision tasks with natural images. However, these are not best\nsuited for multi-gigapixel resolution Whole Slide Images (WSIs) of histology\nslides due to large size of these images. Current approaches construct smaller\npatches from WSIs which results in the loss of contextual information. We\npropose to capture the spatial context using novel Representation-Aggregation\nNetwork (RAN) for segmentation purposes, wherein the first network learns\npatch-level representation and the second network aggregates context from a\ngrid of neighbouring patches. We can use any CNN for representation learning,\nand can utilize CNN or 2D-Long Short Term Memory (2D-LSTM) for\ncontext-aggregation. Our method significantly outperformed conventional\npatch-based CNN approaches on segmentation of tumour in WSIs of breast cancer\ntissue sections.", "authors": ["Abhinav Agarwalla", "Muhammad Shaban", "Nasir M. Rajpoot"], "category": "cs.CV", "comment": "Published in Workshop on Deep Learning in Irregular Domains (DLID) in\n  BMVC2017", "img": "/static/thumbs/1707.08814v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08814v1", "num_discussion": 0, "originally_published_time": "7/27/2017", "pid": "1707.08814v1", "published_time": "7/27/2017", "rawpid": "1707.08814", "tags": ["cs.CV"], "title": "Representation-Aggregation Networks for Segmentation of Multi-Gigapixel\n  Histology Images"}, {"abstract": "The analysis of human motion as a clinical tool can bring many benefits such\nas the early detection of disease and the monitoring of recovery, so in turn\nhelping people to lead independent lives. However, it is currently under used.\nDevelopments in depth cameras, such as Kinect, have opened up the use of motion\nanalysis in settings such as GP surgeries, care homes and private homes. To\nprovide an insight into the use of Kinect in the healthcare domain, we present\na review of the current state of the art. We then propose a method that can\nrepresent human motions from time-series data of arbitrary length, as a single\nvector. Finally, we demonstrate the utility of this method by extracting a set\nof clinically significant features and using them to detect the age related\nchanges in the motions of a set of 54 individuals, with a high degree of\ncertainty (F1- score between 0.9 - 1.0). Indicating its potential application\nin the detection of a range of age-related motion impairments.", "authors": ["Sean Maudsley-Barton", "Jamie McPheey", "Anthony Bukowski", "Daniel Leightleyz", "Moi Hoon Yap"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1707.08813v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08813v1", "num_discussion": 0, "originally_published_time": "7/27/2017", "pid": "1707.08813v1", "published_time": "7/27/2017", "rawpid": "1707.08813", "tags": ["cs.CV"], "title": "A Comparative Study of the Clinical use of Motion Analysis from Kinect\n  Skeleton Data"}, {"abstract": "We introduce \\texttt{pycobra}, a Python library devoted to ensemble learning\n(regression and classification) and visualisation. Its main assets are the\nimplementation of several ensemble learning algorithms, a flexible and generic\ninterface to compare and blend any existing machine learning algorithm\navailable in Python libraries (as long as a \\texttt{predict} method is given),\nand visualisation tools such as Voronoi tessellations. \\texttt{pycobra} is\nfully \\texttt{scikit-learn} compatible and is released under the MIT\nopen-source license. \\texttt{pycobra} can be downloaded from the Python Package\nIndex (PyPi) and Machine Learning Open Source Software (MLOSS). The current\nversion (along with Jupyter notebooks, extensive documentation, and continuous\nintegration tests) is available at\n\\href{https://github.com/bhargavvader/pycobra}{https://github.com/bhargavvader/pycobra}.", "authors": ["Benjamin Guedj", "Bhargav Srinivasa Desikan"], "category": "stat.CO", "comment": "", "img": "/static/thumbs/1707.00558v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.00558v2", "num_discussion": 0, "originally_published_time": "4/25/2017", "pid": "1707.00558v2", "published_time": "7/27/2017", "rawpid": "1707.00558", "tags": ["stat.CO", "stat.ML"], "title": "Pycobra: A Python Toolbox for Ensemble Learning and Visualisation"}, {"abstract": "We study the problem of estimating the relative depth order of point pairs in\na monocular image. Recent advances mainly focus on using deep convolutional\nneural networks (DCNNs) to learn and infer the ordinal information from\nmultiple contextual information of the points pair such as global scene\ncontext, local contextual information, and the locations. However, it remains\nunclear how much each context contributes to the task. To address this, we\nfirst examine the contribution of each context cue [1], [2] to the performance\nin the context of depth order estimation. We find out the local context\nsurrounding the points pair contributes the most and the global scene context\nhelps little. Based on the findings, we propose a simple method, using a\nmulti-scale densely-connected network to tackle the task. Instead of learning\nthe global structure, we dedicate to explore the local structure by learning to\nregress from regions of multiple sizes around the point pairs. Moreover, we use\nthe recent densely connected network [3] to encourage substantial feature reuse\nas well as deepen our network to boost the performance. We show in experiments\nthat the results of our approach is on par with or better than the\nstate-of-the-art methods with the benefit of using only a small number of\ntraining data.", "authors": ["Ruoxi Deng", "Tianqi Zhao", "Chunhua Shen", "Shengjun Liu"], "category": "cs.CV", "comment": "12 pages", "img": "/static/thumbs/1707.08063v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08063v2", "num_discussion": 0, "originally_published_time": "7/25/2017", "pid": "1707.08063v2", "published_time": "7/27/2017", "rawpid": "1707.08063", "tags": ["cs.CV"], "title": "Relative Depth Order Estimation Using Multi-scale Densely Connected\n  Convolutional Networks"}, {"abstract": "Classifiers trained on given databases perform poorly when tested on data\nacquired in different settings. This is explained in domain adaptation through\na shift among distributions of the source and target domains. Attempts to align\nthem have traditionally resulted in works reducing the domain shift by\nintroducing appropriate loss terms, measuring the discrepancies between source\nand target distributions, in the objective function. Here we take a different\nroute, proposing to align the learned representations by embedding in any given\nnetwork specific Domain Alignment Layers, designed to match the source and\ntarget feature distributions to a reference one. Opposite to previous works\nwhich define a priori in which layers adaptation should be performed, our\nmethod is able to automatically learn the degree of feature alignment required\nat different levels of the deep network. Thorough experiments on different\npublic benchmarks, in the unsupervised setting, confirm the power of our\napproach.", "authors": ["Fabio Maria Carlucci", "Lorenzo Porzi", "Barbara Caputo", "Elisa Ricci", "Samuel Rota Bul\u00f2"], "category": "cs.CV", "comment": "arXiv admin note: substantial text overlap with arXiv:1702.06332", "img": "/static/thumbs/1704.08082v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.08082v2", "num_discussion": 0, "originally_published_time": "4/26/2017", "pid": "1704.08082v2", "published_time": "7/27/2017", "rawpid": "1704.08082", "tags": ["cs.CV"], "title": "AutoDIAL: Automatic DomaIn Alignment Layers"}, {"abstract": "This paper presents an augmentation of MSCOCO dataset where speech is added\nto image and text. Speech captions are generated using text-to-speech (TTS)\nsynthesis resulting in 616,767 spoken captions (more than 600h) paired with\nimages. Disfluencies and speed perturbation are added to the signal in order to\nsound more natural. Each speech signal (WAV) is paired with a JSON file\ncontaining exact timecode for each word/syllable/phoneme in the spoken caption.\nSuch a corpus could be used for Language and Vision (LaVi) tasks including\nspeech input or output instead of text. Investigating multimodal learning\nschemes for unsupervised speech pattern discovery is also possible with this\ncorpus, as demonstrated by a preliminary study conducted on a subset of the\ncorpus (10h, 10k spoken captions).", "authors": ["William Havard", "Laurent Besacier", "Olivier Rosec"], "category": "cs.CL", "comment": "corpus available at http://mscoco.org/external/ Accepted to GLU\n  Satellite Workshop of Interspeech ...", "img": "/static/thumbs/1707.08435v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08435v2", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08435v2", "published_time": "7/27/2017", "rawpid": "1707.08435", "tags": ["cs.CL"], "title": "SPEECH-COCO: 600k Visually Grounded Spoken Captions Aligned to MSCOCO\n  Data Set"}, {"abstract": "This work offers a design of a video surveillance system based on a soft\nbiometric -- gait identification from MoCap data. The main focus is on two\nsubstantial issues of the video surveillance scenario: (1) the walkers do not\ncooperate in providing learning data to establish their identities and (2) the\ndata are often noisy or incomplete. We show that only a few examples of human\ngait cycles are required to learn a projection of raw MoCap data onto a\nlow-dimensional sub-space where the identities are well separable. Latent\nfeatures learned by Maximum Margin Criterion (MMC) method discriminate better\nthan any collection of geometric features. The MMC method is also highly robust\nto noisy data and works properly even with only a fraction of joints tracked.\nThe overall workflow of the design is directly applicable for a day-to-day\noperation based on the available MoCap technology and algorithms for gait\nanalysis. In the concept we introduce, a walker\u0027s identity is represented by a\ncluster of gait data collected at their incidents within the surveillance\nsystem: They are how they walk.", "authors": ["Michal Balazia", "Petr Sojka"], "category": "cs.CV", "comment": "Preprint. Full paper accepted at the IEEE/IAPR International Joint\n  Conference on Biometrics (IJCB)...", "img": "/static/thumbs/1706.09443v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1706.09443v2", "num_discussion": 0, "originally_published_time": "6/28/2017", "pid": "1706.09443v2", "published_time": "7/27/2017", "rawpid": "1706.09443", "tags": ["cs.CV", "68T05, 68T10", "I.5"], "title": "You Are How You Walk: Uncooperative MoCap Gait Identification for Video\n  Surveillance with Incomplete and Noisy Data"}, {"abstract": "In this work we analyze the performances of two of the most used word\nembeddings algorithms, skip-gram and continuous bag of words on Italian\nlanguage. These algorithms have many hyper-parameter that have to be carefully\ntuned in order to obtain accurate word representation in vectorial space. We\nprovide an accurate analysis and an evaluation, showing what are the best\nconfiguration of parameters for specific tasks.", "authors": ["Rocco Tripodi", "Stefano Li Pira"], "category": "cs.CL", "comment": "5 pages, 8 figures", "img": "/static/thumbs/1707.08783v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08783v1", "num_discussion": 0, "originally_published_time": "7/27/2017", "pid": "1707.08783v1", "published_time": "7/27/2017", "rawpid": "1707.08783", "tags": ["cs.CL", "cs.AI"], "title": "Analysis of Italian Word Embeddings"}, {"abstract": "We introduce an evolutionary stochastic-local-search (SLS) algorithm for\naddressing a generalized version of the so-called 1/V/D/R cutting-stock\nproblem. Cutting-stock problems are encountered often in industrial\nenvironments and the ability to address them efficiently usually results in\nlarge economic benefits. Traditionally linear-programming-based techniques have\nbeen utilized to address such problems, however their flexibility might be\nlimited when nonlinear constraints and objective functions are introduced. To\nthis end, this paper proposes an evolutionary SLS algorithm for addressing\none-dimensional cutting-stock problems. The contribution lies in the\nintroduction of a flexible structural framework of the optimization that may\naccommodate a large family of diversification strategies including a novel\nparallel pattern appropriate for SLS algorithms (not necessarily restricted to\ncutting-stock problems). We finally demonstrate through experiments in a\nreal-world manufacturing problem the benefit in cost reduction of the\nconsidered diversification strategies.", "authors": ["Georgios C. Chasparis", "Michael Rossbory", "Verena Haunschmid"], "category": "cs.NE", "comment": "", "img": "/static/thumbs/1707.08776v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08776v1", "num_discussion": 0, "originally_published_time": "7/27/2017", "pid": "1707.08776v1", "published_time": "7/27/2017", "rawpid": "1707.08776", "tags": ["cs.NE"], "title": "An Evolutionary Stochastic-Local-Search Framework for One-Dimensional\n  Cutting-Stock Problems"}, {"abstract": "Gaussian process (GP) models have become a well-established frameworkfor the\nadaptive design of costly experiments, and notably of computerexperiments.\nGP-based sequential designs have been found practicallyefficient for various\nobjectives, such as global optimization(estimating the global maximum or\nmaximizer(s) of a function),reliability analysis (estimating a probability of\nfailure) or theestimation of level sets and excursion sets. In this paper, we\ndealwith convergence properties of an important class of sequential\ndesignapproaches, known as stepwise uncertainty reduction (SUR) strategies.Our\napproach relies on the key observation that the sequence ofresidual uncertainty\nmeasures, in SUR strategies, is generally asupermartingale with respect to the\nfiltration generated by theobservations. We study the existence of SUR\nstrategies and establishgeneric convergence results for a broad class thereof.\nWe alsointroduce a special class of uncertainty measures defined in terms\nofregular loss functions, which makes it easier to check that ourconvergence\nresults apply in particular cases. Applications of thelatter include proofs of\nconvergence for the two main SUR strategiesproposed by Bect, Ginsbourger, Li,\nPicheny and Vazquez (Stat. Comp.,2012). To the best of our knowledge, these are\nthe first convergenceproofs for GP-based sequential design algorithms dedicated\nto theestimation of excursions sets and their measure. Coming to\nglobaloptimization algorithms, we also show that the knowledge gradientstrategy\ncan be cast in the SUR framework with an uncertaintyfunctional stemming from a\nregular loss, resulting in furtherconvergence results. We finally establish a\nnew proof of convergencefor the expected improvement algorithm, which is the\nfirst proof forthis algorithm that applies to any GP with continuous sample\npaths.", "authors": ["Julien Bect", "Fran\u00e7ois Bachoc", "David Ginsbourger"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1608.01118v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1608.01118v2", "num_discussion": 0, "originally_published_time": "8/3/2016", "pid": "1608.01118v2", "published_time": "7/27/2017", "rawpid": "1608.01118", "tags": ["stat.ML", "math.PR", "math.ST", "stat.TH"], "title": "A supermartingale approach to Gaussian process based sequential design\n  of experiments"}, {"abstract": "This paper proposes an improved epsilon constraint-handling mechanism, and\ncombines it with a decomposition-based multi-objective evolutionary algorithm\n(MOEA/D) to solve constrained multi-objective optimization problems (CMOPs).\nThe proposed constrained multi-objective evolutionary algorithm (CMOEA) is\nnamed MOEA/D-IEpsilon. It adjusts the epsilon level dynamically according to\nthe ratio of feasible to total solutions (RFS) in the current population. In\norder to evaluate the performance of MOEA/D-IEpsilon, a new set of CMOPs with\ntwo and three objectives is designed, having large infeasible regions (relative\nto the feasible regions), and they are called LIR-CMOPs. Then the fourteen\nbenchmarks, including LIR-CMOP1-14, are used to test MOEA/D-IEpsilon and four\nother decomposition-based CMOEAs, including MOEA/D-Epsilon, MOEA/D-SR,\nMOEA/D-CDP and C-MOEA/D. The experimental results indicate that MOEA/D-IEpsilon\nis significantly better than the other four CMOEAs on all of the test\ninstances, which shows that MOEA/D-IEpsilon is more suitable for solving CMOPs\nwith large infeasible regions. Furthermore, a real-world problem, namely the\nrobot gripper optimization problem, is used to test the five CMOEAs. The\nexperimental results demonstrate that MOEA/D-IEpsilon also outperforms the\nother four CMOEAs on this problem.", "authors": ["Zhun Fan", "Wenji Li", "Xinye Cai", "Han Huang", "Yi Fang", "Yugen You", "Jiajie Mo", "Caimin Wei", "Erik Goodman"], "category": "cs.NE", "comment": "17 pages, 7 figures and 6 tables", "img": "/static/thumbs/1707.08767v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08767v1", "num_discussion": 0, "originally_published_time": "7/27/2017", "pid": "1707.08767v1", "published_time": "7/27/2017", "rawpid": "1707.08767", "tags": ["cs.NE"], "title": "An Improved Epsilon Constraint-handling Method in MOEA/D for CMOPs with\n  Large Infeasible Regions"}, {"abstract": "Recent years witnessed a growing interest in non-standard epistemic logics of\nknowing whether, knowing how, knowing what, knowing why and so on. The new\nepistemic modalities introduced in those logics all share, in their semantics,\nthe general schema of $\\exists x \\Box \\phi$, e.g., knowing how to achieve\n$\\phi$ roughly means that there exists a way such that you know that it is a\nway to ensure that $\\phi$. Moreover, the resulting logics are decidable.\nInspired by those particular logics, in this work, we propose a very general\nand powerful framework based on quantifier-free predicate language extended by\na new modality $\\Box^x$, which packs exactly $\\exists x \\Box$ together. We show\nthat the resulting language, though much more expressive, shares many good\nproperties of the basic propositional modal logic over arbitrary models, such\nas finite-tree-model property and van Benthem-like characterization w.r.t.\\\nfirst-order modal logic. We axiomatize the logic over S5 frames with intuitive\naxioms to capture the interaction between $\\Box^x$ and know-that operator in an\nepistemic setting.", "authors": ["Yanjing Wang"], "category": "cs.AI", "comment": "In Proceedings TARK 2017, arXiv:1707.08250", "img": "/static/thumbs/1707.08764v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08764v1", "num_discussion": 0, "originally_published_time": "7/27/2017", "pid": "1707.08764v1", "published_time": "7/27/2017", "rawpid": "1707.08764", "tags": ["cs.AI", "cs.LO"], "title": "A New Modal Framework for Epistemic Logic"}, {"abstract": "Legal probabilism (LP) claims the degrees of conviction in juridical\nfact-finding are to be modeled exactly the way degrees of beliefs are modeled\nin standard bayesian epistemology. Classical legal probabilism (CLP) adds that\nthe conviction is justified if the credence in guilt given the evidence is\nabove an appropriate guilt probability threshold. The views are challenged on\nvarious counts, especially by the proponents of the so-called narrative\napproach, on which the fact-finders\u0027 decision is the result of a dynamic\ninterplay between competing narratives of what happened. I develop a way a\nbayesian epistemologist can make sense of the narrative approach. I do so by\nformulating a probabilistic framework for evaluating competing narrations in\nterms of formal explications of the informal evaluation criteria used in the\nnarrative approach.", "authors": ["Rafal Urbaniak"], "category": "cs.AI", "comment": "In Proceedings TARK 2017, arXiv:1707.08250", "img": "/static/thumbs/1707.08763v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08763v1", "num_discussion": 0, "originally_published_time": "7/27/2017", "pid": "1707.08763v1", "published_time": "7/27/2017", "rawpid": "1707.08763", "tags": ["cs.AI", "cs.LO"], "title": "Reconciling Bayesian Epistemology and Narration-based Approaches to\n  Judiciary Fact-finding"}, {"abstract": "This paper combines two studies: a topological semantics for epistemic\nnotions and abstract argumentation theory. In our combined setting, we use a\ntopological semantics to represent the structure of an agent\u0027s collection of\nevidence, and we use argumentation theory to single out the relevant sets of\nevidence through which a notion of beliefs grounded on arguments is defined. We\ndiscuss the formal properties of this newly defined notion, providing also a\nformal language with a matching modality together with a sound and complete\naxiom system for it. Despite the fact that our agent can combine her evidence\nin a \u0027rational\u0027 way (captured via the topological structure), argument-based\nbeliefs are not closed under conjunction. This illustrates the difference\nbetween an agent\u0027s reasoning abilities (i.e. the way she is able to combine her\navailable evidence) and the closure properties of her beliefs. We use this\npoint to argue for why the failure of closure under conjunction of belief\nshould not bear the burden of the failure of rationality.", "authors": ["Chenwei Shi", "Sonja Smets", "Fernando R. Vel\u00e1zquez-Quesada"], "category": "cs.AI", "comment": "In Proceedings TARK 2017, arXiv:1707.08250", "img": "/static/thumbs/1707.08762v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08762v1", "num_discussion": 0, "originally_published_time": "7/27/2017", "pid": "1707.08762v1", "published_time": "7/27/2017", "rawpid": "1707.08762", "tags": ["cs.AI", "cs.LO"], "title": "Argument-based Belief in Topological Structures"}, {"abstract": "The existence of a coalition strategy to achieve a goal does not necessarily\nmean that the coalition has enough information to know how to follow the\nstrategy. Neither does it mean that the coalition knows that such a strategy\nexists. The paper studies an interplay between the distributed knowledge,\ncoalition strategies, and coalition \"know-how\" strategies. The main technical\nresult is a sound and complete trimodal logical system that describes the\nproperties of this interplay.", "authors": ["Pavel Naumov", "Jia Tao"], "category": "cs.AI", "comment": "In Proceedings TARK 2017, arXiv:1707.08250", "img": "/static/thumbs/1707.08759v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08759v1", "num_discussion": 0, "originally_published_time": "7/27/2017", "pid": "1707.08759v1", "published_time": "7/27/2017", "rawpid": "1707.08759", "tags": ["cs.AI", "cs.LO"], "title": "Together We Know How to Achieve: An Epistemic Logic of Know-How\n  (Extended Abstract)"}, {"abstract": "We introduce an axiomatic approach to group recommendations, in line of\nprevious work on the axiomatic treatment of trust-based recommendation systems,\nranking systems, and other foundational work on the axiomatic approach to\ninternet mechanisms in social choice settings. In group recommendations we wish\nto recommend to a group of agents, consisting of both opinionated and undecided\nmembers, a joint choice that would be acceptable to them. Such a system has\nmany applications, such as choosing a movie or a restaurant to go to with a\ngroup of friends, recommending games for online game players, \u0026 other communal\nactivities.\n  Our method utilizes a given social graph to extract information on the\nundecided, relying on the agents influencing them. We first show that a set of\nfairly natural desired requirements (a.k.a axioms) leads to an impossibility,\nrendering mutual satisfaction of them unreachable. However, we also show a\nmodified set of axioms that fully axiomatize a group variant of the random-walk\nrecommendation system, expanding a previous result from the individual\nrecommendation case.", "authors": ["Omer Lev", "Moshe Tennenholtz"], "category": "cs.SI", "comment": "In Proceedings TARK 2017, arXiv:1707.08250", "img": "/static/thumbs/1707.08755v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08755v1", "num_discussion": 0, "originally_published_time": "7/27/2017", "pid": "1707.08755v1", "published_time": "7/27/2017", "rawpid": "1707.08755", "tags": ["cs.SI", "cs.AI", "cs.GT", "cs.MA"], "title": "Group Recommendations: Axioms, Impossibilities, and Random Walks"}, {"abstract": "While there have been many attempts, going back to BAN logic, to base\nreasoning about security protocols on epistemic notions, they have not been all\nthat successful. Arguably, this has been due to the particular logics chosen.\nWe present a simple logic based on the well-understood modal operators of\nknowledge, time, and probability, and show that it is able to handle issues\nthat have often been swept under the rug by other approaches, while being\nflexible enough to capture all the higher- level security notions that appear\nin BAN logic. Moreover, while still assuming that the knowledge operator allows\nfor unbounded computation, it can handle the fact that a computationally\nbounded agent cannot decrypt messages in a natural way, by distinguishing\nstrings and message terms. We demonstrate that our logic can capture BAN logic\nnotions by providing a translation of the BAN operators into our logic,\ncapturing belief by a form of probabilistic knowledge.", "authors": ["Joseph Y. Halpern", "Ron van der Meyden", "Riccardo Pucella"], "category": "cs.CR", "comment": "In Proceedings TARK 2017, arXiv:1707.08250", "img": "/static/thumbs/1707.08750v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08750v1", "num_discussion": 0, "originally_published_time": "7/27/2017", "pid": "1707.08750v1", "published_time": "7/27/2017", "rawpid": "1707.08750", "tags": ["cs.CR", "cs.AI", "cs.LO", "cs.MA"], "title": "An Epistemic Foundation for Authentication Logics (Extended Abstract)"}, {"abstract": "The paper provides an analysis of the voting method known as delegable proxy\nvoting, or liquid democracy. The analysis first positions liquid democracy\nwithin the theory of binary aggregation. It then focuses on two issues of the\nsystem: the occurrence of delegation cycles; and the effect of delegations on\nindividual rationality when voting on logically interdependent propositions. It\nfinally points to proposals on how the system may be modified in order to\naddress the above issues.", "authors": ["Zo\u00e9 Christoff", "Davide Grossi"], "category": "cs.MA", "comment": "In Proceedings TARK 2017, arXiv:1707.08250", "img": "/static/thumbs/1707.08741v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08741v1", "num_discussion": 0, "originally_published_time": "7/27/2017", "pid": "1707.08741v1", "published_time": "7/27/2017", "rawpid": "1707.08741", "tags": ["cs.MA", "cs.AI", "cs.CY", "cs.GT"], "title": "Binary Voting with Delegable Proxy: An Analysis of Liquid Democracy"}, {"abstract": "An abstract argumentation framework can be used to model the argumentative\nstance of an agent at a high level of abstraction, by indicating for every pair\nof arguments that is being considered in a debate whether the first attacks the\nsecond. When modelling a group of agents engaged in a debate, we may wish to\naggregate their individual argumentation frameworks to obtain a single such\nframework that reflects the consensus of the group. Even when agents disagree\non many details, there may well be high-level agreement on important semantic\nproperties, such as the acceptability of a given argument. Using techniques\nfrom social choice theory, we analyse under what circumstances such semantic\nproperties agreed upon by the individual agents can be preserved under\naggregation.", "authors": ["Weiwei Chen", "Ulle Endriss"], "category": "cs.AI", "comment": "In Proceedings TARK 2017, arXiv:1707.08250", "img": "/static/thumbs/1707.08740v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08740v1", "num_discussion": 0, "originally_published_time": "7/27/2017", "pid": "1707.08740v1", "published_time": "7/27/2017", "rawpid": "1707.08740", "tags": ["cs.AI"], "title": "Preservation of Semantic Properties during the Aggregation of Abstract\n  Argumentation Frameworks"}, {"abstract": "In the typical framework for boolean games (BG) each player can change the\ntruth value of some propositional atoms, while attempting to make her goal\ntrue. In standard BG goals are propositional formulas, whereas in iterated BG\ngoals are formulas of Linear Temporal Logic. Both notions of BG are\ncharacterised by the fact that agents have exclusive control over their set of\natoms, meaning that no two agents can control the same atom. In the present\ncontribution we drop the exclusivity assumption and explore structures where an\natom can be controlled by multiple agents. We introduce Concurrent Game\nStructures with Shared Propositional Control (CGS-SPC) and show that they ac-\ncount for several classes of repeated games, including iterated boolean games,\ninfluence games, and aggregation games. Our main result shows that, as far as\nverification is concerned, CGS-SPC can be reduced to concurrent game structures\nwith exclusive control. This result provides a polynomial reduction for the\nmodel checking problem of specifications in Alternating-time Temporal Logic on\nCGS-SPC.", "authors": ["Francesco Belardinelli", "Umberto Grandi", "Andreas Herzig", "Dominique Longin", "Emiliano Lorini", "Arianna Novaro", "Laurent Perrussel"], "category": "cs.AI", "comment": "In Proceedings TARK 2017, arXiv:1707.08250", "img": "/static/thumbs/1707.08736v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08736v1", "num_discussion": 0, "originally_published_time": "7/27/2017", "pid": "1707.08736v1", "published_time": "7/27/2017", "rawpid": "1707.08736", "tags": ["cs.AI", "cs.LO"], "title": "Relaxing Exclusive Control in Boolean Games"}, {"abstract": "In this paper we introduce {\\em global and local announcement logic} (GLAL),\na dynamic epistemic logic with two distinct announcement operators --\n$[\\phi]^+_A$ and $[\\phi]^-_A$ indexed to a subset $A$ of the set $Ag$ of all\nagents -- for global and local announcements respectively. The boundary case\n$[\\phi]^+_{Ag}$ corresponds to the public announcement of $\\phi$, as known from\nthe literature. Unlike standard public announcements, which are {\\em model\ntransformers}, the global and local announcements are {\\em pointed model\ntransformers}. In particular, the update induced by the announcement may be\ndifferent in different states of the model. Therefore, the resulting\ncomputations are trees of models, rather than the typical sequences. A\nconsequence of our semantics is that modally bisimilar states may be\ndistinguished in our logic. Then, we provide a stronger notion of bisimilarity\nand we show that it preserves modal equivalence in GLAL. Additionally, we show\nthat GLAL is strictly more expressive than public announcement logic with\ncommon knowledge. We prove a wide range of validities for GLAL involving the\ninteraction between dynamics and knowledge, and show that the satisfiability\nproblem for GLAL is decidable. We illustrate the formal machinery by means of\ndetailed epistemic scenarios.", "authors": ["Francesco Belardinelli", "Hans van Ditmarsch", "Wiebe van der Hoek"], "category": "cs.LO", "comment": "In Proceedings TARK 2017, arXiv:1707.08250", "img": "/static/thumbs/1707.08735v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08735v1", "num_discussion": 0, "originally_published_time": "7/27/2017", "pid": "1707.08735v1", "published_time": "7/27/2017", "rawpid": "1707.08735", "tags": ["cs.LO", "cs.AI"], "title": "A Logic for Global and Local Announcements"}, {"abstract": "Gossip protocols aim at arriving, by means of point-to-point or group\ncommunications, at a situation in which all the agents know each other secrets.\nRecently a number of authors studied distributed epistemic gossip protocols.\nThese protocols use as guards formulas from a simple epistemic logic, which\nmakes their analysis and verification substantially easier.\n  We study here common knowledge in the context of such a logic. First, we\nanalyze when it can be reduced to iterated knowledge. Then we show that the\nsemantics and truth for formulas without nested common knowledge operator are\ndecidable. This implies that implementability, partial correctness and\ntermination of distributed epistemic gossip protocols that use non-nested\ncommon knowledge operator is decidable, as well. Given that common knowledge is\nequivalent to an infinite conjunction of nested knowledge, these results are\nnon-trivial generalizations of the corresponding decidability results for the\noriginal epistemic logic, established in (Apt \u0026 Wojtczak, 2016).\n  K. R. Apt \u0026 D. Wojtczak (2016): On Decidability of a Logic of Gossips. In\nProc. of JELIA 2016, pp. 18-33, doi:10.1007/ 978-3-319-48758-8_2.", "authors": ["Krzysztof R. Apt", "Dominik Wojtczak"], "category": "cs.LO", "comment": "In Proceedings TARK 2017, arXiv:1707.08250", "img": "/static/thumbs/1707.08734v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08734v1", "num_discussion": 0, "originally_published_time": "7/27/2017", "pid": "1707.08734v1", "published_time": "7/27/2017", "rawpid": "1707.08734", "tags": ["cs.LO", "cs.AI"], "title": "Common Knowledge in a Logic of Gossips"}, {"abstract": "Acoustic Event Classification (AEC) has become a significant task for\nmachines to perceive the surrounding auditory scene. However, extracting\neffective representations that capture the underlying characteristics of the\nacoustic events is still challenging. Previous methods mainly focused on\ndesigning the audio features in a \u0027hand-crafted\u0027 manner. Interestingly,\ndata-learnt features have been recently reported to show better performance. Up\nto now, these were only considered on the frame-level. In this paper, we\npropose an unsupervised learning framework to learn a vector representation of\nan audio sequence for AEC. This framework consists of a Recurrent Neural\nNetwork (RNN) encoder and a RNN decoder, which respectively transforms the\nvariable-length audio sequence into a fixed-length vector and reconstructs the\ninput sequence on the generated vector. After training the encoder-decoder, we\nfeed the audio sequences to the encoder and then take the learnt vectors as the\naudio sequence representations. Compared with previous methods, the proposed\nmethod can not only deal with the problem of arbitrary-lengths of audio\nstreams, but also learn the salient information of the sequence. Extensive\nevaluation on a large-size acoustic event database is performed, and the\nempirical results demonstrate that the learnt audio sequence representation\nyields a significant performance improvement by a large margin compared with\nother state-of-the-art hand-crafted sequence features for AEC.", "authors": ["Zixing Zhang", "Ding Liu", "Jing Han", "Bj\u00f6rn Schuller"], "category": "cs.SD", "comment": "", "img": "/static/thumbs/1707.08729v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08729v1", "num_discussion": 0, "originally_published_time": "7/27/2017", "pid": "1707.08729v1", "published_time": "7/27/2017", "rawpid": "1707.08729", "tags": ["cs.SD", "cs.LG"], "title": "Learning Audio Sequence Representations for Acoustic Event\n  Classification"}, {"abstract": "Shot boundary detection (SBD) is an important pre-processing step for video\nmanipulation. Here, each segment of frames is classified as either sharp,\ngradual or no transition. Current SBD techniques analyze hand-crafted features\nand attempt to optimize both detection accuracy and processing speed. However,\nthe heavy computations of optical flow prevents this. To achieve this aim, we\npresent an SBD technique based on spatio-temporal Convolutional Neural Networks\n(CNN). Since current datasets are not large enough to train an accurate SBD\nCNN, we present a new dataset containing more than 3.5 million frames of sharp\nand gradual transitions. The transitions are generated synthetically using\nimage compositing models. Our dataset contain additional 70,000 frames of\nimportant hard-negative no transitions. We perform the largest evaluation to\ndate for one SBD algorithm, on real and synthetic data, containing more than\n4.85 million frames. In comparison to the state of the art, we outperform\ndissolve gradual detection, generate competitive performance for sharp\ndetections and produce significant improvement in wipes. In addition, we are up\nto 11 times faster than the state of the art.", "authors": ["Ahmed Hassanien", "Mohamed Elgharib", "Ahmed Selim", "Sung-Ho Bae", "Mohamed Hefeeda", "Wojciech Matusik"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1705.03281v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1705.03281v2", "num_discussion": 0, "originally_published_time": "5/9/2017", "pid": "1705.03281v2", "published_time": "7/27/2017", "rawpid": "1705.03281", "tags": ["cs.CV"], "title": "Large-scale, Fast and Accurate Shot Boundary Detection through\n  Spatio-temporal Convolutional Neural Networks"}, {"abstract": "In multiview geometry when correspondences among multiple views are unknown\nthe image points can be understood as being unlabeled. This is a common problem\nin computer vision. We give a novel approach to handle such a situation by\nregarding unlabeled point configurations as points on the Chow variety\n$\\text{Sym}_m(\\mathbb{P}^2)$. For two unlabeled points we design an algorithm\nthat solves the triangulation problem with unknown correspondences. Further the\nunlabeled multiview variety $\\text{Sym}_m(V_A)$ is studied.", "authors": ["Andr\u00e9 Wagner"], "category": "cs.CV", "comment": "17 pages", "img": "/static/thumbs/1707.08722v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08722v1", "num_discussion": 0, "originally_published_time": "7/27/2017", "pid": "1707.08722v1", "published_time": "7/27/2017", "rawpid": "1707.08722", "tags": ["cs.CV"], "title": "Algebraic Relations and Triangulation of Unlabeled Image Points"}, {"abstract": "In recent years, the performance of object detection has advanced\nsignificantly with the evolving deep convolutional neural networks. However,\nthe state-of-the-art object detection methods still rely on accurate bounding\nbox annotations that require extensive human labelling. Object detection\nwithout bounding box annotations, i.e, weakly supervised detection methods, are\nstill lagging far behind. As weakly supervised detection only uses image level\nlabels and does not require the ground truth of bounding box location and label\nof each object in an image, it is generally very difficult to distill knowledge\nof the actual appearances of objects. Inspired by curriculum learning, this\npaper proposes an easy-to-hard knowledge transfer scheme that incorporates easy\nweb images to provide prior knowledge of object appearance as a good starting\npoint. While exploiting large-scale free web imagery, we introduce a\nsophisticated labour free method to construct a web dataset with good diversity\nin object appearance. After that, semantic relevance and distribution relevance\nare introduced and utilized in the proposed curriculum training scheme. Our\nend-to-end learning with the constructed web data achieves remarkable\nimprovement across most object classes especially for the classes that are\noften considered hard in other works.", "authors": ["Qingyi Tao", "Hao Yang", "Jianfei Cai"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1707.08721v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08721v1", "num_discussion": 0, "originally_published_time": "7/27/2017", "pid": "1707.08721v1", "published_time": "7/27/2017", "rawpid": "1707.08721", "tags": ["cs.CV"], "title": "Exploiting Web Images for Weakly Supervised Object Detection"}, {"abstract": "Wireless video streaming has traditionally been considered an extremely\npower-hungry operation. Existing approaches optimize the camera and\ncommunication modules individually to minimize their power consumption.\nHowever, the joint redesign and optimization of wireless communication as well\nas the camera is what that provides more power saving. We present an\nultra-low-power wireless video streaming camera. To achieve this, we present a\nnovel \"analog\" video backscatter technique that feeds analog pixels from the\nphoto-diodes directly to the backscatter hardware, thereby eliminating power\nconsuming hardware components such as ADCs and amplifiers. We prototype our\nwireless camera using off-the-shelf hardware and show that our design can\nstream video at up to 13 FPS and can operate up to a distance of 150 feet from\nthe access point. Our COTS prototype consumes 2.36mW. Finally, to demonstrate\nthe potential of our design, we built two proof-of-concept applications: video\nstreaming for micro-robots and security cameras for face detection.", "authors": ["Saman Naderiparizi", "Mehrdad Hessar", "Vamsi Talla", "Shyamnath Gollakota", "Joshua R. Smith"], "category": "cs.ET", "comment": "9 pages, 11 figures", "img": "/static/thumbs/1707.08718v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08718v1", "num_discussion": 0, "originally_published_time": "7/27/2017", "pid": "1707.08718v1", "published_time": "7/27/2017", "rawpid": "1707.08718", "tags": ["cs.ET", "cs.CV"], "title": "Ultra-low-power Wireless Streaming Cameras"}, {"abstract": "SVRG and its variants are among the state of art optimization algorithms for\nlarge scale machine learning problems. It is well known that SVRG converges\nlinearly when the objective function is strongly convex. However this setup can\nbe restrictive, and does not include several important formulations such as\nLasso, group Lasso, logistic regression, and some non-convex models including\ncorrected Lasso and SCAD. In this paper, we prove that, for a class of\nstatistical M-estimators covering examples mentioned above, SVRG solves the\nformulation with {\\em a linear convergence rate} without strong convexity or\neven convexity. Our analysis makes use of {\\em restricted strong convexity},\nunder which we show that SVRG converges linearly to the fundamental statistical\nprecision of the model, i.e., the difference between true unknown parameter\n$\\theta^*$ and the optimal solution $\\hat{\\theta}$ of the model.", "authors": ["Chao Qu", "Yan Li", "Huan Xu"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1611.01957v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1611.01957v3", "num_discussion": 0, "originally_published_time": "11/7/2016", "pid": "1611.01957v3", "published_time": "7/27/2017", "rawpid": "1611.01957", "tags": ["stat.ML", "cs.LG"], "title": "Linear Convergence of SVRG in Statistical Estimation"}, {"abstract": "Sketch-based modeling strives to bring the ease and immediacy of drawing to\nthe 3D world. However, while drawings are easy for humans to create, they are\nvery challenging for computers to interpret due to their sparsity and\nambiguity. We propose a data-driven approach that tackles this challenge by\nlearning to reconstruct 3D shapes from one or more drawings. At the core of our\napproach is a deep convolutional neural network (CNN) that predicts occupancy\nof a voxel grid from a line drawing. This CNN provides us with an initial 3D\nreconstruction as soon as the user completes a single drawing of the desired\nshape. We complement this single-view network with an updater CNN that refines\nan existing prediction given a new drawing of the shape created from a novel\nviewpoint. A key advantage of our approach is that we can apply the updater\niteratively to fuse information from an arbitrary number of viewpoints, without\nrequiring explicit stroke correspondences between the drawings. We train both\nCNNs by rendering synthetic contour drawings from hand-modeled shape\ncollections as well as from procedurally-generated abstract shapes. Finally, we\nintegrate our CNNs in a minimal modeling interface that allows users to\nseamlessly draw an object, rotate it to see its 3D reconstruction, and refine\nit by re-drawing from another vantage point using the 3D reconstruction as\nguidance. The main strengths of our approach are its robustness to freehand\nbitmap drawings, its ability to adapt to different object categories, and the\ncontinuum it offers between single-view and multi-view sketch-based modeling.", "authors": ["Johanna Delanoy", "Adrien Bousseau", "Mathieu Aubry", "Phillip Isola", "Alexei A. Efros"], "category": "cs.GR", "comment": "See our accompanying video on https://youtu.be/DGIYzmlm2pQ", "img": "/static/thumbs/1707.08390v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08390v2", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08390v2", "published_time": "7/27/2017", "rawpid": "1707.08390", "tags": ["cs.GR", "cs.CV", "I.3.5"], "title": "What You Sketch Is What You Get: 3D Sketching using Multi-View Deep\n  Volumetric Prediction"}, {"abstract": "The statistical dependencies which independent component analysis (ICA)\ncannot remove often provide rich information beyond the linear independent\ncomponents. It would thus be very useful to estimate the dependency structure\nfrom data. While such models have been proposed, they usually concentrated on\nhigher-order correlations such as energy (square) correlations. Yet, linear\ncorrelations are a most fundamental and informative form of dependency in many\nreal data sets. Linear correlations are usually completely removed by ICA and\nrelated methods, so they can only be analyzed by developing new methods which\nexplicitly allow for linearly correlated components. In this paper, we propose\na probabilistic model of linear non-Gaussian components which are allowed to\nhave both linear and energy correlations. The precision matrix of the linear\ncomponents is assumed to be randomly generated by a higher-order process and\nexplicitly parametrized by a parameter matrix. The estimation of the parameter\nmatrix is shown to be particularly simple because using score matching, the\nobjective function is a quadratic form. Using simulations with artificial data,\nwe demonstrate that the proposed method improves identifiability of\nnon-Gaussian components by simultaneously learning their correlation structure.\nApplications on simulated complex cells with natural image input, as well as\nspectrograms of natural audio data show that the method finds new kinds of\ndependencies between the components.", "authors": ["Hiroaki Sasaki", "Michael U. Gutmann", "Hayaru Shouno", "Aapo Hyv\u00e4rinen"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1506.05666v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1506.05666v2", "num_discussion": 0, "originally_published_time": "6/18/2015", "pid": "1506.05666v2", "published_time": "7/27/2017", "rawpid": "1506.05666", "tags": ["stat.ML"], "title": "Simultaneous Estimation of Non-Gaussian Components and their Correlation\n  Structure"}, {"abstract": "Determining semantic textual similarity is a core research subject in natural\nlanguage processing. Since vector-based models for sentence representation\noften use shallow information, capturing accurate semantics is difficult. By\ncontrast, logical semantic representations capture deeper levels of sentence\nsemantics, but their symbolic nature does not offer graded notions of textual\nsimilarity. We propose a method for determining semantic textual similarity by\ncombining shallow features with features extracted from natural deduction\nproofs of bidirectional entailment relations between sentence pairs. For the\nnatural deduction proofs, we use ccg2lambda, a higher-order automatic inference\nsystem, which converts Combinatory Categorial Grammar (CCG) derivation trees\ninto semantic representations and conducts natural deduction proofs.\nExperiments show that our system was able to outperform other logic-based\nsystems and that features derived from the proofs are effective for learning\ntextual similarity.", "authors": ["Hitomi Yanaka", "Koji Mineshima", "Pascual Martinez-Gomez", "Daisuke Bekki"], "category": "cs.CL", "comment": "11 pages, 5 figures, accepted as long paper of EMNLP2017", "img": "/static/thumbs/1707.08713v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08713v1", "num_discussion": 0, "originally_published_time": "7/27/2017", "pid": "1707.08713v1", "published_time": "7/27/2017", "rawpid": "1707.08713", "tags": ["cs.CL"], "title": "Determining Semantic Textual Similarity using Natural Deduction Proofs"}, {"abstract": "Orthogonal matching pursuit (OMP) and orthogonal least squares (OLS) are\nwidely used for sparse signal reconstruction in under-determined linear\nregression problems. The performance of these compressed sensing (CS)\nalgorithms depends crucially on the \\textit{a priori} knowledge of either the\nsparsity of the signal ($k_0$) or noise variance ($\\sigma^2$). Both $k_0$ and\n$\\sigma^2$ are unknown in general and extremely difficult to estimate in under\ndetermined models. This limits the application of OMP and OLS in many practical\nsituations. In this article, we develop two computationally efficient\nframeworks namely TF-IGP and RRT-IGP for using OMP and OLS even when $k_0$ and\n$\\sigma^2$ are unavailable. Both TF-IGP and RRT-IGP are analytically shown to\naccomplish successful sparse recovery under the same set of restricted isometry\nconditions on the design matrix required for OMP/OLS with \\textit{a priori}\nknowledge of $k_0$ and $\\sigma^2$. Numerical simulations also indicate a highly\ncompetitive performance of TF-IGP and RRT-IGP in comparison to OMP/OLS with\n\\textit{a priori} knowledge of $k_0$ and $\\sigma^2$.", "authors": ["Sreejith Kallummil", "Sheetal Kalyani"], "category": "stat.ML", "comment": "14 pages and 7 figures", "img": "/static/thumbs/1707.08712v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08712v1", "num_discussion": 0, "originally_published_time": "7/27/2017", "pid": "1707.08712v1", "published_time": "7/27/2017", "rawpid": "1707.08712", "tags": ["stat.ML"], "title": "Signal and Noise Statistics Oblivious Sparse Reconstruction using\n  OMP/OLS"}, {"abstract": "To solve deep metric learning problems and producing feature embeddings,\ncurrent methodologies will commonly use a triplet model to minimise the\nrelative distance between samples from the same class and maximise the relative\ndistance between samples from different classes. Though successful, the\ntraining convergence of this triplet model can be compromised by the fact that\nthe vast majority of the training samples will produce gradients with\nmagnitudes that are close to zero. This issue has motivated the development of\nmethods that explore the global structure of the embedding and other methods\nthat explore hard negative/positive mining. The effectiveness of such mining\nmethods is often associated with intractable computational requirements. In\nthis paper, we propose a novel deep metric learning method that combines the\ntriplet model and the global structure of the embedding space. We rely on a\nsmart mining procedure that produces effective training samples for a low\ncomputational cost. In addition, we propose an adaptive controller that\nautomatically adjusts the smart mining hyper-parameters and speeds up the\nconvergence of the training process. We show empirically that our proposed\nmethod allows for fast and more accurate training of triplet ConvNets than\nother competing mining methods. Additionally, we show that our method achieves\nnew state-of-the-art embedding results for CUB-200-2011 and Cars196 datasets.", "authors": ["Ben Harwood", "Vijay Kumar B G", "Gustavo Carneiro", "Ian Reid", "Tom Drummond"], "category": "cs.CV", "comment": "*Vijay Kumar B G and Ben Harwood contributed equally to this work.\n  Accepted in IEEE International ...", "img": "/static/thumbs/1704.01285v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.01285v3", "num_discussion": 0, "originally_published_time": "4/5/2017", "pid": "1704.01285v3", "published_time": "7/27/2017", "rawpid": "1704.01285", "tags": ["cs.CV"], "title": "Smart Mining for Deep Metric Learning"}, {"abstract": "Facial attribute analysis in the real world scenario is very challenging\nmainly because of complex face variations. Existing works of analyzing face\nattributes are mostly based on the cropped and aligned face images. However,\nthis result in the capability of attribute prediction heavily relies on the\npreprocessing of face detector. To address this problem, we present a novel\njointly learned deep architecture for both facial attribute analysis and face\ndetection. Our framework can process the natural images in the wild and our\nexperiments on CelebA and LFWA datasets clearly show that the state-of-the-art\nperformance is obtained.", "authors": ["Keke He", "Yanwei Fu", "Xiangyang Xue"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1707.08705v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08705v1", "num_discussion": 0, "originally_published_time": "7/27/2017", "pid": "1707.08705v1", "published_time": "7/27/2017", "rawpid": "1707.08705", "tags": ["cs.CV"], "title": "A Jointly Learned Deep Architecture for Facial Attribute Analysis and\n  Face Detection in the Wild"}, {"abstract": "Statistical Relational Models and, more recently, Probabilistic Programming,\nhave been making strides towards an integration of logic and probabilistic\nreasoning. A natural expectation for this project is that a probabilistic logic\nreasoning algorithm reduces to a logic reasoning algorithm when provided a\nmodel that only involves 0-1 probabilities, exhibiting all the advantages of\nlogic reasoning such as short-circuiting, intelligibility, and the ability to\nprovide proof trees for a query answer. In fact, we can take this further and\nrequire that these characteristics be present even for probabilistic models\nwith probabilities \\emph{near} 0 and 1, with graceful degradation as the model\nbecomes more uncertain. We also seek inference that has amortized constant time\ncomplexity on a model\u0027s size (even if still exponential in the induced width of\na more directly relevant portion of it) so that it can be applied to huge\nknowledge bases of which only a relatively small portion is relevant to typical\nqueries. We believe that, among the probabilistic reasoning algorithms, Belief\nPropagation is the most similar to logic reasoning: messages are propagated\namong neighboring variables, and the paths of message-passing are similar to\nproof trees. However, Belief Propagation is either only applicable to tree\nmodels, or approximate (and without guarantees) for precision and convergence.\nIn this paper we present work in progress on an Anytime Exact Belief\nPropagation algorithm that is very similar to Belief Propagation but is exact\neven for graphical models with cycles, while exhibiting soft short-circuiting,\namortized constant time complexity in the model size, and which can provide\nprobabilistic proof trees.", "authors": ["Gabriel Azevedo Ferreira", "Quentin Bertrand", "Charles Maussion", "Rodrigo de Salvo Braz"], "category": "cs.AI", "comment": "Submission to StaRAI-17 workshop at UAI-17 conference", "img": "/static/thumbs/1707.08704v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08704v1", "num_discussion": 0, "originally_published_time": "7/27/2017", "pid": "1707.08704v1", "published_time": "7/27/2017", "rawpid": "1707.08704", "tags": ["cs.AI"], "title": "Anytime Exact Belief Propagation"}, {"abstract": "Multi-robot transfer learning allows a robot to use data generated by a\nsecond, similar robot to improve its own behavior. The potential advantages are\nreducing the time of training and the unavoidable risks that exist during the\ntraining phase. Transfer learning algorithms aim to find an optimal transfer\nmap between different robots. In this paper, we investigate, through a\ntheoretical study of single-input single-output (SISO) systems, the properties\nof such optimal transfer maps. We first show that the optimal transfer learning\nmap is, in general, a dynamic system. The main contribution of the paper is to\nprovide an algorithm for determining the properties of this optimal dynamic map\nincluding its order and regressors (i.e., the variables it depends on). The\nproposed algorithm does not require detailed knowledge of the robots\u0027 dynamics,\nbut relies on basic system properties easily obtainable through simple\nexperimental tests. We validate the proposed algorithm experimentally through\nan example of transfer learning between two different quadrotor platforms.\nExperimental results show that an optimal dynamic map, with correct properties\nobtained from our proposed algorithm, achieves 60-70% reduction of transfer\nlearning error compared to the cases when the data is directly transferred or\ntransferred using an optimal static map.", "authors": ["Mohamed K. Helwa", "Angela P. Schoellig"], "category": "cs.RO", "comment": "7 pages, 6 figures, accepted at the 2017 IEEE/RSJ International\n  Conference on Intelligent Robots a...", "img": "/static/thumbs/1707.08689v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08689v1", "num_discussion": 0, "originally_published_time": "7/27/2017", "pid": "1707.08689v1", "published_time": "7/27/2017", "rawpid": "1707.08689", "tags": ["cs.RO", "cs.LG", "cs.SY"], "title": "Multi-Robot Transfer Learning: A Dynamical System Perspective"}, {"abstract": "We proposed a kind of naturally combined shape-color affine moment invariants\n(SCAMI), which consider both shape and color affine transformations\nsimultaneously in one single system. In the real scene, color and shape\ndeformations always exist in images simultaneously. Simple shape invariants or\ncolor invariants can not be qualified for this situation. The conventional\nmethod is just to make a simple linear combination of the two factors.\nMeanwhile, the manual selection of weights is a complex issue. Our construction\nmethod is based on the multiple integration framework. The integral kernel is\nassigned as the continued product of the shape and color invariant cores. It is\nthe first time to directly derive an invariant to dual affine transformations\nof shape and color. The manual selection of weights is no longer necessary, and\nboth the shape and color transformations are extended to affine transformation\ngroup. With the various of invariant cores, a set of lower-order invariants are\nconstructed and the completeness and independence are discussed detailedly. A\nset of SCAMIs, which called SCAMI24, are recommended, and the effectiveness and\nrobustness have been evaluated on both synthetic and real datasets.", "authors": ["Ming Gong", "You Hao", "Hanlin Mo", "Hua Li"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1705.10928v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1705.10928v2", "num_discussion": 0, "originally_published_time": "5/31/2017", "pid": "1705.10928v2", "published_time": "7/27/2017", "rawpid": "1705.10928", "tags": ["cs.CV"], "title": "Naturally Combined Shape-Color Moment Invariants under Affine\n  Transformations"}, {"abstract": "Sarcasm detection is a key task for many natural language processing tasks.\nIn sentiment analysis, for example, sarcasm can flip the polarity of an\n\"apparently positive\" sentence and, hence, negatively affect polarity detection\nperformance. To date, most approaches to sarcasm detection have treated the\ntask primarily as a text categorization problem. Sarcasm, however, can be\nexpressed in very subtle ways and requires a deeper understanding of natural\nlanguage that standard text categorization techniques cannot grasp. In this\nwork, we develop models based on a pre-trained convolutional neural network for\nextracting sentiment, emotion and personality features for sarcasm detection.\nSuch features, along with the network\u0027s baseline features, allow the proposed\nmodels to outperform the state of the art on benchmark datasets. We also\naddress the often ignored generalizability issue of classifying data that have\nnot been seen by the models at learning phase.", "authors": ["Soujanya Poria", "Erik Cambria", "Devamanyu Hazarika", "Prateek Vij"], "category": "cs.CL", "comment": "Published in COLING 2016", "img": "/static/thumbs/1610.08815v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1610.08815v2", "num_discussion": 0, "originally_published_time": "10/27/2016", "pid": "1610.08815v2", "published_time": "7/27/2017", "rawpid": "1610.08815", "tags": ["cs.CL"], "title": "A Deeper Look into Sarcastic Tweets Using Deep Convolutional Neural\n  Networks"}, {"abstract": "SSD is one of the state-of-the-art object detection algorithms, and it\ncombines high detection accuracy with real-time speed. However, it is widely\nrecognized that SSD is less accurate in detecting small objects compared to\nlarge objects, because it ignores the context from outside the proposal boxes.\nIn this paper, we present CSSD--a shorthand for context-aware single-shot\nmultibox object detector. CSSD is built on top of SSD, with additional layers\nmodeling multi-scale contexts. We describe two variants of CSSD, which differ\nin their context layers, using dilated convolution layers (DiCSSD) and\ndeconvolution layers (DeCSSD) respectively. The experimental results show that\nthe multi-scale context modeling significantly improves the detection accuracy.\nIn addition, we study the relationship between effective receptive fields\n(ERFs) and the theoretical receptive fields (TRFs), particularly on a VGGNet.\nThe empirical results further strengthen our conclusion that SSD coupled with\ncontext layers achieves better detection results especially for small objects\n($+3.2\\% {\\rm AP}_{@0.5}$ on MS-COCO compared to the newest SSD), while\nmaintaining comparable runtime performance.", "authors": ["Wei Xiang", "Dong-Qing Zhang", "Vassilis Athitsos", "Heather Yu"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1707.08682v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08682v1", "num_discussion": 0, "originally_published_time": "7/27/2017", "pid": "1707.08682v1", "published_time": "7/27/2017", "rawpid": "1707.08682", "tags": ["cs.CV"], "title": "Context-aware Single-Shot Detector"}, {"abstract": "Many state-of-the-art deep learning models for question answer retrieval are\nhighly complex, often having a huge number of parameters or complicated word\ninteraction mechanisms. This paper studies if it is possible to achieve equally\ncompetitive performance with smaller and faster neural architectures. Overall,\nour proposed approach is a simple neural network that performs question-answer\nmatching and ranking in Hyperbolic space. We show that QA embeddings learned in\nHyperbolic space results in highly competitive performance on multiple\nbenchmarks, outperforming models with significantly much larger parameters. Our\nproposed approach (90K parameters) remains competitive to models with millions\nof parameters such as Attentive Pooling BiLSTMs or Multi-Perspective\nConvolutional Neural Networks (MP-CNN).", "authors": ["Yi Tay", "Luu Anh Tuan", "Siu Cheung Hui"], "category": "cs.IR", "comment": "", "img": "/static/thumbs/1707.07847v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.07847v2", "num_discussion": 0, "originally_published_time": "7/25/2017", "pid": "1707.07847v2", "published_time": "7/27/2017", "rawpid": "1707.07847", "tags": ["cs.IR", "cs.CL"], "title": "Enabling Efficient Question Answer Retrieval via Hyperbolic Neural\n  Networks"}, {"abstract": "Most state-of-the-art motion segmentation algorithms draw their potential\nfrom modeling motion differences of local entities such as point trajectories\nin terms of pairwise potentials in graphical models. Inference in instances of\nminimum cost multicut problems defined on such graphs al- lows to optimize the\nnumber of the resulting segments along with the segment assignment. However,\npairwise potentials limit the discriminative power of the employed motion\nmodels to translational differences. More complex models such as Euclidean or\naffine transformations call for higher-order potentials and a tractable\ninference in the resulting higher-order graphical models. In this paper, we (1)\nintroduce a generalization of the minimum cost lifted multicut problem to\nhypergraphs, and (2) propose a simple primal feasible heuristic that allows for\na reasonably efficient inference in instances of higher-order lifted multicut\nproblem instances defined on point trajectory hypergraphs for motion\nsegmentation. The resulting motion segmentations improve over the\nstate-of-the-art on the FBMS-59 dataset.", "authors": ["Margret Keuper"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1704.01811v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.01811v2", "num_discussion": 0, "originally_published_time": "4/6/2017", "pid": "1704.01811v2", "published_time": "7/27/2017", "rawpid": "1704.01811", "tags": ["cs.CV"], "title": "Higher-Order Minimum Cost Lifted Multicuts for Motion Segmentation"}, {"abstract": "In this paper we introduce a stochastic projected subgradient method for\nweakly convex (i.e., uniformly prox-regular) nonsmooth, nonconvex functions---a\nwide class of functions which includes the additive and convex composite\nclasses. At a high-level the method is an inexact proximal point iteration in\nwhich the strongly convex proximal subproblems are quickly solved with a\nspecialized stochastic projected subgradient method. The primary contribution\nof this paper is a simple proof that the proposed algorithm converges at the\nsame rate as the stochastic gradient method for smooth nonconvex problems. This\nresult validates the use of stochastic subgradient methods in nonsmooth,\nnonconvex optimization as is common when optimizing neural networks.", "authors": ["Damek Davis", "Benjamin Grimmer"], "category": "math.OC", "comment": "Updated 7/26/2017: Added references to introduction and a couple\n  simple extensions as Sections 3.2...", "img": "/static/thumbs/1707.03505v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.03505v2", "num_discussion": 0, "originally_published_time": "7/12/2017", "pid": "1707.03505v2", "published_time": "7/27/2017", "rawpid": "1707.03505", "tags": ["math.OC", "cs.LG", "65K05, 65K10, 90C26, 90C15, 90C30"], "title": "Proximally Guided Stochastic Subgradient Method for Nonsmooth, Nonconvex\n  Problems"}, {"abstract": "Robots operating alongside humans in diverse, stochastic environments must be\nable to accurately interpret natural language commands. These instructions\noften fall into one of two categories: those that specify a goal condition or\ntarget state, and those that specify explicit actions, or how to perform a\ngiven task. Recent approaches have used reward functions as a semantic\nrepresentation of goal-based commands, which allows for the use of a\nstate-of-the-art planner to find a policy for the given task. However, these\nreward functions cannot be directly used to represent action-oriented commands.\nWe introduce a new hybrid approach, the Deep Recurrent Action-Goal Grounding\nNetwork (DRAGGN), for task grounding and execution that handles natural\nlanguage from either category as input, and generalizes to unseen environments.\nOur robot-simulation results demonstrate that a system successfully\ninterpreting both goal-oriented and action-oriented task specifications brings\nus closer to robust natural language understanding for human-robot interaction.", "authors": ["Siddharth Karamcheti", "Edward C. Williams", "Dilip Arumugam", "Mina Rhee", "Nakul Gopalan", "Lawson L. S. Wong", "Stefanie Tellex"], "category": "cs.AI", "comment": "Accepted at the 1st Workshop on Language Grounding for Robotics at\n  ACL 2017", "img": "/static/thumbs/1707.08668v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08668v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08668v1", "published_time": "7/26/2017", "rawpid": "1707.08668", "tags": ["cs.AI", "cs.CL"], "title": "A Tale of Two DRAGGNs: A Hybrid Approach for Interpreting\n  Action-Oriented and Goal-Oriented Instructions"}, {"abstract": "This paper deals with using word embedding models to trace the temporal\ndynamics of semantic relations between pairs of words. The set-up is similar to\nthe well-known analogies task, but expanded with a time dimension. To this end,\nwe apply incremental updating of the models with new training texts, including\nincremental vocabulary expansion, coupled with learned transformation matrices\nthat let us map between members of the relation. The proposed approach is\nevaluated on the task of predicting insurgent armed groups based on\ngeographical locations. The gold standard data for the time span 1994--2010 is\nextracted from the UCDP Armed Conflicts dataset. The results show that the\nmethod is feasible and outperforms the baselines, but also that important work\nstill remains to be done.", "authors": ["Andrey Kutuzov", "Erik Velldal", "Lilja \u00d8vrelid"], "category": "cs.CL", "comment": "to appear in EMNLP 2017 proceedings", "img": "/static/thumbs/1707.08660v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08660v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08660v1", "published_time": "7/26/2017", "rawpid": "1707.08660", "tags": ["cs.CL"], "title": "Temporal dynamics of semantic relations in word embeddings: an\n  application to predicting armed conflict participants"}, {"abstract": "In this paper, we investigate the cross-database micro-expression recognition\nproblem, where the training and testing samples are from two different\nmicro-expression databases. Under this setting, the training and testing\nsamples would have different feature distributions and hence the performance of\nmost existing micro-expression recognition methods may decrease greatly. To\nsolve this problem, we propose a simple yet effective method called Target\nSample Re-Generator (TSRG) in this paper. By using TSRG, we are able to\nre-generate the samples from target micro-expression database and the\nre-generated target samples would share same or similar feature distributions\nwith the original source samples. For this reason, we can then use the\nclassifier learned based on the labeled source samples to accurately predict\nthe micro-expression categories of the unlabeled target samples. To evaluate\nthe performance of the proposed TSRG method, extensive cross-database\nmicro-expression recognition experiments designed based on SMIC and CASME II\ndatabases are conducted. Compared with recent state-of-the-art cross-database\nemotion recognition methods, the proposed TSRG achieves more promising results.", "authors": ["Yuan Zong", "Xiaohua Huang", "Wenming Zheng", "Zhen Cui", "Guoying Zhao"], "category": "cs.CV", "comment": "To appear at ACM Multimedia 2017", "img": "/static/thumbs/1707.08645v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08645v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08645v1", "published_time": "7/26/2017", "rawpid": "1707.08645", "tags": ["cs.CV"], "title": "Learning a Target Sample Re-Generator for Cross-Database\n  Micro-Expression Recognition"}, {"abstract": "We propose a new encoder-decoder approach to learn distributed sentence\nrepresentations that are applicable to multiple purposes. The model is learned\nby using a convolutional neural network as an encoder to map an input sentence\ninto a continuous vector, and using a long short-term memory recurrent neural\nnetwork as a decoder. Several tasks are considered, including sentence\nreconstruction and future sentence prediction. Further, a hierarchical\nencoder-decoder model is proposed to encode a sentence to predict multiple\nfuture sentences. By training our models on a large collection of novels, we\nobtain a highly generic convolutional sentence encoder that performs well in\npractice. Experimental results on several benchmark datasets, and across a\nbroad range of applications, demonstrate the superiority of the proposed model\nover competing methods.", "authors": ["Zhe Gan", "Yunchen Pu", "Ricardo Henao", "Chunyuan Li", "Xiaodong He", "Lawrence Carin"], "category": "cs.CL", "comment": "Accepted by EMNLP 2017", "img": "/static/thumbs/1611.07897v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1611.07897v2", "num_discussion": 0, "originally_published_time": "11/23/2016", "pid": "1611.07897v2", "published_time": "7/26/2017", "rawpid": "1611.07897", "tags": ["cs.CL", "cs.LG"], "title": "Learning Generic Sentence Representations Using Convolutional Neural\n  Networks"}, {"abstract": "Recognizing facial action units (AUs) during spontaneous facial displays is a\nchallenging problem. Most recently, CNNs have shown promise for facial AU\nrecognition, where predefined and fixed convolution filter sizes are employed.\nIn order to achieve the best performance, the optimal filter size is often\nempirically found by conducting extensive experimental validation. Such a\ntraining process suffers from expensive training cost, especially as the\nnetwork becomes deeper. In addition, AUs activated by different facial muscles\nproduce facial appearance changes at different scales and thus prefer different\nfilter sizes.\n  This paper proposes a novel Optimized Filter Size CNN (OFS-CNN), where the\nfilter sizes and weights of all convolutional layers are learned simultaneously\nfrom the training data along with learning convolution filters. Specifically,\nthe filter size is defined as a continuous variable, which is optimized by\nminimizing the training loss. Experimental results on four AU-coded databases\nhave shown that the proposed OFS-CNN outperforms traditional CNNs with fixed\nfilter sizes and achieves state-of-the-art recognition performance for AU\nrecognition. Furthermore, the OFS-CNN also beats traditional CNNs using the\nbest filter size obtained by exhaustive search and is capable of estimating\noptimal filter size for varying image resolution.", "authors": ["Shizhong Han", "Zibo Meng", "James O\u0027Reilly", "Jie Cai", "Xiaofeng Wang", "Yan Tong"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1707.08630v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08630v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08630v1", "published_time": "7/26/2017", "rawpid": "1707.08630", "tags": ["cs.CV"], "title": "Optimizing Filter Size in Convolutional Neural Networks for Facial\n  Action Unit Recognition"}, {"abstract": "Matching 3D rigid point clouds in complex environments robustly and\naccurately is still a core technique used in many applications. This paper\nproposes a new architecture combining error estimation from sample covariances\nand dual global probability alignment based on the convolution of adaptive\nGaussian Mixture Models (GMM) from point clouds. Firstly, a novel adaptive GMM\nis defined using probability distributions from the corresponding points. Then\nrigid point cloud alignment is performed by maximizing the global probability\nfrom the convolution of dual adaptive GMMs in the whole 2D or 3D space, which\ncan be efficiently optimized and has a large zone of accurate convergence.\nThousands of trials have been conducted on 200 models from public 2D and 3D\ndatasets to demonstrate superior robustness and accuracy in complex\nenvironments with unpredictable noise, outliers, occlusion, initial rotation,\nshape and missing points.", "authors": ["Can Pu", "Nanbo Li", "Robert B Fisher"], "category": "cs.CV", "comment": "9 pages", "img": "/static/thumbs/1707.08626v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08626v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08626v1", "published_time": "7/26/2017", "rawpid": "1707.08626", "tags": ["cs.CV", "cs.AI", "cs.RO"], "title": "Robust Rigid Point Registration based on Convolution of Adaptive\n  Gaussian Mixture Models"}, {"abstract": "Constrained Local Models (CLMs) are a well-established family of methods for\nfacial landmark detection. However, they have recently fallen out of favor to\ncascaded regression-based approaches. This is in part due to the inability of\nexisting CLM local detectors to model the very complex individual landmark\nappearance that is affected by expression, illumination, facial hair, makeup,\nand accessories. In our work, we present a novel local detector --\nConvolutional Experts Network (CEN) -- that brings together the advantages of\nneural architectures and mixtures of experts in an end-to-end framework. We\nfurther propose a Convolutional Experts Constrained Local Model (CE-CLM)\nalgorithm that uses CEN as local detectors. We demonstrate that our proposed\nCE-CLM algorithm outperforms competitive state-of-the-art baselines for facial\nlandmark detection by a large margin on four publicly-available datasets. Our\napproach is especially accurate and robust on challenging profile images.", "authors": ["Amir Zadeh", "Tadas Baltru\u0161aitis", "Louis-Philippe Morency"], "category": "cs.CV", "comment": "Accepted at CVPR-W 2017", "img": "/static/thumbs/1611.08657v5.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1611.08657v5", "num_discussion": 0, "originally_published_time": "11/26/2016", "pid": "1611.08657v5", "published_time": "7/26/2017", "rawpid": "1611.08657", "tags": ["cs.CV", "cs.AI"], "title": "Convolutional Experts Constrained Local Model for Facial Landmark\n  Detection"}, {"abstract": "Existing marker-less motion capture methods often assume known backgrounds,\nstatic cameras, and sequence specific motion priors, which narrows its\napplication scenarios. Here we propose a fully automatic method that given\nmulti-view video, estimates 3D human motion and body shape. We take recent\nSMPLify \\cite{bogo2016keep} as the base method, and extend it in several ways.\nFirst we fit the body to 2D features detected in multi-view images. Second, we\nuse a CNN method to segment the person in each image and fit the 3D body model\nto the contours to further improves accuracy. Third we utilize a generic and\nrobust DCT temporal prior to handle the left and right side swapping issue\nsometimes introduced by the 2D pose estimator. Validation on standard\nbenchmarks shows our results are comparable to the state of the art and also\nprovide a realistic 3D shape avatar. We also demonstrate accurate results on\nHumanEva and on challenging dance sequences from YouTube in monocular case.", "authors": ["Yinghao Huang", "Federica Bogo", "Christoph Classner", "Angjoo Kanazawa", "Peter V. Gehler", "Ijaz Akhter", "Michael J. Black"], "category": "cs.CV", "comment": "We mistakenly uploaded our paper to arXiv and after some careful\n  consideration we think it\u0027s not c...", "img": "/static/thumbs/1707.07548v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.07548v2", "num_discussion": 0, "originally_published_time": "7/24/2017", "pid": "1707.07548v2", "published_time": "7/26/2017", "rawpid": "1707.07548", "tags": ["cs.CV"], "title": "Towards Accurate Markerless Human Shape and Pose Estimation over Time"}, {"abstract": "In this work we present a technique to use natural language to help\nreinforcement learning generalize to unseen environments. This technique uses\nneural machine translation to learn associations between natural language\nbehavior descriptions and state-action information. We then use this learned\nmodel to guide agent exploration to make it more effective at learning in\nunseen environments. We evaluate this technique using the popular arcade game,\nFrogger, under ideal and non-ideal conditions. This evaluation shows that our\nmodified policy shaping algorithm improves over a Q-learning agent as well as a\nbaseline version of policy shaping.", "authors": ["Brent Harrison", "Upol Ehsan", "Mark O. Riedl"], "category": "cs.AI", "comment": "10 pages, 5 figures", "img": "/static/thumbs/1707.08616v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08616v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08616v1", "published_time": "7/26/2017", "rawpid": "1707.08616", "tags": ["cs.AI", "cs.CL", "cs.LG", "stat.ML"], "title": "Guiding Reinforcement Learning Exploration Using Natural Language"}, {"abstract": "Increasingly, practitioners apply neural networks to complex problems in\nnatural language processing (NLP), such as syntactic parsing, that have rich\noutput structures. Many such applications require deterministic constraints on\nthe output values; for example, requiring that the sequential outputs encode a\nvalid tree. While hidden units might capture such properties, the network is\nnot always able to learn them from the training data alone, and practitioners\nmust then resort to post-processing. In this paper, we present an inference\nmethod for neural networks that enforces deterministic constraints on outputs\nwithout performing post-processing or expensive discrete search over the\nfeasible space. Instead, for each input, we nudge the continuous weights until\nthe network\u0027s unconstrained inference procedure generates an output that\nsatisfies the constraints. We find that our method reduces the number of\nviolating outputs by up to 94%, while improving accuracy in constituency\nparsing.", "authors": ["Jay Yoon Lee", "Michael Wick", "Jean-Baptiste Tristan", "Jaime Carbonell"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1707.08608v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08608v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08608v1", "published_time": "7/26/2017", "rawpid": "1707.08608", "tags": ["cs.CL"], "title": "Enforcing Constraints on Outputs with Unconstrained Inference"}, {"abstract": "When an agent cannot represent a perfectly accurate model of its\nenvironment\u0027s dynamics, model-based reinforcement learning (MBRL) can fail\ncatastrophically. Planning involves composing the predictions of the model;\nwhen flawed predictions are composed, even minor errors can compound and render\nthe model useless for planning. Hallucinated Replay (Talvitie 2014) trains the\nmodel to \"correct\" itself when it produces errors, substantially improving MBRL\nwith flawed models. This paper theoretically analyzes this approach,\nilluminates settings in which it is likely to be effective or ineffective, and\npresents a novel error bound, showing that a model\u0027s ability to self-correct is\nmore tightly related to MBRL performance than one-step prediction error. These\nresults inspire an MBRL algorithm for deterministic MDPs with performance\nguarantees that are robust to model class limitations.", "authors": ["Erik Talvitie"], "category": "cs.LG", "comment": "Original paper appeared in Proceedings of the 31st AAAI Conference on\n  Artificial Intelligence, 201...", "img": "/static/thumbs/1612.06018v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1612.06018v2", "num_discussion": 0, "originally_published_time": "12/19/2016", "pid": "1612.06018v2", "published_time": "7/26/2017", "rawpid": "1612.06018", "tags": ["cs.LG", "cs.AI", "I.2.6; I.2.8"], "title": "Self-Correcting Models for Model-Based Reinforcement Learning"}, {"abstract": "This paper studies Bayesian ranking and selection (R\u0026S) problems with\ncorrelated prior beliefs and continuous domains, i.e. Bayesian optimization\n(BO). Knowledge gradient methods [Frazier et al., 2008, 2009] have been widely\nstudied for discrete R\u0026S problems, which sample the one-step Bayes-optimal\npoint. When used over continuous domains, previous work on the knowledge\ngradient [Scott et al., 2011, Wu and Frazier, 2016, Wu et al., 2017] often rely\non a discretized finite approximation. However, the discretization introduces\nerror and scales poorly as the dimension of domain grows. In this paper, we\ndevelop a fast discretization-free knowledge gradient method for Bayesian\noptimization. Our method is not restricted to the fully sequential setting, but\nuseful in all settings where knowledge gradient can be used over continuous\ndomains. We show how our method can be generalized to handle (i) batch of\npoints suggestion (parallel knowledge gradient); (ii) the setting where\nderivative information is available in the optimization process\n(derivative-enabled knowledge gradient). In numerical experiments, we\ndemonstrate that the discretization-free knowledge gradient method finds global\noptima significantly faster than previous Bayesian optimization algorithms on\nboth synthetic test functions and real-world applications, especially when\nfunction evaluations are noisy; and derivative-enabled knowledge gradient can\nfurther improve the performances, even outperforming the gradient-based\noptimizer such as BFGS when derivative information is available.", "authors": ["Jian Wu", "Peter I. Frazier"], "category": "stat.ML", "comment": "This paper, which combines and extends two conference papers\n  (arXiv:1703.04389, arXiv:1606.04414),...", "img": "/static/thumbs/1707.06541v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.06541v2", "num_discussion": 0, "originally_published_time": "7/20/2017", "pid": "1707.06541v2", "published_time": "7/26/2017", "rawpid": "1707.06541", "tags": ["stat.ML", "cs.AI", "cs.LG", "math.OC", "math.PR"], "title": "Discretization-free Knowledge Gradient Methods for Bayesian Optimization"}, {"abstract": "Pileup involves the contamination of the energy distribution arising from the\nprimary collision of interest (leading vertex) by radiation from soft\ncollisions (pileup). We develop a new technique for removing this contamination\nusing machine learning and convolutional neural networks. The network takes as\ninput the energy distribution of charged leading vertex particles, charged\npileup particles, and all neutral particles and outputs the energy distribution\nof particles coming from leading vertex alone. The PUMML algorithm performs\nremarkably well at eliminating pileup distortion on a wide range of simple and\ncomplex jet observables. We test the robustness of the algorithm in a number of\nways and discuss how the network can be trained directly on data.", "authors": ["Patrick T. Komiske", "Eric M. Metodiev", "Benjamin Nachman", "Matthew D. Schwartz"], "category": "hep-ph", "comment": "16 pages, 6 figures", "img": "/static/thumbs/1707.08600v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08600v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08600v1", "published_time": "7/26/2017", "rawpid": "1707.08600", "tags": ["hep-ph", "hep-ex", "stat.ML"], "title": "Pileup Mitigation with Machine Learning (PUMML)"}, {"abstract": "A main workhorse for statistical learning and signal processing using sparse\nmodels is the least absolute shrinkage and selection operator (Lasso). The\nLasso has been adapted recently for massive network-structured datasets, i.e.,\nbig data over networks. In particular, the network Lasso allows to recover (or\nlearn) graph signals from a small number of noisy signal samples by using the\ntotal variation semi-norm as a regularizer. Some work has been devoted to\nstudying efficient and scalable implementations of the network Lasso. However,\nonly little is known about the conditions on the underlying network structure\nwhich ensure a high accuracy of the network Lasso. By leveraging concepts of\ncompressed sensing, we address this gap and derive precise conditions on the\nunderlying network topology and sampling set which guarantee the network lasso\nto deliver an accurate estimate of the entire underlying graph signal.", "authors": ["Alexander Jung", "Nguyen Tran Quang", "Alexandru Mara"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1704.02107v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.02107v2", "num_discussion": 0, "originally_published_time": "4/7/2017", "pid": "1704.02107v2", "published_time": "7/26/2017", "rawpid": "1704.02107", "tags": ["stat.ML"], "title": "When is Network Lasso Accurate?"}, {"abstract": "We propose a new self-organizing hierarchical softmax formulation for\nneural-network-based language models over large vocabularies. Instead of using\na predefined hierarchical structure, our approach is capable of learning word\nclusters with clear syntactical and semantic meaning during the language model\ntraining process. We provide experiments on standard benchmarks for language\nmodeling and sentence compression tasks. We find that this approach is as fast\nas other efficient softmax approximations, while achieving comparable or even\nbetter performance relative to similar full softmax models.", "authors": ["Yikang Shen", "Shawn Tan", "Chrisopher Pal", "Aaron Courville"], "category": "cs.AI", "comment": "", "img": "/static/thumbs/1707.08588v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08588v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08588v1", "published_time": "7/26/2017", "rawpid": "1707.08588", "tags": ["cs.AI", "cs.CL"], "title": "Self-organized Hierarchical Softmax"}, {"abstract": "Thanks to the recent developments of Convolutional Neural Networks, the\nperformance of face verification methods has increased rapidly. In a typical\nface verification method, feature normalization is a critical step for boosting\nperformance. This motivates us to introduce and study the effect of\nnormalization during training. But we find this is non-trivial, despite\nnormalization being differentiable. We identify and study four issues related\nto normalization through mathematical analysis, which yields understanding and\nhelps with parameter settings. Based on this analysis we propose two strategies\nfor training using normalized features. The first is a modification of softmax\nloss, which optimizes cosine similarity instead of inner-product. The second is\na reformulation of metric learning by introducing an agent vector for each\nclass. We show that both strategies, and small variants, consistently improve\nperformance by between 0.2% to 0.4% on the LFW dataset based on two models.\nThis is significant because the performance of the two models on LFW dataset is\nclose to saturation at over 98%. Codes and models are released on\nhttps://github.com/happynear/NormFace", "authors": ["Feng Wang", "Xiang Xiang", "Jian Cheng", "Alan L. Yuille"], "category": "cs.CV", "comment": "camera-ready version", "img": "/static/thumbs/1704.06369v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.06369v4", "num_discussion": 0, "originally_published_time": "4/21/2017", "pid": "1704.06369v4", "published_time": "7/26/2017", "rawpid": "1704.06369", "tags": ["cs.CV"], "title": "NormFace: L2 Hypersphere Embedding for Face Verification"}, {"abstract": "Recently, increased computational power and data availability, as well as\nalgorithmic advances, have led machine learning techniques to impressive\nresults in regression, classification, data-generation and reinforcement\nlearning tasks. Despite these impressive results, the proximity to the physical\nlimits of chip fabrication alongside the increasing size of datasets are\nmotivating a growing number of researchers to explore the possibility of\nharnessing the power of quantum computation to speed-up classical machine\nlearning algorithms. Here we review the literature in quantum machine learning\nand discuss perspectives for a mixed readership of classical machine learning\nand quantum computation experts. Particular emphasis will be placed on\nclarifying the limitations of quantum algorithms, how they compare with their\nbest classical counterparts and why quantum resources are expected to provide\nadvantages for learning problems. Learning in the presence of noise and certain\ncomputationally hard problems in machine learning are identified as promising\ndirections for the field. Practical questions, like how to upload classical\ndata into quantum form, will also be addressed.", "authors": ["Carlo Ciliberto", "Mark Herbster", "Alessandro Davide Ialongo", "Massimiliano Pontil", "Andrea Rocchetto", "Simone Severini", "Leonard Wossnig"], "category": "quant-ph", "comment": "", "img": "/static/thumbs/1707.08561v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08561v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08561v1", "published_time": "7/26/2017", "rawpid": "1707.08561", "tags": ["quant-ph", "cs.LG", "stat.ML"], "title": "Quantum machine learning: a classical perspective"}, {"abstract": "Sports channel video portals offer an exciting domain for research on\nmultimodal, multilingual analysis. We present methods addressing the problem of\nautomatic video highlight prediction based on joint visual features and textual\nanalysis of the real-world audience discourse with complex slang, in both\nEnglish and traditional Chinese. We present a novel dataset based on League of\nLegends championships recorded from North American and Taiwanese Twitch.tv\nchannels (will be released for further research), and demonstrate strong\nresults on these using multimodal, character-level CNN-RNN model architectures.", "authors": ["Cheng-Yang Fu", "Joon Lee", "Mohit Bansal", "Alexander C. Berg"], "category": "cs.CL", "comment": "EMNLP 2017", "img": "/static/thumbs/1707.08559v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08559v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08559v1", "published_time": "7/26/2017", "rawpid": "1707.08559", "tags": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "title": "Video Highlight Prediction Using Audience Chat Reactions"}, {"abstract": "This paper proposes a practical approach to addressing limitations posed by\nuse of single active electrodes in applications for sleep stage classification.\nElectroencephalography (EEG)-based characterizations of sleep stage progression\ncontribute the diagnosis and monitoring of the many pathologies of sleep.\nSeveral prior reports have explored ways of automating the analysis of sleep\nEEG and of reducing the complexity of the data needed for reliable\ndiscrimination of sleep stages in order to make it possible to perform sleep\nstudies at lower cost in the home (rather than only in specialized clinical\nfacilities). However, these reports have involved recordings from electrodes\nplaced on the cranial vertex or occiput, which can be uncomfortable or\ndifficult for subjects to position. Those that have utilized single EEG\nchannels which contain less sleep information, have showed poor classification\nperformance. We have taken advantage of Rectifier Neural Network for feature\ndetection and Long Short-Term Memory (LSTM) network for sequential data\nlearning to optimize classification performance with single electrode\nrecordings. After exploring alternative electrode placements, we found a\ncomfortable configuration of a single-channel EEG on the forehead and have\nshown that it can be integrated with additional electrodes for simultaneous\nrecording of the electroocuolgram (EOG). Evaluation of data from 62 people\n(with 494 hours sleep) demonstrated better performance of our analytical\nalgorithm for automated sleep classification than existing approaches using\nvertex or occipital electrode placements. Use of this recording configuration\nwith neural network deconvolution promises to make clinically indicated home\nsleep studies practical.", "authors": ["Hao Dong", "Akara Supratak", "Wei Pan", "Chao Wu", "Paul M. Matthews", "Yike Guo"], "category": "q-bio.NC", "comment": "IEEE Transactions on Neural Systems and Rehabilitation Engineering\n  2017", "img": "/static/thumbs/1610.06421v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1610.06421v2", "num_discussion": 0, "originally_published_time": "10/15/2016", "pid": "1610.06421v2", "published_time": "7/26/2017", "rawpid": "1610.06421", "tags": ["q-bio.NC", "cs.CV", "cs.LG", "cs.NE"], "title": "Mixed Neural Network Approach for Temporal Sleep Stage Classification"}, {"abstract": "Current virtual reality (VR) training simulators of liver punctures often\nrely on static 3D patient data and use an unrealistic (sinusoidal) periodic\nanimation of the respiratory movement. Existing methods for the animation of\nbreathing motion support simple mathematical or patient-specific, estimated\nbreathing models. However with personalized breathing models for each new\npatient, a heavily dose relevant or expensive 4D data acquisition is mandatory\nfor keyframe-based motion modeling. Given the reference 4D data, first a model\nbuilding stage using linear regression motion field modeling takes place. Then\nthe methodology shown here allows the transfer of existing reference\nrespiratory motion models of a 4D reference patient to a new static 3D patient.\nThis goal is achieved by using non-linear inter-patient registration to warp\none personalized 4D motion field model to new 3D patient data. This cost- and\ndose-saving new method is shown here visually in a qualitative proof-of-concept\nstudy.", "authors": ["Andre Mastmeyer", "Matthias Wilms", "Heinz Handels"], "category": "cs.CV", "comment": "World Society for Computer Graphics - WSCG 2017 publication, 9 pages,\n  5 figures, 1 movie online", "img": "/static/thumbs/1707.08554v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08554v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08554v1", "published_time": "7/26/2017", "rawpid": "1707.08554", "tags": ["cs.CV"], "title": "Interpatient Respiratory Motion Model Transfer for Virtual Reality\n  Simulations of Liver Punctures"}, {"abstract": "Interpreting gradient methods as fixed-point iterations, we provide a\ndetailed analysis of those methods for minimizing convex objective functions.\nDue to their conceptual and algorithmic simplicity, gradient methods are widely\nused in machine learning for massive datasets (big data). In particular,\nstochastic gradient methods are considered the de- facto standard for training\ndeep neural networks. Studying gradient methods within the realm of fixed-point\ntheory provides us with powerful tools to analyze their convergence properties.\nIn particular, gradient methods using inexact or noisy gradients, such as\nstochastic gradient descent, can be studied conveniently using well-known\nresults on inexact fixed-point iterations. Moreover, as we demonstrate in this\npaper, the fixed-point approach allows an elegant derivation of accelerations\nfor basic gradient methods. In particular, we will show how gradient descent\ncan be accelerated by a fixed-point preserving transformation of an operator\nassociated with the objective function.", "authors": ["Alexander Jung"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1706.09880v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1706.09880v3", "num_discussion": 0, "originally_published_time": "6/29/2017", "pid": "1706.09880v3", "published_time": "7/26/2017", "rawpid": "1706.09880", "tags": ["stat.ML"], "title": "A Fixed-Point of View on Gradient Methods for Big Data"}, {"abstract": "This paper considers a demand response agent that must find a near-optimal\nsequence of decisions based on sparse observations of its environment.\nExtracting a relevant set of features from these observations is a challenging\ntask and may require substantial domain knowledge. One way to tackle this\nproblem is to store sequences of past observations and actions in the state\nvector, making it high dimensional, and apply techniques from deep learning.\nThis paper investigates the capabilities of different deep learning techniques,\nsuch as convolutional neural networks and recurrent neural networks, to extract\nrelevant features for finding near-optimal policies for a residential heating\nsystem and electric water heater that are hindered by sparse observations. Our\nsimulation results indicate that in this specific scenario, feeding sequences\nof time-series to an LSTM network, which is a specific type of recurrent neural\nnetwork, achieved a higher performance than stacking these time-series in the\ninput of a convolutional neural network or deep neural network.", "authors": ["Frederik Ruelens", "Bert J. Claessens", "Peter Vrancx", "Fred Spiessens", "Geert Deconinck"], "category": "cs.LG", "comment": "submitted and waiting review in IEEE transactions on smart grid 2017", "img": "/static/thumbs/1707.08553v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08553v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08553v1", "published_time": "7/26/2017", "rawpid": "1707.08553", "tags": ["cs.LG"], "title": "Direct Load Control of Thermostatically Controlled Loads Based on Sparse\n  Observations Using Deep Reinforcement Learning"}, {"abstract": "This paper describes an implementation of the L-BFGS method designed to deal\nwith two adversarial situations. The first occurs in distributed computing\nenvironments where some of the computational nodes devoted to the evaluation of\nthe function and gradient are unable to return results on time. A similar\nchallenge occurs in a multi-batch approach in which the data points used to\ncompute function and gradients are purposely changed at each iteration to\naccelerate the learning process. Difficulties arise because L-BFGS employs\ngradient differences to update the Hessian approximations, and when these\ngradients are computed using different data points the updating process can be\nunstable. This paper shows how to perform stable quasi-Newton updating in the\nmulti-batch setting, studies the convergence properties for both convex and\nnonconvex functions, and illustrates the behavior of the algorithm in a\ndistributed computing platform on binary classification logistic regression and\nneural network training problems that arise in machine learning.", "authors": ["Albert S. Berahas", "Martin Tak\u00e1\u010d"], "category": "math.OC", "comment": "47 pages, 26 figures. Extension of NIPS 2016 paper: arXiv:1605.06049", "img": "/static/thumbs/1707.08552v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08552v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08552v1", "published_time": "7/26/2017", "rawpid": "1707.08552", "tags": ["math.OC", "cs.LG", "stat.ML"], "title": "A Robust Multi-Batch L-BFGS Method for Machine Learning"}, {"abstract": "This paper describes a builder entry, named \"strawman\", to the sentence-level\nsentiment analysis task of the \"Build It, Break It\" shared task of the First\nWorkshop on Building Linguistically Generalizable NLP Systems. The goal of a\nbuilder is to provide an automated sentiment analyzer that would serve as a\ntarget for breakers whose goal is to find pairs of minimally-differing\nsentences that break the analyzer.", "authors": ["Kyunghyun Cho"], "category": "cs.CL", "comment": "A builder entry to the sentence-level sentiment analysis task of the\n  \"Build It, Break It\" shared t...", "img": "/static/thumbs/1707.08939v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08939v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08939v1", "published_time": "7/26/2017", "rawpid": "1707.08939", "tags": ["cs.CL"], "title": "Strawman: an Ensemble of Deep Bag-of-Ngrams for Sentiment Analysis"}, {"abstract": "Deep learning has enabled major advances in the fields of computer vision,\nnatural language processing, and multimedia among many others. Developing a\ndeep learning system is arduous and complex, as it involves constructing neural\nnetwork architectures, managing training/trained models, tuning optimization\nprocess, preprocessing and organizing data, etc. TensorLayer is a versatile\nPython library that aims at helping researchers and engineers efficiently\ndevelop deep learning systems. It offers rich abstractions for neural networks,\nmodel and data management, and parallel workflow mechanism. While boosting\nefficiency, TensorLayer maintains both performance and scalability. TensorLayer\nwas released in September 2016 on GitHub, and has helped people from academia\nand industry develop real-world applications of deep learning.", "authors": ["Hao Dong", "Akara Supratak", "Luo Mai", "Fangde Liu", "Axel Oehmichen", "Simiao Yu", "Yike Guo"], "category": "cs.AI", "comment": "ACM Multimedia 2017", "img": "/static/thumbs/1707.08551v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08551v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08551v1", "published_time": "7/26/2017", "rawpid": "1707.08551", "tags": ["cs.AI", "cs.DC", "cs.SC"], "title": "TensorLayer: A Versatile Library for Efficient Deep Learning Development"}, {"abstract": "In this paper, we introduce a new distributional method for modeling\npredicate-argument thematic fit judgments. We use a syntax-based DSM to build a\nprototypical representation of verb-specific roles: for every verb, we extract\nthe most salient second order contexts for each of its roles (i.e. the most\nsalient dimensions of typical role fillers), and then we compute thematic fit\nas a weighted overlap between the top features of candidate fillers and role\nprototypes. Our experiments show that our method consistently outperforms a\nbaseline re-implementing a state-of-the-art system, and achieves better or\ncomparable results to those reported in the literature for the other\nunsupervised systems. Moreover, it provides an explicit representation of the\nfeatures characterizing verb-specific semantic roles.", "authors": ["Enrico Santus", "Emmanuele Chersoni", "Alessandro Lenci", "Philippe Blache"], "category": "cs.CL", "comment": "9 pages, 2 figures, 5 tables, EMNLP, 2017, thematic fit, selectional\n  preference, semantic role, DS...", "img": "/static/thumbs/1707.05967v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.05967v2", "num_discussion": 0, "originally_published_time": "7/19/2017", "pid": "1707.05967v2", "published_time": "7/26/2017", "rawpid": "1707.05967", "tags": ["cs.CL"], "title": "Measuring Thematic Fit with Distributional Feature Overlap"}, {"abstract": "This paper introduces Wisture, a new online machine learning solution for\nrecognizing touch-less dynamic hand gestures on a smartphone. Wisture relies on\nthe standard Wi-Fi Received Signal Strength (RSS) using a Long Short-Term\nMemory (LSTM) Recurrent Neural Network (RNN), thresholding filters and traffic\ninduction. Unlike other Wi-Fi based gesture recognition methods, the proposed\nmethod does not require a modification of the smartphone hardware or the\noperating system, and performs the gesture recognition without interfering with\nthe normal operation of other smartphone applications. We discuss the\ncharacteristics of Wisture, and conduct extensive experiments to compare its\nperformance against state-of-the-art machine learning solutions in terms of\nboth accuracy and time efficiency. The experiments include a set of different\nscenarios in terms of both spatial setup and traffic between the smartphone and\nWi-Fi access points (AP). The results show that Wisture achieves an online\nrecognition accuracy of up to 94% (average 78%) in detecting and classifying\nthree hand gestures.", "authors": ["Mohamed Abudulaziz Ali Haseeb", "Ramviyas Parasuraman"], "category": "cs.HC", "comment": "10 pages", "img": "/static/thumbs/1707.08569v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08569v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08569v1", "published_time": "7/26/2017", "rawpid": "1707.08569", "tags": ["cs.HC", "cs.LG", "cs.NI"], "title": "Wisture: RNN-based Learning of Wireless Signals for Gesture Recognition\n  in Unmodified Smartphones"}, {"abstract": "An emerging problem in computer vision is the reconstruction of 3D shape and\npose of an object from a single image. Hitherto, the problem has been addressed\nthrough the application of canonical deep learning methods to regress from the\nimage directly to the 3D shape and pose labels. These approaches, however, are\nproblematic from two perspectives. First, they are minimizing the error between\n3D shapes and pose labels - with little thought about the nature of this label\nerror when reprojecting the shape back onto the image. Second, they rely on the\nonerous and ill-posed task of hand labeling natural images with respect to 3D\nshape and pose. In this paper we define the new task of pose-aware shape\nreconstruction from a single image, and we advocate that cheaper 2D annotations\nof objects silhouettes in natural images can be utilized. We design\narchitectures of pose-aware shape reconstruction which re-project the predicted\nshape back on to the image using the predicted pose. Our evaluation on several\nobject categories demonstrates the superiority of our method for predicting\npose-aware 3D shapes from natural images.", "authors": ["Rui Zhu", "Hamed Kiani Galoogahi", "Chaoyang Wang", "Simon Lucey"], "category": "cs.CV", "comment": "First sub", "img": "/static/thumbs/1707.04682v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.04682v2", "num_discussion": 0, "originally_published_time": "7/15/2017", "pid": "1707.04682v2", "published_time": "7/26/2017", "rawpid": "1707.04682", "tags": ["cs.CV"], "title": "Rethinking Reprojection: Closing the Loop for Pose-aware\n  ShapeReconstruction from a Single Image"}, {"abstract": "Applying generic media-agnostic summarization to music allows for higher\nefficiency in automatic processing, storage, and sharing of datasets, while\nalso alleviating copyright issues. This process has already been proven useful\nin the context of music genre classification. In this paper, we generalize\nconclusions from previous work by evaluating the impact of generic\nsummarization of music from a probabilistic perspective and agnostic relative\nto certain tasks. We estimate Gaussian distributions for original and\nsummarized songs and compute their relative entropy, in order to measure how\nmuch information is lost in the summarization process. Our results suggest that\nrelative entropy is a good predictor of summarization performance and\ntherefore, a good measure of information loss, in the context of tasks relying\non a bag-of-features model. Motivated by this observation, we further propose a\nsimple yet expressive summarization method, based on building summaries that\nminimize relative entropy with respect to the original song, that objectively\noutperforms previous methods and is better suited to avoid copyright issues.", "authors": ["Francisco Raposo", "David Martins de Matos", "Ricardo Ribeiro"], "category": "cs.IR", "comment": "10 pages, 1 algorithm, 3 figures, 8 tables", "img": "/static/thumbs/1612.02350v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1612.02350v3", "num_discussion": 0, "originally_published_time": "12/7/2016", "pid": "1612.02350v3", "published_time": "7/26/2017", "rawpid": "1612.02350", "tags": ["cs.IR", "cs.LG", "cs.SD", "H.5.5"], "title": "An Information-theoretic Approach to Machine-oriented Music\n  Summarization"}, {"abstract": "Identification and counting of cells and mitotic figures is a standard task\nin diagnostic histopathology. Due to the large overall cell count on\nhistological slides and the potential sparse prevalence of some relevant cell\ntypes or mitotic figures, retrieving annotation data for sufficient statistics\nis a tedious task and prone to a significant error in assessment. Automatic\nclassification and segmentation is a classic task in digital pathology, yet it\nis not solved to a sufficient degree.\n  We present a novel approach for cell and mitotic figure classification, based\non a deep convolutional network with an incorporated Spatial Transformer\nNetwork. The network was trained on a novel data set with ten thousand mitotic\nfigures, about ten times more than previous data sets. The algorithm is able to\nderive the cell class (mitotic tumor cells, non-mitotic tumor cells and\ngranulocytes) and their position within an image. The mean accuracy of the\nalgorithm in a five-fold cross-validation is 91.45%.\n  In our view, the approach is a promising step into the direction of a more\nobjective and accurate, semi-automatized mitosis counting supporting the\npathologist.", "authors": ["Marc Aubreville", "Maximilian Krappmann", "Christof Bertram", "Robert Klopfleisch", "Andreas Maier"], "category": "cs.CV", "comment": "5 pages, 4 figures, EG VCBM 2017 contribution", "img": "/static/thumbs/1707.08525v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08525v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08525v1", "published_time": "7/26/2017", "rawpid": "1707.08525", "tags": ["cs.CV"], "title": "A Guided Spatial Transformer Network for Histology Cell Differentiation"}, {"abstract": "At the heart of deep learning we aim to use neural networks as function\napproximators - training them to produce outputs from inputs in emulation of a\nground truth function or data creation process. In many cases we only have\naccess to input-output pairs from the ground truth, however it is becoming more\ncommon to have access to derivatives of the target output with respect to the\ninput - for example when the ground truth function is itself a neural network\nsuch as in network compression or distillation. Generally these target\nderivatives are not computed, or are ignored. This paper introduces Sobolev\nTraining for neural networks, which is a method for incorporating these target\nderivatives in addition the to target values while training. By optimising\nneural networks to not only approximate the function\u0027s outputs but also the\nfunction\u0027s derivatives we encode additional information about the target\nfunction within the parameters of the neural network. Thereby we can improve\nthe quality of our predictors, as well as the data-efficiency and\ngeneralization capabilities of our learned function approximation. We provide\ntheoretical justifications for such an approach as well as examples of\nempirical evidence on three distinct domains: regression on classical\noptimisation datasets, distilling policies of an agent playing Atari, and on\nlarge-scale applications of synthetic gradients. In all three domains the use\nof Sobolev Training, employing target derivatives in addition to target values,\nresults in models with higher accuracy and stronger generalisation.", "authors": ["Wojciech Marian Czarnecki", "Simon Osindero", "Max Jaderberg", "Grzegorz \u015awirszcz", "Razvan Pascanu"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1706.04859v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1706.04859v3", "num_discussion": 0, "originally_published_time": "6/15/2017", "pid": "1706.04859v3", "published_time": "7/26/2017", "rawpid": "1706.04859", "tags": ["cs.LG"], "title": "Sobolev Training for Neural Networks"}, {"abstract": "For years, recursive neural networks (RvNNs) have shown to be suitable for\nrepresenting text into fixed-length vectors and achieved good performance on\nseveral natural language processing tasks. However, the main drawback of RvNN\nis that it requires explicit tree structure (e.g. parse tree), which makes data\npreparation and model implementation hard. In this paper, we propose a novel\ntree-structured long short-term memory (Tree-LSTM) architecture that\nefficiently learns how to compose task-specific tree structures only from plain\ntext data. To achieve this property, our model uses Straight-Through (ST)\nGumbel-Softmax estimator to decide the parent node among candidates and to\ncalculate gradients of the discrete decision. We evaluate the proposed model on\nnatural language interface and sentiment analysis and show that our model\noutperforms or at least comparable to previous Tree-LSTM-based works.\nEspecially in the natural language interface task, our model establishes the\nnew state-of-the-art accuracy of 85.4%. We also find that our model converges\nsignificantly faster and needs less memory than other models of complex\nstructures.", "authors": ["Jihun Choi", "Kang Min Yoo", "Sang-goo Lee"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1707.02786v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.02786v2", "num_discussion": 0, "originally_published_time": "7/10/2017", "pid": "1707.02786v2", "published_time": "7/26/2017", "rawpid": "1707.02786", "tags": ["cs.CL"], "title": "Unsupervised Learning of Task-Specific Tree Structures with Tree-LSTMs"}, {"abstract": "Forensic science suffers from a lack of studies with high-quality design,\nsuch as randomized controlled trials (RCT). Evidence in forensic science may be\nof insufficient quality, which is a major concern. Results from RCT are\ncriticized for providing artificial results that are not useful in real life\nand unfit for individualized prescription. Various sources of collected data\n(e.g. data collected in routine practice) could be exploited for distinct\ngoals. Obstacles remain before such data can be practically accessed and used,\nincluding technical issues. We present an easy-to-use software dedicated to\ninnovative data analyses for practitioners and researchers. We provide 2\nexamples in forensics. Spe3dLab has been developed by 3 French teams: a\nbioinformatics laboratory (LaTIM), a private partner (Tekliko) and a department\nof forensic medicine (Jean Verdier Hospital). It was designed to be open\nsource, relying on documented and maintained libraries, query-oriented and\ncapable of handling the entire data process from capture to export of best\npredictive models for their integration in information systems. Spe3dLab was\nused for 2 specific forensics applications: i) the search for multiple causal\nfactors and ii) the best predictive model of the functional impairment (total\nincapacity to work, TIW) of assault survivors. 2,892 patients were included\nover a 6-month period. Time to evaluation was the only direct cause identified\nfor TIW, and victim category was an indirect cause. The specificity and\nsensitivity of the predictive model were 99.9% and 90%, respectively. Spe3dLab\nis a quick and efficient tool for accessing observational, routinely collected\ndata and performing innovative analyses. Analyses can be exported for\nvalidation and routine use by practitioners, e.g., for computer-aided\nevaluation of complex problems. It can provide a fully integrated solution for\nindividualized medicine.", "authors": ["Vincent Laugier", "Eric Stindel", "Alcibiade Lichterowicz", "S\u00e9verine Ansart", "Thomas Lef\u00e8vre"], "category": "cs.CY", "comment": "15 pages, 3 tables, 5 figures", "img": "/static/thumbs/1707.08454v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08454v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08454v1", "published_time": "7/26/2017", "rawpid": "1707.08454", "tags": ["cs.CY", "cs.AI"], "title": "Making the best of data derived from a daily practice in clinical legal\n  medicine for research and practice - the example of Spe3dLab"}, {"abstract": "Bayesian nonparametrics are a class of probabilistic models in which the\nmodel size is inferred from data. A recently developed methodology in this\nfield is small-variance asymptotic analysis, a mathematical technique for\nderiving learning algorithms that capture much of the flexibility of Bayesian\nnonparametric inference algorithms, but are simpler to implement and less\ncomputationally expensive. Past work on small-variance analysis of Bayesian\nnonparametric inference algorithms has exclusively considered batch models\ntrained on a single, static dataset, which are incapable of capturing time\nevolution in the latent structure of the data. This work presents a\nsmall-variance analysis of the maximum a posteriori filtering problem for a\ntemporally varying mixture model with a Markov dependence structure, which\ncaptures temporally evolving clusters within a dataset. Two clustering\nalgorithms result from the analysis: D-Means, an iterative clustering algorithm\nfor linearly separable, spherical clusters; and SD-Means, a spectral clustering\nalgorithm derived from a kernelized, relaxed version of the clustering problem.\nEmpirical results from experiments demonstrate the advantages of using D-Means\nand SD-Means over contemporary clustering algorithms, in terms of both\ncomputational cost and clustering accuracy.", "authors": ["Trevor Campbell", "Brian Kulis", "Jonathan How"], "category": "stat.ML", "comment": "27 pages", "img": "/static/thumbs/1707.08493v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08493v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08493v1", "published_time": "7/26/2017", "rawpid": "1707.08493", "tags": ["stat.ML"], "title": "Dynamic Clustering Algorithms via Small-Variance Analysis of Markov\n  Chain Mixture Models"}, {"abstract": "Domain adaptation is an important open problem in deep reinforcement learning\n(RL). In many scenarios of interest data is hard to obtain, so agents may learn\na source policy in a setting where data is readily available, with the hope\nthat it generalises well to the target domain. We propose a new multi-stage RL\nagent, DARLA (DisentAngled Representation Learning Agent), which learns to see\nbefore learning to act. DARLA\u0027s vision is based on learning a disentangled\nrepresentation of the observed environment. Once DARLA can see, it is able to\nacquire source policies that are robust to many domain shifts - even with no\naccess to the target domain. DARLA significantly outperforms conventional\nbaselines in zero-shot domain adaptation scenarios, an effect that holds across\na variety of RL environments (Jaco arm, DeepMind Lab) and base RL algorithms\n(DQN, A3C and EC).", "authors": ["Irina Higgins", "Arka Pal", "Andrei A. Rusu", "Loic Matthey", "Christopher P Burgess", "Alexander Pritzel", "Matthew Botvinick", "Charles Blundell", "Alexander Lerchner"], "category": "stat.ML", "comment": "ICML 2017", "img": "/static/thumbs/1707.08475v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08475v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08475v1", "published_time": "7/26/2017", "rawpid": "1707.08475", "tags": ["stat.ML", "cs.AI", "cs.LG"], "title": "DARLA: Improving Zero-Shot Transfer in Reinforcement Learning"}, {"abstract": "Dictionary learning and component analysis are part of one of the most\nwell-studied and active research fields, at the intersection of signal and\nimage processing, computer vision, and statistical machine learning. In\ndictionary learning, the current methods of choice are arguably K-SVD and its\nvariants, which learn a dictionary (i.e., a decomposition) for sparse coding\nvia Singular Value Decomposition. In robust component analysis, leading methods\nderive from Principal Component Pursuit (PCP), which recovers a low-rank matrix\nfrom sparse corruptions of unknown magnitude and support. However, K-SVD is\nsensitive to the presence of noise and outliers in the training set.\nAdditionally, PCP does not provide a dictionary that respects the structure of\nthe data (e.g., images), and requires expensive SVD computations when solved by\nconvex relaxation. In this paper, we introduce a new robust decomposition of\nimages by combining ideas from sparse dictionary learning and PCP. We propose a\nnovel Kronecker-decomposable component analysis which is robust to gross\ncorruption, can be used for low-rank modeling, and leverages separability to\nsolve significantly smaller problems. We design an efficient learning algorithm\nby drawing links with a restricted form of tensor factorization. The\neffectiveness of the proposed approach is demonstrated on real-world\napplications, namely background subtraction and image denoising, by performing\na thorough comparison with the current state of the art.", "authors": ["Mehdi Bahri", "Yannis Panagakis", "Stefanos Zafeiriou"], "category": "stat.ML", "comment": "Accepted for publication at ICCV 2017", "img": "/static/thumbs/1703.07886v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.07886v2", "num_discussion": 0, "originally_published_time": "3/22/2017", "pid": "1703.07886v2", "published_time": "7/26/2017", "rawpid": "1703.07886", "tags": ["stat.ML", "cs.CV"], "title": "Robust Kronecker-Decomposable Component Analysis for Low-Rank Modeling"}, {"abstract": "Over the years, Twitter has become one of the largest communication platforms\nproviding key data to various applications such as brand monitoring, trend\ndetection, among others. Entity linking is one of the major tasks in natural\nlanguage understanding from tweets and it associates entity mentions in text to\ncorresponding entries in knowledge bases in order to provide unambiguous\ninterpretation and additional con- text. State-of-the-art techniques have\nfocused on linking explicitly mentioned entities in tweets with reasonable\nsuccess. However, we argue that in addition to explicit mentions i.e. The movie\nGravity was more ex- pensive than the mars orbiter mission entities (movie\nGravity) can also be mentioned implicitly i.e. This new space movie is crazy.\nyou must watch it!. This paper introduces the problem of implicit entity\nlinking in tweets. We propose an approach that models the entities by\nexploiting their factual and contextual knowledge. We demonstrate how to use\nthese models to perform implicit entity linking on a ground truth dataset with\n397 tweets from two domains, namely, Movie and Book. Specifically, we show: 1)\nthe importance of linking implicit entities and its value addition to the\nstandard entity linking task, and 2) the importance of exploiting contextual\nknowledge associated with an entity for linking their implicit mentions. We\nalso make the ground truth dataset publicly available to foster the research in\nthis new research area.", "authors": ["Sujan Perera", "Pablo N. Mendes", "Adarsh Alex", "Amit Sheth", "Krishnaprasad Thirunarayan"], "category": "cs.CL", "comment": "This paper was accepted at the Extended Semantic Web Conference 2016\n  as a full research track pape...", "img": "/static/thumbs/1707.08470v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08470v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08470v1", "published_time": "7/26/2017", "rawpid": "1707.08470", "tags": ["cs.CL", "cs.AI"], "title": "Implicit Entity Linking in Tweets"}, {"abstract": "We present a matrix-factorization algorithm that scales to input matrices\nwith both huge number of rows and columns. Learned factors may be sparse or\ndense and/or non-negative, which makes our algorithm suitable for dictionary\nlearning, sparse component analysis, and non-negative matrix factorization. Our\nalgorithm streams matrix columns while subsampling them to iteratively learn\nthe matrix factors. At each iteration, the row dimension of a new sample is\nreduced by subsampling, resulting in lower time complexity compared to a simple\nstreaming algorithm. Our method comes with convergence guarantees to reach a\nstationary point of the matrix-factorization problem. We demonstrate its\nefficiency on massive functional Magnetic Resonance Imaging data (2 TB), and on\npatches extracted from hyperspectral images (103 GB). For both problems, which\ninvolve different penalties on rows and columns, we obtain significant\nspeed-ups compared to state-of-the-art algorithms.", "authors": ["Arthur Mensch", "Julien Mairal", "Bertrand Thirion", "Gael Varoquaux"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1701.05363v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1701.05363v2", "num_discussion": 0, "originally_published_time": "1/19/2017", "pid": "1701.05363v2", "published_time": "7/26/2017", "rawpid": "1701.05363", "tags": ["stat.ML", "cs.LG", "math.OC", "q-bio.NC"], "title": "Stochastic Subsampling for Factorizing Huge Matrices"}, {"abstract": "We present a quantitative analysis of human word association pairs and study\nthe types of relations presented in the associations. We put our main focus on\nthe correlation between response types and respondent characteristics such as\noccupation and gender by contrasting syntagmatic and paradigmatic associations.\nFinally, we propose a personalised distributed word association model and show\nthe importance of incorporating demographic factors into the models commonly\nused in natural language processing.", "authors": ["Ekaterina Vylomova", "Andrei Shcherbakov", "Yuriy Philippovich", "Galina Cherkasova"], "category": "cs.CL", "comment": "AIST 2017 camera-ready", "img": "/static/thumbs/1707.08458v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08458v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08458v1", "published_time": "7/26/2017", "rawpid": "1707.08458", "tags": ["cs.CL"], "title": "Men Are from Mars, Women Are from Venus: Evaluation and Modelling of\n  Verbal Associations"}, {"abstract": "Manual annotations of temporal bounds for object interactions (i.e. start and\nend times) are typical training input to recognition, localization and\ndetection algorithms. For three publicly available egocentric datasets, we\nuncover inconsistencies in ground truth temporal bounds within and across\nannotators and datasets. We systematically assess the robustness of\nstate-of-the-art approaches to changes in labeled temporal bounds, for object\ninteraction recognition. As boundaries are trespassed, a drop of up to 10% is\nobserved for both Improved Dense Trajectories and Two-Stream Convolutional\nNeural Network.\n  We demonstrate that such disagreement stems from a limited understanding of\nthe distinct phases of an action, and propose annotating based on the Rubicon\nBoundaries, inspired by a similarly named cognitive model, for consistent\ntemporal bounds of object interactions. Evaluated on a public dataset, we\nreport a 4% increase in overall accuracy, and an increase in accuracy for 55%\nof classes when Rubicon Boundaries are used for temporal annotations.", "authors": ["Davide Moltisanti", "Michael Wray", "Walterio Mayol-Cuevas", "Dima Damen"], "category": "cs.CV", "comment": "ICCV 2017", "img": "/static/thumbs/1703.09026v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.09026v2", "num_discussion": 0, "originally_published_time": "3/27/2017", "pid": "1703.09026v2", "published_time": "7/26/2017", "rawpid": "1703.09026", "tags": ["cs.CV"], "title": "Trespassing the Boundaries: Labeling Temporal Bounds for Object\n  Interactions in Egocentric Video"}, {"abstract": "Many of the recent approaches to polyphonic piano note onset transcription\nrequire training a machine learning model on a large piano database. However,\nsuch approaches are limited by dataset availability; additional training data\nis difficult to produce, and proposed systems often perform poorly on novel\nrecording conditions. We propose a method to quickly synthesize arbitrary\nquantities of training data, avoiding the need for curating large datasets.\nVarious aspects of piano note dynamics - including nonlinearity of note\nsignatures with velocity, different articulations, temporal clustering of\nonsets, and nonlinear note partial interference - are modeled to match the\ncharacteristics of real pianos. Our method also avoids the disentanglement\nproblem, a recently noted issue affecting machine-learning based approaches. We\ntrain a feed-forward neural network with two hidden layers on our generated\ntraining data and achieve both good transcription performance on the large MAPS\npiano dataset and excellent generalization qualities.", "authors": ["Samuel Li"], "category": "stat.ML", "comment": "Comments are welcome", "img": "/static/thumbs/1707.08438v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08438v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08438v1", "published_time": "7/26/2017", "rawpid": "1707.08438", "tags": ["stat.ML", "cs.SD"], "title": "Context-Independent Polyphonic Piano Onset Transcription with an\n  Infinite Training Dataset"}, {"abstract": "In Multimodal Neural Machine Translation (MNMT), a neural model generates a\ntranslated sentence that describes an image, given the image itself and one\nsource descriptions in English. This is considered as the multimodal image\ncaption translation task. The images are processed with Convolutional Neural\nNetwork (CNN) to extract visual features exploitable by the translation model.\nSo far, the CNNs used are pre-trained on object detection and localization\ntask. We hypothesize that richer architecture, such as dense captioning models,\nmay be more suitable for MNMT and could lead to improved translations. We\nextend this intuition to the word-embeddings, where we compute both linguistic\nand visual representation for our corpus vocabulary. We combine and compare\ndifferent confi", "authors": ["Jean-Benoit Delbrouck", "St\u00e9phane Dupont", "Omar Seddati"], "category": "cs.CL", "comment": "Accepted to GLU 2017. arXiv admin note: text overlap with\n  arXiv:1707.00995", "img": "/static/thumbs/1707.01009v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.01009v4", "num_discussion": 0, "originally_published_time": "7/4/2017", "pid": "1707.01009v4", "published_time": "7/26/2017", "rawpid": "1707.01009", "tags": ["cs.CL"], "title": "Visually Grounded Word Embeddings and Richer Visual Features for\n  Improving Multimodal Neural Machine Translation"}, {"abstract": "Many settings require a decision maker to repeatedly choose from a set of\ninterventions to apply to an individual without knowing the interventions\u0027\nefficacy a priori. However, repeated application of a specific intervention may\nreduce its efficacy, while abstaining from applying an intervention may cause\nits efficacy to recover. Such phenomena are observed in many real world\nsettings such as personalized healthcare-adherence improving interventions and\ntargeted online advertising. Though finding an optimal intervention policy for\nmodels with this structure is PSPACE-complete, we propose and analyze a new\nclass of models called ROGUE (Reducing or Gaining Unknown Efficacy) bandits,\nwhich we show in this paper can capture these phenomena and can be efficiently\nsolved. We first present a consistent maximum likelihood approach to estimate\nthe parameters of these models, and conduct a statistical analysis to construct\nfinite sample concentration bounds. These statistical bounds are used to derive\nan upper confidence bound strategy that we call the ROGUE Upper Confidence\nBound (ROGUE-UCB) algorithm. Our theoretical analysis shows that the ROGUE-UCB\nalgorithm achieves logarithmic in time regret, unlike existing algorithms which\nresult in linear regret. We conclude with a numerical experiment using real\nworld data from a personalized healthcare-adherence improving intervention to\nincrease physical activity. Here, the goal is to optimize the selection of\nmessages (e.g., confidence increasing vs. knowledge increasing) to send to each\nindividual each day to increase adherence and physical activity. Our results\nshow that ROGUE-UCB performs better in terms of aggregated regret and average\nreward when compared to state of the art algorithms, and the use of ROGUE-UCB\nincreases daily step counts by roughly 1,000 steps a day (about a half-mile\nmore of walking) as compared to other algorithms.", "authors": ["Yonatan Mintz", "Anil Aswani", "Philip Kaminsky", "Elena Flowers", "Yoshimi Fukuoka"], "category": "math.OC", "comment": "", "img": "/static/thumbs/1707.08423v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08423v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08423v1", "published_time": "7/26/2017", "rawpid": "1707.08423", "tags": ["math.OC", "cs.LG"], "title": "Non-Stationary Bandits with Habituation and Recovery Dynamics"}, {"abstract": "Nowadays, there are many approaches designed for the task of detecting\ncommunities in social networks. Among them, some methods only consider the\ntopological graph structure, while others take use of both the graph structure\nand the node attributes. In real-world networks, there are many uncertain and\nnoisy attributes in the graph. In this paper, we will present how we detect\ncommunities in graphs with uncertain attributes in the first step. The\nnumerical, probabilistic as well as evidential attributes are generated\naccording to the graph structure. In the second step, some noise will be added\nto the attributes. We perform experiments on graphs with different types of\nattributes and compare the detection results in terms of the Normalized Mutual\nInformation (NMI) values. The experimental results show that the clustering\nwith evidential attributes gives better results comparing to those with\nprobabilistic and numerical attributes. This illustrates the advantages of\nevidential attributes.", "authors": ["Salma Ben Dhaou", "Kuang Zhou", "Mouloud Kharoune", "Arnaud Martin", "Boutheina Ben Yaghlane"], "category": "cs.AI", "comment": "20th International Conference on Information Fusion, Jul 2017, Xi\u0027an,\n  China", "img": "/static/thumbs/1707.08418v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08418v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08418v1", "published_time": "7/26/2017", "rawpid": "1707.08418", "tags": ["cs.AI", "cs.SI"], "title": "The Advantage of Evidential Attributes in Social Networks"}, {"abstract": "This paper addresses the challenge of 3D human pose estimation from a single\ncolor image. Despite the general success of the end-to-end learning paradigm,\ntop performing approaches employ a two-step solution consisting of a\nConvolutional Network (ConvNet) for 2D joint localization and a subsequent\noptimization step to recover 3D pose. In this paper, we identify the\nrepresentation of 3D pose as a critical issue with current ConvNet approaches\nand make two important contributions towards validating the value of end-to-end\nlearning for this task. First, we propose a fine discretization of the 3D space\naround the subject and train a ConvNet to predict per voxel likelihoods for\neach joint. This creates a natural representation for 3D pose and greatly\nimproves performance over the direct regression of joint coordinates. Second,\nto further improve upon initial estimates, we employ a coarse-to-fine\nprediction scheme. This step addresses the large dimensionality increase and\nenables iterative refinement and repeated processing of the image features. The\nproposed approach outperforms all state-of-the-art methods on standard\nbenchmarks achieving a relative error reduction greater than 30% on average.\nAdditionally, we investigate using our volumetric representation in a related\narchitecture which is suboptimal compared to our end-to-end approach, but is of\npractical interest, since it enables training when no image with corresponding\n3D groundtruth is available, and allows us to present compelling results for\nin-the-wild images.", "authors": ["Georgios Pavlakos", "Xiaowei Zhou", "Konstantinos G. Derpanis", "Kostas Daniilidis"], "category": "cs.CV", "comment": "CVPR 2017 Camera Ready. Project Page:\n  https://www.seas.upenn.edu/~pavlakos/projects/volumetric/", "img": "/static/thumbs/1611.07828v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1611.07828v2", "num_discussion": 0, "originally_published_time": "11/23/2016", "pid": "1611.07828v2", "published_time": "7/26/2017", "rawpid": "1611.07828", "tags": ["cs.CV"], "title": "Coarse-to-Fine Volumetric Prediction for Single-Image 3D Human Pose"}, {"abstract": "In the last two decades Computer Aided Diagnostics (CAD) systems were\ndeveloped to help radiologists analyze screening mammograms. The benefits of\ncurrent CAD technologies appear to be contradictory and they should be improved\nto be ultimately considered useful. Since 2012 deep convolutional neural\nnetworks (CNN) have been a tremendous success in image recognition, reaching\nhuman performance. These methods have greatly surpassed the traditional\napproaches, which are similar to currently used CAD solutions. Deep CNN-s have\nthe potential to revolutionize medical image analysis. We propose a CAD system\nbased on one of the most successful object detection frameworks, Faster R-CNN.\nThe system detects and classifies malignant or benign lesions on a mammogram\nwithout any human intervention. Our approach described here has achieved the\n2nd place in the Digital Mammography DREAM Challenge with $ AUC = 0.85 $. The\nproposed method also sets the state of the art classification performance on\nthe public INbreast database, $ AUC = 0.95$. When used as a detector, the\nsystem reaches high sensitivity with very few false positive marks per image on\nthe INbreast dataset.", "authors": ["Dezs\u0151 Ribli", "Anna Horv\u00e1th", "Zsuzsa Unger", "P\u00e9ter Pollner", "Istv\u00e1n Csabai"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1707.08401v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08401v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08401v1", "published_time": "7/26/2017", "rawpid": "1707.08401", "tags": ["cs.CV"], "title": "Detecting and classifying lesions in mammograms with Deep Learning"}, {"abstract": "Trace norm regularization is a widely used approach for learning low rank\nmatrices. A standard optimization strategy is based on formulating the problem\nas one of low rank matrix factorization which, however, leads to a non-convex\nproblem. In practice this approach works well, and it is often computationally\nfaster than standard convex solvers such as proximal gradient methods.\nNevertheless, it is not guaranteed to converge to a global optimum, and the\noptimization can be trapped at poor stationary points. In this paper we show\nthat it is possible to characterize all critical points of the non-convex\nproblem. This allows us to provide an efficient criterion to determine whether\na critical point is also a global minimizer. Our analysis suggests an iterative\nmeta-algorithm that dynamically expands the parameter space and allows the\noptimization to escape any non-global critical point, thereby converging to a\nglobal minimizer. The algorithm can be applied to problems such as matrix\ncompletion or multitask learning, and our analysis holds for any random\ninitialization of the factor matrices. Finally, we confirm the good performance\nof the algorithm on synthetic and real datasets.", "authors": ["Carlo Ciliberto", "Dimitris Stamos", "Massimiliano Pontil"], "category": "cs.LG", "comment": "22 pages, 4 figures, 1 Table", "img": "/static/thumbs/1706.08934v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1706.08934v2", "num_discussion": 0, "originally_published_time": "6/27/2017", "pid": "1706.08934v2", "published_time": "7/26/2017", "rawpid": "1706.08934", "tags": ["cs.LG", "stat.ML"], "title": "Reexamining Low Rank Matrix Factorization for Trace Norm Regularization"}, {"abstract": "A lot of search approaches have been explored for the selection of features\nin pattern classification domain in order to discover significant subset of the\nfeatures which produces better accuracy. In this paper, we introduced a Harmony\nSearch (HS) algorithm based feature selection method for feature dimensionality\nreduction in handwritten Bangla word recognition problem. This algorithm has\nbeen implemented to reduce the feature dimensionality of a technique described\nin one of our previous papers by S. Bhowmik et al.[1]. In the said paper, a set\nof 65 elliptical features were computed for handwritten Bangla word recognition\npurpose and a recognition accuracy of 81.37% was achieved using Multi Layer\nPerceptron (MLP) classifier. In the present work, a subset containing 48\nfeatures (approximately 75% of said feature vector) has been selected by HS\nbased wrapper feature selection method which produces an accuracy rate of\n90.29%. Reasonable outcomes also validates that the introduced algorithm\nutilizes optimal number of features while showing higher classification\naccuracies when compared to two standard evolutionary algorithms like Genetic\nAlgorithm (GA), Particle Swarm Optimization (PSO) and statistical feature\ndimensionality reduction technique like Principal Component Analysis (PCA).\nThis confirms the suitability of HS algorithm to the holistic handwritten word\nrecognition problem.", "authors": ["Supratim Das", "Pawan Kumar Singh", "Showmik Bhowmik", "Ram Sarkar", "Mita Nasipuri"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1707.08398v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08398v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08398v1", "published_time": "7/26/2017", "rawpid": "1707.08398", "tags": ["cs.CV", "68T10"], "title": "A Harmony Search Based Wrapper Feature Selection Method for Holistic\n  Bangla word Recognition"}, {"abstract": "Optoacoustic (photoacoustic) tomography reconstructs maps of the initial\npressure rise induced by the absorption of light pulses in tissue. In practice,\ndue to inaccurate assumptions in the forward model employed, noise and other\nexperimental factors, the images often contain errors, occasionally manifested\nas negative values. We present optoacoustic tomography based on an entropy\nmaximization algorithm that uses logarithmic regularization as a potent method\nfor imparting non-negative image reconstruction. We experimentally investigate\nthe performance achieved by the entropy maximization scheme on phantoms and in\nvivo samples. The findings demonstrate that the proposed scheme reconstructs\nphysically relevant image values devoid of unwanted negative contrast, thus\nimproving quantitative imaging performance.", "authors": ["Jaya Prakash", "Subhamoy Mandal", "Daniel Razansky", "Vasilis Ntziachristos"], "category": "physics.med-ph", "comment": "Currently under consideration for publication by IEEE", "img": "/static/thumbs/1707.08391v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08391v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08391v1", "published_time": "7/26/2017", "rawpid": "1707.08391", "tags": ["physics.med-ph", "cs.CV"], "title": "Maximum entropy based non-negative optoacoustic tomographic image\n  reconstruction"}, {"abstract": "Augmented accuracy in prediction of diabetes will open up new frontiers in\nhealth prognostics. Data overfitting is a performance-degrading issue in\ndiabetes prognosis. In this study, a prediction system for the disease of\ndiabetes is pre-sented where the issue of overfitting is minimized by using the\ndropout method. Deep learning neural network is used where both fully connected\nlayers are fol-lowed by dropout layers. The output performance of the proposed\nneural network is shown to have outperformed other state-of-art methods and it\nis recorded as by far the best performance for the Pima Indians Diabetes Data\nSet.", "authors": ["Akm Ashiquzzaman", "Abdul Kawsar Tushar", "Md. Rashedul Islam", "Jong-Myon Kim"], "category": "cs.CV", "comment": "8 pages, 3 Figures, 3 Tables; Conference - 7th iCatse International\n  Conference on IT Convergence a...", "img": "/static/thumbs/1707.08386v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08386v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08386v1", "published_time": "7/26/2017", "rawpid": "1707.08386", "tags": ["cs.CV"], "title": "Reduction of Overfitting in Diabetes Prediction Using Deep Learning\n  Neural Network"}, {"abstract": "Increased accuracy in predictive models for handwritten character recognition\nwill open up new frontiers for optical character recognition. Major drawbacks\nof predictive machine learning models are headed by the elongated training time\ntaken by some models, and the requirement that training and test data be in the\nsame feature space and consist of the same distribution. In this study, these\nobstacles are minimized by presenting a model for transferring knowledge from\none task to another. This model is presented for the recognition of handwritten\nnumerals in Indic languages. The model utilizes convolutional neural networks\nwith backpropagation for error reduction and dropout for data overfitting. The\noutput performance of the proposed neural network is shown to have closely\nmatched other state-of-the-art methods using only a fraction of time used by\nthe state-of-the-arts.", "authors": ["Abdul Kawsar Tushar", "Akm Ashiquzzaman", "Afia Afrin", "Md. Rashedul Islam"], "category": "cs.CV", "comment": "10 pages; 2 figures, 4 tables; conference - International Conference\n  On Computational Vision and B...", "img": "/static/thumbs/1707.08385v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08385v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08385v1", "published_time": "7/26/2017", "rawpid": "1707.08385", "tags": ["cs.CV"], "title": "A Novel Transfer Learning Approach upon Hindi, Arabic, and Bangla\n  Numerals using Convolutional Neural Networks"}, {"abstract": "In this article, we consider a stochastic numerical simulator to assess the\nimpact of some factors on a phenomenon. The simulator is seen as a black box\nwith inputs and outputs. The quality of a simulation, hereafter referred to as\nfidelity, is assumed to be tunable by means of an additional input of the\nsimulator (e.g., a mesh size parameter): high-fidelity simulations provide more\naccurate results, but are time-consuming. Using a limited computation-time\nbudget, we want to estimate, for any value of the physical inputs, the\nprobability that a certain scalar output of the simulator will exceed a given\ncritical threshold at the highest fidelity level. The problem is addressed in a\nBayesian framework, using a Gaussian process model of the multi-fidelity\nsimulator. We consider a Bayesian estimator of the probability, together with\nan associated measure of uncertainty, and propose a new multi-fidelity\nsequential design strategy, called Maximum Speed of Uncertainty Reduction\n(MSUR), to select the value of physical inputs and the fidelity level of new\nsimulations. The MSUR strategy is tested on an example.", "authors": ["R\u00e9mi Stroh", "S\u00e9verine Demeyer", "Nicolas Fischer", "Julien Bect", "Emmanuel Vazquez"], "category": "stat.CO", "comment": "61th World Statistics Congress of the International Statistical\n  Institute (ISI 2017), Jul 2017, Ma...", "img": "/static/thumbs/1707.08384v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08384v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08384v1", "published_time": "7/26/2017", "rawpid": "1707.08384", "tags": ["stat.CO", "stat.ME", "stat.ML"], "title": "Sequential design of experiments to estimate a probability of exceeding\n  a threshold in a multi-fidelity stochastic simulator"}, {"abstract": "A deep neural network based architecture was constructed to predict amino\nacid side chain conformation with unprecedented accuracy. Amino acid side chain\nconformation prediction is essential for protein homology modeling and protein\ndesign. Current widely-adopted methods use physics-based energy functions to\nevaluate side chain conformation. Here, using a deep neural network\narchitecture without physics-based assumptions, we have demonstrated that side\nchain conformation prediction accuracy can be improved by more than 25%,\nespecially for aromatic residues compared with current standard methods. More\nstrikingly, the prediction method presented here is robust enough to identify\nindividual conformational outliers from high resolution structures in a protein\ndata bank without providing its structural factors. We envisage that our amino\nacid side chain predictor could be used as a quality check step for future\nprotein structure model validation and many other potential applications such\nas side chain assignment in Cryo-electron microscopy, crystallography model\nauto-building, protein folding and small molecule ligand docking.", "authors": ["Ke Liu", "Xiangyan Sun", "Jun Ma", "Zhenyu Zhou", "Qilin Dong", "Shengwen Peng", "Junqiu Wu", "Suocheng Tan", "G\u00fcnter Blobel", "Jie Fan"], "category": "q-bio.BM", "comment": "", "img": "/static/thumbs/1707.08381v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08381v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08381v1", "published_time": "7/26/2017", "rawpid": "1707.08381", "tags": ["q-bio.BM", "cs.LG", "stat.ML"], "title": "Prediction of amino acid side chain conformation using a deep neural\n  network"}, {"abstract": "The arrangement of products in store shelves is carefully planned to maximize\nsales and keep customers happy. However, verifying compliance of real shelves\nto the ideal layout is a costly task routinely performed by the store\npersonnel. In this paper, we propose a computer vision pipeline to recognize\nproducts on shelves and verify compliance to the planned layout. We deploy\nlocal invariant features together with a novel formulation of the product\nrecognition problem as a sub-graph isomorphism between the items appearing in\nthe given image and the ideal layout. This allows for auto-localizing the given\nimage within the aisle or store and improving recognition dramatically.", "authors": ["Alessio Tonioni", "Luigi Di Stefano"], "category": "cs.CV", "comment": "Slightly extended version of the paper accepted at ICIAP 2017", "img": "/static/thumbs/1707.08378v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08378v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08378v1", "published_time": "7/26/2017", "rawpid": "1707.08378", "tags": ["cs.CV"], "title": "Product recognition in store shelves as a sub-graph isomorphism problem"}, {"abstract": "Our work addresses two important issues with recurrent neural networks: (1)\nthey are over-parameterized, and (2) the recurrence matrix is ill-conditioned.\nThe former increases the sample complexity of learning and the training time.\nThe latter causes the vanishing and exploding gradient problem. We present a\nflexible recurrent neural network model called Kronecker Recurrent Units (KRU).\nKRU achieves parameter efficiency in RNNs through a Kronecker factored\nrecurrent matrix. It overcomes the ill-conditioning of the recurrent matrix by\nenforcing soft unitary constraints on the factors. Thanks to the small\ndimensionality of the factors, maintaining these constraints is computationally\nefficient. Our experimental results on five standard data-sets reveal that KRU\ncan reduce the number of parameters by three orders of magnitude in the\nrecurrent weight matrix compared to the existing recurrent models, without\ntrading the statistical performance. These results in particular show that\nwhile there are advantages in having a high dimensional recurrent space, the\ncapacity of the recurrent part of the model can be dramatically reduced.", "authors": ["Cijo Jose", "Moustpaha Cisse", "Francois Fleuret"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1705.10142v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1705.10142v4", "num_discussion": 0, "originally_published_time": "5/29/2017", "pid": "1705.10142v4", "published_time": "7/26/2017", "rawpid": "1705.10142", "tags": ["cs.LG"], "title": "Kronecker Recurrent Units"}, {"abstract": "An efficient Singular Value Decomposition (SVD) algorithm is an important\ntool for distributed and streaming computation in big data problems. It is\nobserved that update of singular vectors of a rank-1 perturbed matrix is\nsimilar to a Cauchy matrix-vector product. With this observation, in this\npaper, we present an efficient method for updating Singular Value Decomposition\nof rank-1 perturbed matrix in $O(n^2 \\ \\text{log}(\\frac{1}{\\epsilon}))$ time.\nThe method uses Fast Multipole Method (FMM) for updating singular vectors in\n$O(n \\ \\text{log} (\\frac{1}{\\epsilon}))$ time, where $\\epsilon$ is the\nprecision of computation.", "authors": ["Ratnik Gandhi", "Amoli Rajgor"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1707.08369v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08369v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08369v1", "published_time": "7/26/2017", "rawpid": "1707.08369", "tags": ["cs.LG", "math.NA"], "title": "Updating Singular Value Decomposition for Rank One Matrix Perturbation"}, {"abstract": "With recent innovations in dense image captioning, it is now possible to\ndescribe every object of the scene with a caption while objects are determined\nby bounding boxes. However, interpretation of such an output is not trivial due\nto the existence of many overlapping bounding boxes. Furthermore, in current\ncaptioning frameworks, the user is not able to involve personal preferences to\nexclude out of interest areas. In this paper, we propose a novel hybrid deep\nlearning architecture for interactive region segmentation and captioning where\nthe user is able to specify an arbitrary region of the image that should be\nprocessed. To this end, a dedicated Fully Convolutional Network (FCN) named\nLyncean FCN (LFCN) is trained using our special training data to isolate the\nUser Intention Region (UIR) as the output of an efficient segmentation. In\nparallel, a dense image captioning model is utilized to provide a wide variety\nof captions for that region. Then, the UIR will be explained with the caption\nof the best match bounding box. To the best of our knowledge, this is the first\nwork that provides such a comprehensive output. Our experiments show the\nsuperiority of the proposed approach over state-of-the-art interactive\nsegmentation methods on several well-known datasets. In addition, replacement\nof the bounding boxes with the result of the interactive segmentation leads to\na better understanding of the dense image captioning output as well as accuracy\nenhancement for the object detection in terms of Intersection over Union (IoU).", "authors": ["Ali Sharifi Boroujerdi", "Maryam Khanian", "Michael Breuss"], "category": "cs.CV", "comment": "17, pages, 9 figures", "img": "/static/thumbs/1707.08364v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08364v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08364v1", "published_time": "7/26/2017", "rawpid": "1707.08364", "tags": ["cs.CV", "68T45"], "title": "Deep Interactive Region Segmentation and Captioning"}, {"abstract": "This paper introduces a general Bayesian non- parametric latent feature model\nsuitable to per- form automatic exploratory analysis of heterogeneous datasets,\nwhere the attributes describing each object can be either discrete, continuous\nor mixed variables. The proposed model presents several important properties.\nFirst, it accounts for heterogeneous data while can be inferred in linear time\nwith respect to the number of objects and attributes. Second, its Bayesian\nnonparametric nature allows us to automatically infer the model complexity from\nthe data, i.e., the number of features necessary to capture the latent\nstructure in the data. Third, the latent features in the model are\nbinary-valued variables, easing the interpretability of the obtained latent\nfeatures in data exploration tasks.", "authors": ["Isabel Valera", "Melanie F. Pradier", "Zoubin Ghahramani"], "category": "stat.ML", "comment": "presented at 2017 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2017), Sydney, ...", "img": "/static/thumbs/1707.08352v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08352v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08352v1", "published_time": "7/26/2017", "rawpid": "1707.08352", "tags": ["stat.ML", "cs.LG"], "title": "General Latent Feature Modeling for Data Exploration Tasks"}, {"abstract": "We present a novel deep learning framework that models the scene dependent\nimage processing inside cameras. Often called as the radiometric calibration,\nthe process of recovering RAW images from processed images (JPEG format in the\nsRGB color space) is essential for many computer vision tasks that rely on\nphysically accurate radiance values. All previous works rely on the\ndeterministic imaging model where the color transformation stays the same\nregardless of the scene and thus they can only be applied for images taken\nunder the manual mode. In this paper, we propose a data-driven approach to\nlearn the scene dependent and locally varying image processing inside cameras\nunder the automode. Our method incorporates both the global and the local scene\ncontext into pixel-wise features via multi-scale pyramid of learnable histogram\nlayers. The results show that we can model the imaging pipeline of different\ncameras that operate under the automode accurately in both directions (from RAW\nto sRGB, from sRGB to RAW) and we show how we can apply our method to improve\nthe performance of image deblurring.", "authors": ["Seonghyeon Nam", "Seon Joo Kim"], "category": "cs.CV", "comment": "To appear in ICCV 2017", "img": "/static/thumbs/1707.08350v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08350v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08350v1", "published_time": "7/26/2017", "rawpid": "1707.08350", "tags": ["cs.CV"], "title": "Modelling the Scene Dependent Imaging in Cameras with a Deep Neural\n  Network"}, {"abstract": "We describe a machine learning approach for the 2017 shared task on Native\nLanguage Identification (NLI). The proposed approach combines several kernels\nusing multiple kernel learning. While most of our kernels are based on\ncharacter p-grams (also known as n-grams) extracted from essays or speech\ntranscripts, we also use a kernel based on i-vectors, a low-dimensional\nrepresentation of audio recordings, provided by the shared task organizers. For\nthe learning stage, we choose Kernel Discriminant Analysis (KDA) over Kernel\nRidge Regression (KRR), because the former classifier obtains better results\nthan the latter one on the development set. In our previous work, we have used\na similar machine learning approach to achieve state-of-the-art NLI results.\nThe goal of this paper is to demonstrate that our shallow and simple approach\nbased on string kernels (with minor improvements) can pass the test of time and\nreach state-of-the-art performance in the 2017 NLI shared task, despite the\nrecent advances in natural language processing. We participated in all three\ntracks, in which the competitors were allowed to use only the essays (essay\ntrack), only the speech transcripts (speech track), or both (fusion track).\nUsing only the data provided by the organizers for training our models, we have\nreached a macro F1 score of 86.95% in the closed essay track, a macro F1 score\nof 87.55% in the closed speech track, and a macro F1 score of 93.19% in the\nclosed fusion track. With these scores, our team (UnibucKernel) ranked in the\nfirst group of teams in all three tracks, while attaining the best scores in\nthe speech and the fusion tracks.", "authors": ["Radu Tudor Ionescu", "Marius Popescu"], "category": "cs.CL", "comment": "In Proceedings of the 12th Workshop on Building Educational\n  Applications Using NLP, 2017", "img": "/static/thumbs/1707.08349v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08349v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08349v1", "published_time": "7/26/2017", "rawpid": "1707.08349", "tags": ["cs.CL"], "title": "Can string kernels pass the test of time in Native Language\n  Identification?"}, {"abstract": "We propose a no-reference image quality assessment (NR-IQA) approach that\nlearns from rankings (RankIQA). To address the problem of limited IQA dataset\nsize, we train a Siamese Network to rank images in terms of image quality by\nusing synthetically generated distortions for which relative image quality is\nknown. These ranked image sets can be automatically generated without laborious\nhuman labeling. We then use fine-tuning to transfer the knowledge represented\nin the trained Siamese Network to a traditional CNN that estimates absolute\nimage quality from single images. We demonstrate how our approach can be made\nsignificantly more efficient than traditional Siamese Networks by forward\npropagating a batch of images through a single network and backpropagating\ngradients derived from all pairs of images in the batch. Experiments on the\nTID2013 benchmark show that we improve the state-of-the-art by over 5%.\nFurthermore, on the LIVE benchmark we show that our approach is superior to\nexisting NR-IQA techniques and that we even outperform the state-of-the-art in\nfull-reference IQA (FR-IQA) methods without having to resort to high-quality\nreference images to infer IQA.", "authors": ["Xialei Liu", "Joost van de Weijer", "Andrew D. Bagdanov"], "category": "cs.CV", "comment": "Accepted by ICCV 2017", "img": "/static/thumbs/1707.08347v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08347v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08347v1", "published_time": "7/26/2017", "rawpid": "1707.08347", "tags": ["cs.CV"], "title": "RankIQA: Learning from Rankings for No-reference Image Quality\n  Assessment"}, {"abstract": "This paper proposes a novel formulation for the multi-object\ntracking-by-detection paradigm for two (or more) input detectors. Using\nfull-body and heads detections, the fusion helps to recover heavily occluded\npersons and to reduce false positives. The assignment of the two input features\nto a person and the extraction of the trajectories is commonly solved from one\nbinary quadratic program (BQP). Due to the computational complexity of the\nNP-hard QP, we approximate the solution using the Frank-Wolfe algorithm. We\npropose several improvements to this solver affecting better minimization and\nshorter computations, compared to off-the-shelf BQP-solvers and the standard\nFrank-Wolfe algorithm. Evaluation on pedestrian tracking is provided for\nmultiple scenarios, showing improved tracking quality over single input feature\ntrackers and standard QP-solvers. Finally we present the performance of our\ntracker on the challenging \\MOTNEW benchmark, being comparable to\nstate-of-the-art trackers.", "authors": ["Roberto Henschel", "Laura Leal-Taix\u00e9", "Daniel Cremers", "Bodo Rosenhahn"], "category": "cs.CV", "comment": "13 pages, 7 figures", "img": "/static/thumbs/1705.08314v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1705.08314v2", "num_discussion": 0, "originally_published_time": "5/23/2017", "pid": "1705.08314v2", "published_time": "7/26/2017", "rawpid": "1705.08314", "tags": ["cs.CV"], "title": "Improvements to Frank-Wolfe optimization for multi-detector multi-object\n  tracking"}, {"abstract": "Users like sharing personal photos with others through social media. At the\nsame time, they might want to make automatic identification in such photos\ndifficult or even impossible. Classic obfuscation methods such as blurring are\nnot only unpleasant but also not as effective as one would expect. Recent\nstudies on adversarial image perturbations (AIP) suggest that it is possible to\nconfuse recognition systems effectively without unpleasant artifacts. However,\nin the presence of counter measures against AIPs, it is unclear how effective\nAIP would be in particular when the choice of counter measure is unknown. Game\ntheory provides tools for studying the interaction between agents with\nuncertainties in the strategies. We introduce a general game theoretical\nframework for the user-recogniser dynamics, and present a case study that\ninvolves current state of the art AIP and person recognition techniques. We\nderive the optimal strategy for the user that assures an upper bound on the\nrecognition rate independent of the recogniser\u0027s counter measure. Code is\navailable at https://goo.gl/hgvbNK.", "authors": ["Seong Joon Oh", "Mario Fritz", "Bernt Schiele"], "category": "cs.CV", "comment": "To appear at ICCV\u002717", "img": "/static/thumbs/1703.09471v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.09471v2", "num_discussion": 0, "originally_published_time": "3/28/2017", "pid": "1703.09471v2", "published_time": "7/26/2017", "rawpid": "1703.09471", "tags": ["cs.CV", "cs.CR", "cs.GT"], "title": "Adversarial Image Perturbation for Privacy Protection -- A Game Theory\n  Perspective"}, {"abstract": "Sequential pattern mining algorithms are widely used to explore care pathways\ndatabase, but they generate a deluge of patterns, mostly redundant or useless.\nClinicians need tools to express complex mining queries in order to generate\nless but more significant patterns. These algorithms are not versatile enough\nto answer complex clinician queries. This article proposes to apply a\ndeclarative pattern mining approach based on Answer Set Programming paradigm.\nIt is exemplified by a pharmaco-epidemiological study investigating the\npossible association between hospitalization for seizure and antiepileptic drug\nswitch from a french medico-administrative database.", "authors": ["Thomas Guyet", "Andr\u00e9 Happe", "Yann Dauxais"], "category": "cs.AI", "comment": "", "img": "/static/thumbs/1707.08342v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08342v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08342v1", "published_time": "7/26/2017", "rawpid": "1707.08342", "tags": ["cs.AI"], "title": "Declarative Sequential Pattern Mining of Care Pathways"}, {"abstract": "Single image super resolution (SR), which refers to reconstruct a\nhigher-resolution (HR) image from the observed low-resolution (LR) image, has\nreceived substantial attention due to its tremendous application potentials.\nDespite the breakthroughs of recently proposed SR methods using convolutional\nneural networks (CNNs), their generated results usually lack of preserving\nstructural (high-frequency) details. In this paper, regarding global boundary\ncontext and residual context as complimentary information for enhancing\nstructural details in image restoration, we develop a contextualized multi-task\nlearning framework to address the SR problem. Specifically, our method first\nextracts convolutional features from the input LR image and applies one\ndeconvolutional module to interpolate the LR feature maps in a content-adaptive\nway. Then, the resulting feature maps are fed into two branched sub-networks.\nDuring the neural network training, one sub-network outputs salient image\nboundaries and the HR image, and the other sub-network outputs the local\nresidual map, i.e., the residual difference between the generated HR image and\nground-truth image. On several standard benchmarks (i.e., Set5, Set14 and\nBSD200), our extensive evaluations demonstrate the effectiveness of our SR\nmethod on achieving both higher restoration quality and computational\nefficiency compared with several state-of-the-art SR approaches. The source\ncode and some SR results can be found at:\nhttp://hcp.sysu.edu.cn/structure-preserving-image-super-resolution/", "authors": ["Yukai Shi", "Keze Wang", "Chongyu Chen", "Li Xu", "Liang Lin"], "category": "cs.CV", "comment": "To appear in Transactions on Multimedia 2017", "img": "/static/thumbs/1707.08340v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08340v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08340v1", "published_time": "7/26/2017", "rawpid": "1707.08340", "tags": ["cs.CV"], "title": "Structure-Preserving Image Super-resolution via Contextualized\n  Multi-task Learning"}, {"abstract": "Hashing has been widely used for large-scale approximate nearest neighbor\nsearch because of its storage and search efficiency. Recent work has found that\ndeep supervised hashing can significantly outperform non-deep supervised\nhashing in many applications. However, most existing deep supervised hashing\nmethods adopt a symmetric strategy to learn one deep hash function for both\nquery points and database (retrieval) points. The training of these symmetric\ndeep supervised hashing methods is typically time-consuming, which makes them\nhard to effectively utilize the supervised information for cases with\nlarge-scale database. In this paper, we propose a novel deep supervised hashing\nmethod, called asymmetric deep supervised hashing (ADSH), for large-scale\nnearest neighbor search. ADSH treats the query points and database points in an\nasymmetric way. More specifically, ADSH learns a deep hash function only for\nquery points, while the hash codes for database points are directly learned.\nThe training of ADSH is much more efficient than that of traditional symmetric\ndeep supervised hashing methods. Experiments show that ADSH can achieve\nstate-of-the-art performance in real applications.", "authors": ["Qing-Yuan Jiang", "Wu-Jun Li"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1707.08325v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08325v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08325v1", "published_time": "7/26/2017", "rawpid": "1707.08325", "tags": ["cs.LG", "stat.ML"], "title": "Asymmetric Deep Supervised Hashing"}, {"abstract": "Neural machine translation (NMT), a new approach to machine translation, has\nachieved promising results comparable to those of traditional approaches such\nas statistical machine translation (SMT). Despite its recent success, NMT\ncannot handle a larger vocabulary because the training complexity and decoding\ncomplexity proportionally increase with the number of target words. This\nproblem becomes even more serious when translating patent documents, which\ncontain many technical terms that are observed infrequently. In this paper, we\npropose to select phrases that contain out-of-vocabulary words using the\nstatistical approach of branching entropy. This allows the proposed NMT system\nto be applied to a translation task of any language pair without any\nlanguage-specific knowledge about technical term identification. The selected\nphrases are then replaced with tokens during training and post-translated by\nthe phrase translation table of SMT. Evaluation on Japanese-to-Chinese,\nChinese-to-Japanese, Japanese-to-English and English-to-Japanese patent\nsentence translation proved the effectiveness of phrases selected with\nbranching entropy, where the proposed NMT model achieves a substantial\nimprovement over a baseline NMT model without our proposed technique. Moreover,\nthe number of translation errors of under-translation by the baseline NMT model\nwithout our proposed technique reduces to around half by the proposed NMT\nmodel.", "authors": ["Zi Long", "Ryuichiro Kimura", "Takehito Utsuro", "Tomoharu Mitsuhashi", "Mikio Yamamoto"], "category": "cs.CL", "comment": "MT summit 2017 poster", "img": "/static/thumbs/1704.04520v5.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.04520v5", "num_discussion": 0, "originally_published_time": "4/14/2017", "pid": "1704.04520v5", "published_time": "7/26/2017", "rawpid": "1704.04520", "tags": ["cs.CL"], "title": "Neural Machine Translation Model with a Large Vocabulary Selected by\n  Branching Entropy"}, {"abstract": "A variety of representation learning approaches have been investigated for\nreinforcement learning; much less attention, however, has been given to\ninvestigating the utility of sparse coding. Outside of reinforcement learning,\nsparse coding representations have been widely used, with non-convex objectives\nthat result in discriminative representations. In this work, we develop a\nsupervised sparse coding objective for policy evaluation. Despite the\nnon-convexity of this objective, we prove that all local minima are global\nminima, making the approach amenable to simple optimization strategies. We\nempirically show that it is key to use a supervised objective, rather than the\nmore straightforward unsupervised sparse coding approach. We compare the\nlearned representations to a canonical fixed sparse representation, called\ntile-coding, demonstrating that the sparse coding representation outperforms a\nwide variety of tilecoding representations.", "authors": ["Lei Le", "Raksha Kumaraswamy", "Martha White"], "category": "cs.AI", "comment": "6(+1) pages, 2 figures, International Joint Conference on Artificial\n  Intelligence 2017", "img": "/static/thumbs/1707.08316v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08316v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08316v1", "published_time": "7/26/2017", "rawpid": "1707.08316", "tags": ["cs.AI", "cs.LG", "stat.ML"], "title": "Learning Sparse Representations in Reinforcement Learning with Sparse\n  Coding"}, {"abstract": "This paper presents GridNet, a new Convolutional Neural Network (CNN)\narchitecture for semantic image segmentation (full scene labelling). Classical\nneural networks are implemented as one stream from the input to the output with\nsubsampling operators applied in the stream in order to reduce the feature maps\nsize and to increase the receptive field for the final prediction. However, for\nsemantic image segmentation, where the task consists in providing a semantic\nclass to each pixel of an image, feature maps reduction is harmful because it\nleads to a resolution loss in the output prediction. To tackle this problem,\nour GridNet follows a grid pattern allowing multiple interconnected streams to\nwork at different resolutions. We show that our network generalizes many well\nknown networks such as conv-deconv, residual or U-Net networks. GridNet is\ntrained from scratch and achieves competitive results on the Cityscapes\ndataset.", "authors": ["Damien Fourure", "R\u00e9mi Emonet", "Elisa Fromont", "Damien Muselet", "Alain Tremeau", "Christian Wolf"], "category": "cs.CV", "comment": "Accepted for publication at BMVC 2017", "img": "/static/thumbs/1707.07958v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.07958v2", "num_discussion": 0, "originally_published_time": "7/25/2017", "pid": "1707.07958v2", "published_time": "7/26/2017", "rawpid": "1707.07958", "tags": ["cs.CV"], "title": "Residual Conv-Deconv Grid Network for Semantic Segmentation"}, {"abstract": "In the paper, we study the stochastic alternating direction method of\nmultipliers (ADMM) for the nonconvex optimizations, and propose three classes\nof the nonconvex stochastic ADMM with variance reduction, based on different\nreduced variance stochastic gradients. Specifically, the first class called the\nnonconvex stochastic variance reduced gradient ADMM (SVRG-ADMM), uses a\nmulti-stage scheme to progressively reduce the variance of stochastic\ngradients. The second is the nonconvex stochastic average gradient ADMM\n(SAG-ADMM), which additionally uses the old gradients estimated in the previous\niteration. The third called SAGA-ADMM is an extension of the SAG-ADMM method.\nMoreover, under some mild conditions, we establish the iteration complexity\nbound of $O(1/\\epsilon)$ of the proposed methods to obtain an\n$\\epsilon$-stationary solution of the nonconvex optimizations. In particular,\nwe provide a general framework to analyze the iteration complexity of these\nnonconvex stochastic ADMM methods with variance reduction. Finally, some\nnumerical experiments demonstrate the effectiveness of our methods.", "authors": ["Feihu Huang", "Songcan Chen", "Zhaosong Lu"], "category": "math.OC", "comment": "34 pages, 4 figures and 2 tables", "img": "/static/thumbs/1610.02758v5.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1610.02758v5", "num_discussion": 0, "originally_published_time": "10/10/2016", "pid": "1610.02758v5", "published_time": "7/26/2017", "rawpid": "1610.02758", "tags": ["math.OC", "stat.ML"], "title": "Stochastic Alternating Direction Method of Multipliers with Variance\n  Reduction for Nonconvex Optimization"}, {"abstract": "Given two consecutive frames from a pair of stereo cameras, 3D scene flow\nmethods simultaneously estimate the 3D geometry and motion of the observed\nscene. Many existing approaches use superpixels for regularization, but may\npredict inconsistent shapes and motions inside rigidly moving objects. We\ninstead assume that scenes consist of foreground objects rigidly moving in\nfront of a static background, and use semantic cues to produce pixel-accurate\nscene flow estimates. Our cascaded classification framework accurately models\n3D scenes by iteratively refining semantic segmentation masks, stereo\ncorrespondences, 3D rigid motion estimates, and optical flow fields. We\nevaluate our method on the challenging KITTI autonomous driving benchmark, and\nshow that accounting for the motion of segmented vehicles leads to\nstate-of-the-art performance.", "authors": ["Zhile Ren", "Deqing Sun", "Jan Kautz", "Erik B. Sudderth"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1707.08313v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08313v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08313v1", "published_time": "7/26/2017", "rawpid": "1707.08313", "tags": ["cs.CV"], "title": "Cascaded Scene Flow Prediction using Semantic Segmentation"}, {"abstract": "One of the major hurdles preventing the full exploitation of information from\nonline communities is the widespread concern regarding the quality and\ncredibility of user-contributed content. Prior works in this domain operate on\na static snapshot of the community, making strong assumptions about the\nstructure of the data (e.g., relational tables), or consider only shallow\nfeatures for text classification.\n  To address the above limitations, we propose probabilistic graphical models\nthat can leverage the joint interplay between multiple factors in online\ncommunities --- like user interactions, community dynamics, and textual content\n--- to automatically assess the credibility of user-contributed online content,\nand the expertise of users and their evolution with user-interpretable\nexplanation. To this end, we devise new models based on Conditional Random\nFields for different settings like incorporating partial expert knowledge for\nsemi-supervised learning, and handling discrete labels as well as numeric\nratings for fine-grained analysis. This enables applications such as extracting\nreliable side-effects of drugs from user-contributed posts in healthforums, and\nidentifying credible content in news communities.\n  Online communities are dynamic, as users join and leave, adapt to evolving\ntrends, and mature over time. To capture this dynamics, we propose generative\nmodels based on Hidden Markov Model, Latent Dirichlet Allocation, and Brownian\nMotion to trace the continuous evolution of user expertise and their language\nmodel over time. This allows us to identify expert users and credible content\njointly over time, improving state-of-the-art recommender systems by explicitly\nconsidering the maturity of users. This also enables applications such as\nidentifying helpful product reviews, and detecting fake and anomalous reviews\nwith limited information.", "authors": ["Subhabrata Mukherjee"], "category": "cs.SI", "comment": "PhD thesis, Mar 2017", "img": "/static/thumbs/1707.08309v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08309v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08309v1", "published_time": "7/26/2017", "rawpid": "1707.08309", "tags": ["cs.SI", "cs.AI", "cs.CL", "cs.IR", "stat.ML"], "title": "Probabilistic Graphical Models for Credibility Analysis in Evolving\n  Online Communities"}, {"abstract": "To date, most convolutional neural network architectures output predictions\nby flattening 3rd-order activation tensors, and applying fully-connected output\nlayers. This approach has two drawbacks: (i) we lose rich, multi-modal\nstructure during the flattening process and (ii) fully-connected layers require\nmany parameters. We present the first attempt to circumvent these issues by\nexpressing the output of a neural network directly as the the result of a\nmulti-linear mapping from an activation tensor to the output. By imposing\nlow-rank constraints on the regression tensor, we can efficiently solve\nproblems for which existing solutions are badly parametrized. Our proposed\ntensor regression layer replaces flattening operations and fully-connected\nlayers by leveraging multi-modal structure in the data and expressing the\nregression weights via a low rank tensor decomposition. Additionally, we\ncombine tensor regression with tensor contraction to further increase\nefficiency. Augmenting the VGG and ResNet architectures, we demonstrate large\nreductions in the number of parameters with negligible impact on performance on\nthe ImageNet dataset.", "authors": ["Jean Kossaifi", "Zachary C. Lipton", "Aran Khanna", "Tommaso Furlanello", "Anima Anandkumar"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1707.08308v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08308v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08308v1", "published_time": "7/26/2017", "rawpid": "1707.08308", "tags": ["cs.LG"], "title": "Tensor Regression Networks"}, {"abstract": "We present a simple generative framework for learning to predict previously\nunseen classes, based on estimating class-attribute-gated class-conditional\ndistributions. We model each class-conditional distribution as an exponential\nfamily distribution and the parameters of the distribution of each seen/unseen\nclass are defined as functions of the respective observed class attributes.\nThese functions can be learned using only the seen class data and can be used\nto predict the parameters of the class-conditional distribution of each unseen\nclass. Unlike most existing methods for zero-shot learning that represent\nclasses as fixed embeddings in some vector space, our generative model\nnaturally represents each class as a probability distribution. It is simple to\nimplement and also allows leveraging additional unlabeled data from unseen\nclasses to improve the estimates of their class-conditional distributions using\ntransductive/semi-supervised learning. Moreover, it extends seamlessly to\nfew-shot learning by easily updating these distributions when provided with a\nsmall number of additional labelled examples from unseen classes. Through a\ncomprehensive set of experiments on several benchmark data sets, we demonstrate\nthe efficacy of our framework.", "authors": ["Vinay Kumar Verma", "Piyush Rai"], "category": "cs.LG", "comment": "Accepted in ECML-PKDD 2017, 16 Pages", "img": "/static/thumbs/1707.08040v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08040v2", "num_discussion": 0, "originally_published_time": "7/25/2017", "pid": "1707.08040v2", "published_time": "7/26/2017", "rawpid": "1707.08040", "tags": ["cs.LG", "cs.CV", "stat.ML"], "title": "A Simple Exponential Family Framework for Zero-Shot Learning"}, {"abstract": "Omnidirectional cameras are widely used in such areas as robotics and virtual\nreality as they provide a wide field of view. Their images are often processed\nwith classical methods, which might unfortunately lead to non-optimal solutions\nas these methods are designed for planar images that have different geometrical\nproperties than omnidirectional ones. In this paper we study image\nclassification task by taking into account the specific geometry of\nomnidirectional cameras with graph-based representations. In particular, we\nextend deep learning architectures to data on graphs; we propose a principled\nway of graph construction such that convolutional filters respond similarly for\nthe same pattern on different positions of the image regardless of lens\ndistortions. Our experiments show that the proposed method outperforms current\ntechniques for the omnidirectional image classification problem.", "authors": ["Renata Khasanova", "Pascal Frossard"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1707.08301v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08301v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08301v1", "published_time": "7/26/2017", "rawpid": "1707.08301", "tags": ["cs.CV", "cs.LG"], "title": "Graph-Based Classification of Omnidirectional Images"}, {"abstract": "Deep learning models (DLMs) are state-of-the-art techniques in speech\nrecognition. However, training good DLMs can be time consuming especially for\nproduction-size models and corpora. Although several parallel training\nalgorithms have been proposed to improve training efficiency, there is no clear\nguidance on which one to choose for the task in hand due to lack of systematic\nand fair comparison among them. In this paper we aim at filling this gap by\ncomparing four popular parallel training algorithms in speech recognition,\nnamely asynchronous stochastic gradient descent (ASGD), blockwise model-update\nfiltering (BMUF), bulk synchronous parallel (BSP) and elastic averaging\nstochastic gradient descent (EASGD), on 1000-hour LibriSpeech corpora using\nfeed-forward deep neural networks (DNNs) and convolutional, long short-term\nmemory, DNNs (CLDNNs). Based on our experiments, we recommend using BMUF as the\ntop choice to train acoustic models since it is most stable, scales well with\nnumber of GPUs, can achieve reproducible results, and in many cases even\noutperforms single-GPU SGD. ASGD can be used as a substitute in some cases.", "authors": ["Wenpeng Li", "BinBin Zhang", "Lei Xie", "Dong Yu"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1703.05880v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.05880v2", "num_discussion": 0, "originally_published_time": "3/17/2017", "pid": "1703.05880v2", "published_time": "7/26/2017", "rawpid": "1703.05880", "tags": ["cs.CL", "cs.LG", "cs.SD"], "title": "Empirical Evaluation of Parallel Training Algorithms on Acoustic\n  Modeling"}, {"abstract": "The current processes for building machine learning systems require\npractitioners with deep knowledge of machine learning. This significantly\nlimits the number of machine learning systems that can be created and has led\nto a mismatch between the demand for machine learning systems and the ability\nfor organizations to build them. We believe that in order to meet this growing\ndemand for machine learning systems we must significantly increase the number\nof individuals that can teach machines. We postulate that we can achieve this\ngoal by making the process of teaching machines easy, fast and above all,\nuniversally accessible.\n  While machine learning focuses on creating new algorithms and improving the\naccuracy of \"learners\", the machine teaching discipline focuses on the efficacy\nof the \"teachers\". Machine teaching as a discipline is a paradigm shift that\nfollows and extends principles of software engineering and programming\nlanguages. We put a strong emphasis on the teacher and the teacher\u0027s\ninteraction with data, as well as crucial components such as techniques and\ndesign principles of interaction and visualization.\n  In this paper, we present our position regarding the discipline of machine\nteaching and articulate fundamental machine teaching principles. We also\ndescribe how, by decoupling knowledge about machine learning algorithms from\nthe process of teaching, we can accelerate innovation and empower millions of\nnew uses for machine learning models.", "authors": ["Patrice Y. Simard", "Saleema Amershi", "David M. Chickering", "Alicia Edelman Pelton", "Soroush Ghorashi", "Christopher Meek", "Gonzalo Ramos", "Jina Suh", "Johan Verwey", "Mo Wang", "John Wernsing"], "category": "cs.LG", "comment": "Also available at: http://aka.ms/machineteachingpaper", "img": "/static/thumbs/1707.06742v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.06742v2", "num_discussion": 0, "originally_published_time": "7/21/2017", "pid": "1707.06742v2", "published_time": "7/26/2017", "rawpid": "1707.06742", "tags": ["cs.LG", "cs.AI", "cs.HC", "cs.SE", "stat.ML"], "title": "Machine Teaching: A New Paradigm for Building Machine Learning Systems"}, {"abstract": "Entropy is a fundamental property of a repertoire. Here, we present an\nefficient algorithm to estimate the entropy of types with the help of Zhang\u0027s\nestimator. The algorithm takes advantage of the fact that the number of\ndifferent frequencies in a text is in general much smaller than the number of\ntypes. We justify the convenience of the algorithm by means of an analysis of\nthe statistical properties of texts from more than 1000 languages. Our work\nopens up various possibilities for future research.", "authors": ["Antoni Lozano", "Bernardino Casas", "Chris Bentz", "Ramon Ferrer-i-Cancho"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1707.08290v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08290v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08290v1", "published_time": "7/26/2017", "rawpid": "1707.08290", "tags": ["cs.CL"], "title": "Fast calculation of entropy with Zhang\u0027s estimator"}, {"abstract": "Image matting plays an important role in image and video editing. However,\nthe formulation of image matting is inherently ill-posed. Traditional methods\nusually employ interaction to deal with the image matting problem with trimaps\nand strokes, and cannot run on the mobile phone in real-time. In this paper, we\npropose a real-time automatic deep matting approach for mobile devices. By\nleveraging the densely connected blocks and the dilated convolution, a light\nfull convolutional network is designed to predict a coarse binary mask for\nportrait images. And a feathering block, which is edge-preserving and matting\nadaptive, is further developed to learn the guided filter and transform the\nbinary mask into alpha matte. Finally, an automatic portrait animation system\nbased on fast deep matting is built on mobile devices, which does not need any\ninteraction and can realize real-time matting with 15 fps. The experiments show\nthat the proposed approach achieves comparable results with the\nstate-of-the-art matting solvers.", "authors": ["Bingke Zhu", "Yingying Chen", "Jinqiao Wang", "Si Liu", "Bo Zhang", "Ming Tang"], "category": "cs.CV", "comment": "ACM Multimedia Conference (MM) 2017 camera-ready", "img": "/static/thumbs/1707.08289v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08289v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08289v1", "published_time": "7/26/2017", "rawpid": "1707.08289", "tags": ["cs.CV"], "title": "Fast Deep Matting for Portrait Animation on Mobile Phone"}, {"abstract": "One of the main benefits of a wrist-worn computer is its ability to collect a\nvariety of physiological data in a minimally intrusive manner. Among these\ndata, electrodermal activity (EDA) is readily collected and provides a window\ninto a person\u0027s emotional and sympathetic responses. EDA data collected using a\nwearable wristband are easily influenced by motion artifacts (MAs) that may\nsignificantly distort the data and degrade the quality of analyses performed on\nthe data if not identified and removed. Prior work has demonstrated that MAs\ncan be successfully detected using supervised machine learning algorithms on a\nsmall data set collected in a lab setting. In this paper, we demonstrate that\nunsupervised learning algorithms perform competitively with supervised\nalgorithms for detecting MAs on EDA data collected in both a lab-based setting\nand a real-world setting comprising about 23 hours of data. We also find,\nsomewhat surprisingly, that incorporating accelerometer data as well as EDA\nimproves detection accuracy only slightly for supervised algorithms and\nsignificantly degrades the accuracy of unsupervised algorithms.", "authors": ["Yuning Zhang", "Maysam Haghdan", "Kevin S. Xu"], "category": "cs.HC", "comment": "To appear at International Symposium on Wearable Computers (ISWC)\n  2017", "img": "/static/thumbs/1707.08287v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08287v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08287v1", "published_time": "7/26/2017", "rawpid": "1707.08287", "tags": ["cs.HC", "cs.LG", "I.5.4; H.1.2"], "title": "Unsupervised Motion Artifact Detection in Wrist-Measured Electrodermal\n  Activity Data"}, {"abstract": "Convolutional Neural Networks (CNNs) are well established models capable of\nachieving state-of-the-art classification accuracy for various computer vision\ntasks. However, they are becoming increasingly larger, using millions of\nparameters, while they are restricted to handling images of fixed size. In this\npaper, a quantization-based approach, inspired from the well-known\nBag-of-Features model, is proposed to overcome these limitations. The proposed\napproach, called Convolutional BoF (CBoF), uses RBF neurons to quantize the\ninformation extracted from the convolutional layers and it is able to natively\nclassify images of various sizes as well as to significantly reduce the number\nof parameters in the network. In contrast to other global pooling operators and\nCNN compression techniques the proposed method utilizes a trainable pooling\nlayer that it is end-to-end differentiable, allowing the network to be trained\nusing regular back-propagation and to achieve greater distribution shift\ninvariance than competitive methods. The ability of the proposed method to\nreduce the parameters of the network and increase the classification accuracy\nover other state-of-the-art techniques is demonstrated using three image\ndatasets.", "authors": ["Nikolaos Passalis", "Anastasios Tefas"], "category": "cs.CV", "comment": "Accepted at ICCV 2017", "img": "/static/thumbs/1707.08105v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08105v2", "num_discussion": 0, "originally_published_time": "7/25/2017", "pid": "1707.08105v2", "published_time": "7/26/2017", "rawpid": "1707.08105", "tags": ["cs.CV"], "title": "Learning Bag-of-Features Pooling for Deep Convolutional Neural Networks"}, {"abstract": "Generative adversarial networks (GANs) are considered as a totally different\ntype of generative models. However, it is well known that GANs are very hard to\ntrain. There have been proposed many different techniques in order to stabilize\ntheir training procedures.\n  In this paper, we propose a novel training method called manifold matching\nand a new GAN model called manifold matching GAN (MMGAN). In MMGAN, vector\nrepresentations extracted from the last layer of the discriminator are used to\ntrain the generator. It finds two manifolds representing vector representations\nof real and fake images. If these two manifolds are matched, it means that real\nand fake images are identical in the perspective of the discriminator because\nthe manifolds are constructed from the discriminator\u0027s last layer. In general,\nit is much easier to train the discriminator and it becomes more accurate as\nepoch goes by. This implies that the manifold matching also becomes very\naccurate as the discriminator is trained. We also use the kernel trick to find\nbetter manifolds.\n  We conduct in-depth experiments with three image datasets and several\nstate-of-the-art GAN models. Our experiments demonstrate the efficacy of the\nproposed MMGAN model.", "authors": ["Noseong Park", "Ankesh Anand", "Joel Ruben Antony Moniz", "Kookjin Lee", "Tanmoy Chakraborty", "Jaegul Choo", "Hongkyu Park", "Youngmin Kim"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1707.08273v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08273v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08273v1", "published_time": "7/26/2017", "rawpid": "1707.08273", "tags": ["cs.LG"], "title": "MMGAN: Manifold Matching Generative Adversarial Network for Generating\n  Images"}, {"abstract": "Deep Learning has made a great progress for these years. However, it is still\ndifficult to master the implement of various models because different\nresearchers may release their code based on different frameworks or interfaces.\nIn this paper, we proposed a computation graph based framework which only aims\nto introduce well-known interfaces. It will help a lot when reproducing a newly\nmodel or transplanting models that were implemented by other frameworks.\nAdditionally, we implement numerous recent models covering both Computer Vision\nand Nature Language Processing. We demonstrate that our framework will not\nsuffer from model-starving because it is much easier to make full use of the\nworks that are already done.", "authors": ["Ting Pan"], "category": "cs.SE", "comment": "", "img": "/static/thumbs/1707.08265v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08265v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08265v1", "published_time": "7/26/2017", "rawpid": "1707.08265", "tags": ["cs.SE", "cs.LG"], "title": "Dragon: A Computation Graph Virtual Machine Based Deep Learning\n  Framework"}, {"abstract": "We present a conditional generative model that maps low-dimensional\nembeddings of multiple modalities of data to a common latent space hence\nextracting semantic relationships between them. The embedding specific to a\nmodality is first extracted and subsequently a constrained optimization\nprocedure is performed to project the two embedding spaces to a common\nmanifold. The individual embeddings are generated back from this common latent\nspace. However, in order to enable independent conditional inference for\nseparately extracting the corresponding embeddings from the common latent space\nrepresentation, we deploy a proxy variable trick - wherein, the single shared\nlatent space is replaced by the respective separate latent spaces of each\nmodality. We design an objective function, such that, during training we can\nforce these separate spaces to lie close to each other, by minimizing the\ndistance between their probability distribution functions. Experimental results\ndemonstrate that the learned joint model can generalize to learning concepts of\ndouble MNIST digits with additional attributes of colors,from both textual and\nspeech input.", "authors": ["Subhajit Chaudhury", "Sakyasingha Dasgupta", "Asim Munawar", "Md. A. Salam Khan", "Ryuki Tachibana"], "category": "cs.LG", "comment": "7 pages, 4 figures, ICML 2017 Workshop on Implicit Models", "img": "/static/thumbs/1707.00860v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.00860v2", "num_discussion": 0, "originally_published_time": "7/4/2017", "pid": "1707.00860v2", "published_time": "7/26/2017", "rawpid": "1707.00860", "tags": ["cs.LG", "cs.AI", "cs.CV"], "title": "Conditional generation of multi-modal data using constrained embedding\n  space mapping"}, {"abstract": "In this work, we provide a solution for posturing the anthropomorphic\nRobonaut-2 hand and arm for grasping based on visual information. A mapping\nfrom visual features extracted from a convolutional neural network (CNN) to\ngrasp points is learned. We demonstrate that a CNN pre-trained for image\nclassification can be applied to a grasping task based on a small set of\ngrasping examples. Our approach takes advantage of the hierarchical nature of\nthe CNN by identifying features that capture the hierarchical support relations\nbetween filters in different CNN layers and locating their 3D positions by\ntracing activations backwards in the CNN. When this backward trace terminates\nin the RGB-D image, important manipulable structures are thereby localized.\nThese features that reside in different layers of the CNN are then associated\nwith controllers that engage different kinematic subchains in the hand/arm\nsystem for grasping. A grasping dataset is collected using demonstrated\nhand/object relationships for Robonaut-2 to evaluate the proposed approach in\nterms of the precision of the resulting preshape postures. We demonstrate that\nthis approach outperforms baseline approaches in cluttered scenarios on the\ngrasping dataset and a point cloud based approach on a grasping task using\nRobonaut-2.", "authors": ["Li Yang Ku", "Erik Learned-Miller", "Rod Grupen"], "category": "cs.CV", "comment": "8 pages, 9 figures, IROS 2017", "img": "/static/thumbs/1609.03947v5.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1609.03947v5", "num_discussion": 0, "originally_published_time": "9/13/2016", "pid": "1609.03947v5", "published_time": "7/26/2017", "rawpid": "1609.03947", "tags": ["cs.CV", "cs.RO"], "title": "Associating Grasp Configurations with Hierarchical Features in\n  Convolutional Neural Networks"}, {"abstract": "Sleep disorders, such as sleep apnea, parasomnias, and hypersomnia, affect\n50-70 million adults in the United States (Hillman et al., 2006). Overnight\npolysomnography (PSG), including brain monitoring using electroencephalography\n(EEG), is a central component of the diagnostic evaluation for sleep disorders.\nWhile PSG is conventionally performed by trained technologists, the recent rise\nof powerful neural network learning algorithms combined with large\nphysiological datasets offers the possibility of automation, potentially making\nexpert-level sleep analysis more widely available. We propose SLEEPNET (Sleep\nEEG neural network), a deployed annotation tool for sleep staging. SLEEPNET\nuses a deep recurrent neural network trained on the largest sleep physiology\ndatabase assembled to date, consisting of PSGs from over 10,000 patients from\nthe Massachusetts General Hospital (MGH) Sleep Laboratory. SLEEPNET achieves\nhuman-level annotation performance on an independent test set of 1,000 EEGs,\nwith an average accuracy of 85.76% and algorithm-expert inter-rater agreement\n(IRA) of kappa = 79.46%, comparable to expert-expert IRA.", "authors": ["Siddharth Biswal", "Joshua Kulas", "Haoqi Sun", "Balaji Goparaju", "M Brandon Westover", "Matt T Bianchi", "Jimeng Sun"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1707.08262v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08262v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08262v1", "published_time": "7/26/2017", "rawpid": "1707.08262", "tags": ["cs.LG"], "title": "SLEEPNET: Automated Sleep Staging System via Deep Learning"}, {"abstract": "We propose to use thought-provoking children\u0027s questions (TPCQs), namely\nHighlights BrainPlay questions, as a new method to drive artificial\nintelligence research and to evaluate the capabilities of general-purpose AI\nsystems. These questions are designed to stimulate thought and learning in\nchildren, and they can be used to do the same thing in AI systems, while\ndemonstrating the system\u0027s reasoning capabilities to the evaluator. We\nintroduce the TPCQ task, which which takes a TPCQ question as input and\nproduces as output (1) answers to the question and (2) learned generalizations.\nWe discuss how BrainPlay questions stimulate learning. We analyze 244 BrainPlay\nquestions, and we report statistics on question type, question class, answer\ncardinality, answer class, types of knowledge needed, and types of reasoning\nneeded. We find that BrainPlay questions span many aspects of intelligence.\nBecause the answers to BrainPlay questions and the generalizations learned from\nthem are often highly open-ended, we suggest using human judges for evaluation.", "authors": ["Erik T. Mueller", "Henry Minsky"], "category": "cs.AI", "comment": "update for EGPAI 2017", "img": "/static/thumbs/1508.06924v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1508.06924v3", "num_discussion": 0, "originally_published_time": "8/27/2015", "pid": "1508.06924v3", "published_time": "7/26/2017", "rawpid": "1508.06924", "tags": ["cs.AI"], "title": "Using Thought-Provoking Children\u0027s Questions to Drive Artificial\n  Intelligence Research"}, {"abstract": "The article studies navigability of an autonomous agent in a maze where some\nrooms may be indistinguishable. In a previous work the authors have shown that\nthe properties of navigability in such a setting depend on whether an agent has\nperfect recall. Navigability by an agent with perfect recall is a transitive\nrelation and without is not transitive.\n  This article introduces a notion of restricted navigability and shows that a\ncertain form of transitivity holds for restricted navigability, even for an\nagent without perfect recall. The main technical result is a sound and complete\nlogical system describing the properties of restricted navigability.", "authors": ["Kaya Deuser", "Pavel Naumov"], "category": "cs.AI", "comment": "", "img": "/static/thumbs/1707.08255v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08255v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08255v1", "published_time": "7/26/2017", "rawpid": "1707.08255", "tags": ["cs.AI", "cs.LO"], "title": "Navigability with Imperfect Information"}, {"abstract": "Semantic Segmentation using deep convolutional neural network pose more\ncomplex challenge for any GPU intensive work, as it has to compute million of\nparameters resulting to huge consumption of memory. Moreover, extracting finer\nfeatures and conducting supervised training tends to increase the complexity\nfurthermore. With the introduction of Fully Convolutional Neural Network, which\nuses finer strides and utilizes deconvolutional layers for upsampling, it has\nbeen a go to for any image segmentation task. We propose two segmentation\narchitecture transferring weights from the popular classification neural net\nVGG19 and VGG16 which were trained on Imagenet classification dataset,\ntransform all the fully connected layers to convolutional layers, use dilated\nconvolution for decreasing the parameters, moreover we add more finer strides\nand attach four skip architectures which are concatenated with the\ndeconvolutional layers in steps. We train on two stages, first with PASCAL\nVOC2012 training data and then with SBD training and validation set. With our\nmodel, FCN-2s-Dilated-VGG19 we yield better score for PASCAL VOC2012 test set\nwith a meanIOU of 69 percent which is 1.8 percent better than FCN-8s. And with\nFCN-2s-Dilated-VGG16 we score a meanIOU of 67.6 percent. On the other hand our\nmodel consumes up to 10-20 percent less memory than FCN-8s for training with\nNVIDIA Pascal GPUs, making it more efficient and less memory consuming\narchitecture for pixel-wise segmentation.", "authors": ["Sharif Amit Kamran", "Ali Shihab Sabbir"], "category": "cs.CV", "comment": "6 pages", "img": "/static/thumbs/1707.08254v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08254v1", "num_discussion": 0, "originally_published_time": "7/26/2017", "pid": "1707.08254v1", "published_time": "7/26/2017", "rawpid": "1707.08254", "tags": ["cs.CV"], "title": "Efficient Yet Deep Convolutional Neural Networks for Semantic\n  Segmentation"}, {"abstract": "This volume consists of papers presented at the Sixteenth Conference on\nTheoretical Aspects of Rationality and Knowledge (TARK) held at the University\nof Liverpool, UK, from July 24 to 26, 2017.\n  TARK conferences bring together researchers from a wide variety of fields,\nincluding Computer Science (especially, Artificial Intelligence, Cryptography,\nDistributed Computing), Economics (especially, Decision Theory, Game Theory,\nSocial Choice Theory), Linguistics, Philosophy (especially, Philosophical\nLogic), and Cognitive Psychology, in order to further understand the issues\ninvolving reasoning about rationality and knowledge.", "authors": ["J\u00e9r\u00f4me Lang"], "category": "cs.GT", "comment": "", "img": "/static/thumbs/1707.08250v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08250v1", "num_discussion": 0, "originally_published_time": "7/25/2017", "pid": "1707.08250v1", "published_time": "7/25/2017", "rawpid": "1707.08250", "tags": ["cs.GT", "cs.AI", "cs.CR", "cs.LO"], "title": "Proceedings Sixteenth Conference on Theoretical Aspects of Rationality\n  and Knowledge"}, {"abstract": "We study the active learning problem of top-$k$ ranking from multi-wise\ncomparisons under the popular multinomial logit model. Our goal is to identify\nthe top-$k$ items with high probability by adaptively querying sets for\ncomparisons and observing the noisy output of the most preferred item from each\ncomparison. To achieve this goal, we design a new active ranking algorithm\nwithout using any information about the underlying items\u0027 preference scores. We\nalso establish a matching lower bound on the sample complexity even when the\nset of preference scores is given to the algorithm. These two results together\nshow that the proposed algorithm is instance optimal (up to logarithmic\nfactors). Our work extends the existing literature on rank aggregation in three\ndirections. First, instead of studying a static problem with fixed data, we\ninvestigate the top-$k$ ranking problem in an active learning setting. Second,\nwe provide the instance optimality, which is a much stronger theoretical\nguarantee. Finally, we extend the pairwise comparison to the multi-wise\ncomparison, which has not been fully explored in ranking literature.", "authors": ["Xi Chen", "Yuanzhi Li", "Jieming Mao"], "category": "cs.DS", "comment": "", "img": "/static/thumbs/1707.08238v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08238v1", "num_discussion": 0, "originally_published_time": "7/25/2017", "pid": "1707.08238v1", "published_time": "7/25/2017", "rawpid": "1707.08238", "tags": ["cs.DS", "stat.ML"], "title": "An Instance Optimal Algorithm for Top-k Ranking under the Multinomial\n  Logit Model"}, {"abstract": "Automotive manufacturers attempting to bring autonomous vehicles to market\nmust make the case that their product is sufficiently safe for public\ndeployment. Much of this case will likely rely upon outcomes from real-world\ntesting, requiring manufacturers to be strategic about how they allocate\ntesting resources in order to maximize their chances of demonstrating system\nsafety. This work frames the partially observable and belief-dependent problem\nof autonomous vehicle test scheduling as a Markov decision process, which can\nbe solved efficiently to yield exact, optimal manufacturer testing policies. By\nsolving for policies over a wide range of problem formulations, we are able to\nprovide high-level guidance for manufacturers and regulators on issues relating\nto the real-world testing of autonomous vehicles. This guidance spans an array\nof topics, including circumstances under which manufacturers should continue\ntesting despite observed incidents, when manufacturers should test\naggressively, and when regulators should increase or reduce the real-world\ntesting requirements for an autonomous vehicle.", "authors": ["Jeremy Morton", "Tim A. Wheeler", "Mykel J. Kochenderfer"], "category": "cs.AI", "comment": "11 pages, 5 figures, 5 tables", "img": "/static/thumbs/1707.08234v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08234v1", "num_discussion": 0, "originally_published_time": "7/25/2017", "pid": "1707.08234v1", "published_time": "7/25/2017", "rawpid": "1707.08234", "tags": ["cs.AI"], "title": "Optimal Testing of Self-Driving Cars"}, {"abstract": "This work studies the parameter identification problem for the Markov chain\nchoice model of Blanchet, Gallego, and Goyal used in assortment planning. In\nthis model, the product selected by a customer is determined by a Markov chain\nover the products, where the products in the offered assortment are absorbing\nstates. The underlying parameters of the model were previously shown to be\nidentifiable from the choice probabilities for the all-products assortment,\ntogether with choice probabilities for assortments of all-but-one products.\nObtaining and estimating choice probabilities for such large assortments is not\ndesirable in many settings. The main result of this work is that the parameters\nmay be identified from assortments of sizes two and three, regardless of the\ntotal number of products. The result is obtained via a simple and efficient\nparameter recovery algorithm.", "authors": ["Arushi Gupta", "Daniel Hsu"], "category": "math.ST", "comment": "10 pages", "img": "/static/thumbs/1706.00729v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1706.00729v3", "num_discussion": 0, "originally_published_time": "6/2/2017", "pid": "1706.00729v3", "published_time": "7/25/2017", "rawpid": "1706.00729", "tags": ["math.ST", "cs.LG", "stat.ML", "stat.TH"], "title": "Parameter identification in Markov chain choice models"}, {"abstract": "Biomedical information is growing rapidly in the recent years and retrieving\nuseful data through information extraction system is getting more attention. In\nthe current research, we focus on different aspects of relation extraction\ntechniques in biomedical domain and briefly describe the state-of-the-art for\nrelation extraction between a variety of biological elements.", "authors": ["Elham Shahab"], "category": "cs.CL", "comment": "updated keywords and reference format", "img": "/static/thumbs/1707.05850v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.05850v3", "num_discussion": 0, "originally_published_time": "7/18/2017", "pid": "1707.05850v3", "published_time": "7/25/2017", "rawpid": "1707.05850", "tags": ["cs.CL"], "title": "A Short Survey of Biomedical Relation Extraction Techniques"}, {"abstract": "Rectified Linear Units (ReLUs) are widely used in feed-forward neural\nnetworks, and in convolutional neural networks in particular. However, they can\nbe rarely found in recurrent neural networks due to the unboundedness and the\npositive image of the rectified linear activation function. In this paper, we\nintroduce Dual Rectified Linear Units (DReLUs), a novel type of rectified unit\nthat comes with a positive and negative image that is unbounded. We show that\nwe can successfully replace the tanh activation function in the recurrent step\nof quasi recurrent neural networks. In addition, DReLUs are less prone to the\nvanishing gradient problem, they are noise robust, and they induce sparse\nactivations. Therefore, we are able to stack up to eight quasi recurrent\nlayers, making it possible to improve the current state-of-the-art in\ncharacter-level language modeling over architectures based on shallow Long\nShort-Term Memory (LSTM).", "authors": ["Fr\u00e9deric Godin", "Jonas Degrave", "Joni Dambre", "Wesley De Neve"], "category": "cs.CL", "comment": "Submitted to Neural Networks", "img": "/static/thumbs/1707.08214v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08214v1", "num_discussion": 0, "originally_published_time": "7/25/2017", "pid": "1707.08214v1", "published_time": "7/25/2017", "rawpid": "1707.08214", "tags": ["cs.CL", "cs.LG"], "title": "DReLUs: Dual Rectified Linear Units"}, {"abstract": "We present a new algorithm for the 2D Radix-2 Sliding Window Fourier\nTransform (SWFT). Our algorithm avoids repeating calculations in overlapping\nwindows by using a tree representation of the Cooley-Tukey Fast Fourier\nTransform (FFT). For an $N_0 \\times N_1$ array and $n_0 = 2^{m_0} \\times n_1 =\n2^{m_1}$ windows, our algorithm takes $O(N_0 N_1 n_0 n_1)$ operations, which is\nfaster than taking a 2D FFT in each window. We provide a C implementation of\nthe algorithm, compare ours with existing algorithms, and show how the\nalgorithm extends to higher dimensions.", "authors": ["Lee F. Richardson", "William F. Eddy"], "category": "cs.DS", "comment": "10 pages, 3 figures, submitted to ACM TOMS", "img": "/static/thumbs/1707.08213v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08213v1", "num_discussion": 0, "originally_published_time": "7/25/2017", "pid": "1707.08213v1", "published_time": "7/25/2017", "rawpid": "1707.08213", "tags": ["cs.DS", "stat.CO", "stat.ML"], "title": "An Algorithm for the 2D Radix-2 Sliding Window Fourier Transform"}, {"abstract": "In this paper, we present a new task that investigates how people interact\nwith and make judgments about towers of blocks. In Experiment~1, participants\nin the lab solved a series of problems in which they had to re-configure three\nblocks from an initial to a final configuration. We recorded whether they used\none hand or two hands to do so. In Experiment~2, we asked participants online\nto judge whether they think the person in the lab used one or two hands. The\nresults revealed a close correspondence between participants\u0027 actions in the\nlab, and the mental simulations of participants online. To explain\nparticipants\u0027 actions and mental simulations, we develop a model that plans\nover a symbolic representation of the situation, executes the plan using a\ngeometric solver, and checks the plan\u0027s feasibility by taking into account the\nphysical constraints of the scene. Our model explains participants\u0027 actions and\njudgments to a high degree of quantitative accuracy.", "authors": ["Ilker Yildirim", "Tobias Gerstenberg", "Basil Saeed", "Marc Toussaint", "Josh Tenenbaum"], "category": "cs.AI", "comment": "", "img": "/static/thumbs/1707.08212v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08212v1", "num_discussion": 0, "originally_published_time": "7/25/2017", "pid": "1707.08212v1", "published_time": "7/25/2017", "rawpid": "1707.08212", "tags": ["cs.AI", "cs.RO", "stat.ML"], "title": "Physical problem solving: Joint planning with symbolic, geometric, and\n  dynamic constraints"}, {"abstract": "Both hybrid automata and action languages are formalisms for describing the\nevolution of dynamic systems. This paper establishes a formal relationship\nbetween them. We show how to succinctly represent hybrid automata in an action\nlanguage which in turn is defined as a high-level notation for answer set\nprogramming modulo theories (ASPMT) --- an extension of answer set programs to\nthe first-order level similar to the way satisfiability modulo theories (SMT)\nextends propositional satisfiability (SAT). We first show how to represent\nlinear hybrid automata with convex invariants by an action language modulo\ntheories. A further translation into SMT allows for computing them using SMT\nsolvers that support arithmetic over reals. Next, we extend the representation\nto the general class of non-linear hybrid automata allowing even non-convex\ninvariants. We represent them by an action language modulo ODE (Ordinary\nDifferential Equations), which can be compiled into satisfiability modulo ODE.\nWe developed a prototype system cplus2aspmt based on these translations, which\nallows for a succinct representation of hybrid transition systems that can be\ncomputed effectively by the state-of-the-art SMT solver dReal.", "authors": ["Joohyung Lee", "Nikhil Loney", "Yunsong Meng"], "category": "cs.AI", "comment": "Paper presented at the 33nd International Conference on Logic\n  Programming (ICLP 2017), Melbourne, ...", "img": "/static/thumbs/1707.06387v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.06387v2", "num_discussion": 0, "originally_published_time": "7/20/2017", "pid": "1707.06387v2", "published_time": "7/25/2017", "rawpid": "1707.06387", "tags": ["cs.AI", "cs.FL", "cs.LO"], "title": "Representing Hybrid Automata by Action Language Modulo Theories"}, {"abstract": "LPMLN is a recent addition to probabilistic logic programming languages. Its\nmain idea is to overcome the rigid nature of the stable model semantics by\nassigning a weight to each rule in a way similar to Markov Logic is defined. We\npresent two implementations of LPMLN, $\\text{LPMLN2ASP}$ and\n$\\text{LPMLN2MLN}$. System $\\text{LPMLN2ASP}$ translates LPMLN programs into\nthe input language of answer set solver $\\text{CLINGO}$, and using weak\nconstraints and stable model enumeration, it can compute most probable stable\nmodels as well as exact conditional and marginal probabilities. System\n$\\text{LPMLN2MLN}$ translates LPMLN programs into the input language of Markov\nLogic solvers, such as $\\text{ALCHEMY}$, $\\text{TUFFY}$, and $\\text{ROCKIT}$,\nand allows for performing approximate probabilistic inference on LPMLN\nprograms. We also demonstrate the usefulness of the LPMLN systems for computing\nother languages, such as ProbLog and Pearl\u0027s Causal Models, that are shown to\nbe translatable into LPMLN. (Under consideration for acceptance in TPLP)", "authors": ["Joohyung Lee", "Samidh Talsania", "Yi Wang"], "category": "cs.AI", "comment": "Paper presented at the 33nd International Conference on Logic\n  Programming (ICLP 2017), Melbourne, ...", "img": "/static/thumbs/1707.06325v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.06325v2", "num_discussion": 0, "originally_published_time": "7/19/2017", "pid": "1707.06325v2", "published_time": "7/25/2017", "rawpid": "1707.06325", "tags": ["cs.AI", "cs.LO"], "title": "Computing LPMLN Using ASP and MLN Solvers"}, {"abstract": "Deep neural network (DNN) architectures have been shown to outperform\ntraditional pipelines for object segmentation and pose estimation using RGBD\ndata, but the performance of these DNN pipelines is directly tied to how\nrepresentative the training data is of the true data. Hence a key requirement\nfor employing these methods in practice is to have a large set of labeled data\nfor your specific robotic manipulation task, a requirement that is not\ngenerally satisfied by existing datasets. In this paper we develop a pipeline\nto rapidly generate high quality RGBD data with pixelwise labels and object\nposes. We use an RGBD camera to collect video of a scene from multiple\nviewpoints and leverage existing reconstruction techniques to produce a 3D\ndense reconstruction. We label the 3D reconstruction using a human assisted\nICP-fitting of object meshes. By reprojecting the results of labeling the 3D\nscene we can produce labels for each RGBD image of the scene. This pipeline\nenabled us to collect over 1,000,000 labeled object instances in just a few\ndays. We use this dataset to answer questions related to how much training data\nis required, and of what quality the data must be, to achieve high performance\nfrom a DNN architecture.", "authors": ["Pat Marion", "Peter R. Florence", "Lucas Manuelli", "Russ Tedrake"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1707.04796v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.04796v2", "num_discussion": 0, "originally_published_time": "7/15/2017", "pid": "1707.04796v2", "published_time": "7/25/2017", "rawpid": "1707.04796", "tags": ["cs.CV", "cs.RO"], "title": "A Pipeline for Generating Ground Truth Labels for Real RGBD Data of\n  Cluttered Scenes"}, {"abstract": "Nonnegative matrix factorization (NMF) is a powerful tool in data exploratory\nanalysis by discovering the hidden features and part-based patterns from\nhigh-dimensional data. NMF and its variants have been successfully applied into\ndiverse fields such as pattern recognition, signal processing, data mining,\nbioinformatics and so on. Recently, NMF has been extended to analyze multiple\nmatrices simultaneously. However, a unified framework is still lacking. In this\npaper, we introduce a sparse multiple relationship data regularized joint\nmatrix factorization (JMF) framework and two adapted prediction models for\npattern recognition and data integration. Next, we present four update\nalgorithms to solve this framework. The merits and demerits of these algorithms\nare systematically explored. Furthermore, extensive computational experiments\nusing both synthetic data and real data demonstrate the effectiveness of JMF\nframework and related algorithms on pattern recognition and data mining.", "authors": ["Lihua Zhang", "Shihua Zhang"], "category": "cs.CV", "comment": "14 pages, 7 figures", "img": "/static/thumbs/1707.08183v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08183v1", "num_discussion": 0, "originally_published_time": "7/25/2017", "pid": "1707.08183v1", "published_time": "7/25/2017", "rawpid": "1707.08183", "tags": ["cs.CV", "I.5.1; G.1.6; H.2.8"], "title": "A Unified Joint Matrix Factorization Framework for Data Integration"}, {"abstract": "This paper presents the results of the RepEval 2017 Shared Task, which\nevaluated neural network sentence representation learning models on the\nMulti-Genre Natural Language Inference corpus (MultiNLI) recently introduced by\nWilliams et al. (2017). All of the five participating teams beat the\nbidirectional LSTM (BiLSTM) and continuous bag of words baselines reported in\nWilliams et al.. The best single model used stacked BiLSTMs with residual\nconnections to extract sentence features and reached 74.5% accuracy on the\ngenre-matched test set. Surprisingly, the results of the competition were\nfairly consistent across the genre-matched and genre-mismatched test sets, and\nacross subsets of the test data representing a variety of linguistic phenomena,\nsuggesting that all of the submitted systems learned reasonably\ndomain-independent representations for sentence meaning.", "authors": ["Nikita Nangia", "Adina Williams", "Angeliki Lazaridou", "Samuel R. Bowman"], "category": "cs.CL", "comment": "10 pages, 1 figure, 6 tables, in Proceedings of The Second Workshop\n  on Evaluating Vector Space Rep...", "img": "/static/thumbs/1707.08172v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08172v1", "num_discussion": 0, "originally_published_time": "7/25/2017", "pid": "1707.08172v1", "published_time": "7/25/2017", "rawpid": "1707.08172", "tags": ["cs.CL"], "title": "The RepEval 2017 Shared Task: Multi-Genre Natural Language Inference\n  with Sentence Representations"}, {"abstract": "With the development of neural networks based machine learning and their\nusage in mission critical applications, voices are rising against the\n\\textit{black box} aspect of neural networks as it becomes crucial to\nunderstand their limits and capabilities. With the rise of neuromorphic\nhardware, it is even more critical to understand how a neural network, as a\ndistributed system, tolerates the failures of its computing nodes, neurons, and\nits communication channels, synapses. Experimentally assessing the robustness\nof neural networks involves the quixotic venture of testing all the possible\nfailures, on all the possible inputs, which ultimately hits a combinatorial\nexplosion for the first, and the impossibility to gather all the possible\ninputs for the second.\n  In this paper, we prove an upper bound on the expected error of the output\nwhen a subset of neurons crashes. This bound involves dependencies on the\nnetwork parameters that can be seen as being too pessimistic in the average\ncase. It involves a polynomial dependency on the Lipschitz coefficient of the\nneurons activation function, and an exponential dependency on the depth of the\nlayer where a failure occurs. We back up our theoretical results with\nexperiments illustrating the extent to which our prediction matches the\ndependencies between the network parameters and robustness. Our results show\nthat the robustness of neural networks to the average crash can be estimated\nwithout the need to neither test the network on all failure configurations, nor\naccess the training set used to train the network, both of which are\npractically impossible requirements.", "authors": ["El Mahdi El Mhamdi", "Rachid Guerraoui", "Sebastien Rouault"], "category": "stat.ML", "comment": "36th IEEE International Symposium on Reliable Distributed Systems 26\n  - 29 September 2017. Hong Kon...", "img": "/static/thumbs/1707.08167v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08167v1", "num_discussion": 0, "originally_published_time": "7/25/2017", "pid": "1707.08167v1", "published_time": "7/25/2017", "rawpid": "1707.08167", "tags": ["stat.ML", "cs.AI", "cs.DC", "cs.LG", "cs.NE"], "title": "On The Robustness of a Neural Network"}, {"abstract": "ProbLog is a state-of-art combination of logic programming and probabilities;\nin particular ProbLog offers parameter learning through a variant of the EM\nalgorithm. However, the resulting learning algorithm is rather slow, even when\nthe data are complete. In this short paper we offer some insights that lead to\norders of magnitude improvements in ProbLog\u0027s parameter learning speed with\ncomplete data.", "authors": ["Francisco H. O. V. de Faria", "Arthur C. Gusm\u00e3o", "Fabio G. Cozman", "Denis D. Mau\u00e1"], "category": "cs.AI", "comment": "StarAI - International Workshop on Statistical Relational AI", "img": "/static/thumbs/1707.08151v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08151v1", "num_discussion": 0, "originally_published_time": "7/25/2017", "pid": "1707.08151v1", "published_time": "7/25/2017", "rawpid": "1707.08151", "tags": ["cs.AI", "97R40"], "title": "Speeding-up ProbLog\u0027s Parameter Learning"}, {"abstract": "Deep learning technologies such as convolutional neural networks (CNN)\nprovide powerful methods for image recognition and have recently been employed\nin the field of automated carcinoma detection in confocal laser endomicroscopy\n(CLE) images. CLE is a (sub-)surface microscopic imaging technique that reaches\nmagnifications of up to 1000x and is thus suitable for in vivo structural\ntissue analysis.\n  In this work, we aim to evaluate the prospects of a priorly developed deep\nlearning-based algorithm targeted at the identification of oral squamous cell\ncarcinoma with regard to its generalization to further anatomic locations of\nsquamous cell carcinomas in the area of head and neck. We applied the algorithm\non images acquired from the vocal fold area of five patients with\nhistologically verified squamous cell carcinoma and presumably healthy control\nimages of the clinically normal contra-lateral vocal cord.\n  We find that the network trained on the oral cavity data reaches an accuracy\nof 89.45% and an area-under-the-curve (AUC) value of 0.955, when applied on the\nvocal cords data. Compared to the state of the art, we achieve very similar\nresults, yet with an algorithm that was trained on a completely disjunct data\nset. Concatenating both data sets yielded further improvements in\ncross-validation with an accuracy of 90.81% and AUC of 0.970.\n  In this study, for the first time to our knowledge, a deep learning mechanism\nfor the identification of oral carcinomas using CLE Images could be applied to\nother disciplines in the area of head and neck. This study shows the prospect\nof the algorithmic approach to generalize well on other malignant entities of\nthe head and neck, regardless of the anatomical location and furthermore in an\nexaminer-independent manner.", "authors": ["Marc Aubreville", "Miguel Goncalves", "Christian Knipfer", "Nicolai Oetter", "Tobias Wuerfl", "Helmut Neumann", "Florian Stelzle", "Christopher Bohr", "Andreas Maier"], "category": "cs.CV", "comment": "8 pages, 8 figures, submitted to BIOIMAGING 2018", "img": "/static/thumbs/1707.08149v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08149v1", "num_discussion": 0, "originally_published_time": "7/25/2017", "pid": "1707.08149v1", "published_time": "7/25/2017", "rawpid": "1707.08149", "tags": ["cs.CV"], "title": "Patch-based Carcinoma Detection on Confocal Laser Endomicroscopy Images\n  - A Cross-Site Robustness Assessment"}, {"abstract": "Current image transformation and recoloring algorithms try to introduce\nartistic effects in the photographed images, based on user input of target\nimage(s) or selection of pre-designed filters. These manipulations, although\nintended to enhance the impact of an image on the viewer, do not include the\noption of image transformation by specifying the affect information. In this\npaper we present an automatic image-transformation method that transforms the\nsource image such that it can induce an emotional affect on the viewer, as\ndesired by the user. Our proposed novel image emotion transfer algorithm does\nnot require a user-specified target image. The proposed algorithm uses features\nextracted from top layers of deep convolutional neural network and the\nuser-specified emotion distribution to select multiple target images from an\nimage database for color transformation, such that the resultant image has\ndesired emotional impact. Our method can handle more diverse set of photographs\nthan the previous methods. We conducted a detailed user study showing the\neffectiveness of our proposed method. A discussion and reasoning of failure\ncases has also been provided, indicating inherent limitation of color-transfer\nbased methods in the use of emotion assignment.", "authors": ["Afsheen Rafaqat Ali", "Mohsen Ali"], "category": "cs.CV", "comment": "Accepted at British Machine Vision Conference (BMVC) 2017", "img": "/static/thumbs/1707.08148v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08148v1", "num_discussion": 0, "originally_published_time": "7/25/2017", "pid": "1707.08148v1", "published_time": "7/25/2017", "rawpid": "1707.08148", "tags": ["cs.CV"], "title": "Automatic Image Transformation for Inducing Affect"}, {"abstract": "Generative moment matching network (GMMN), which is based on the maximum mean\ndiscrepancy (MMD) measure, is a generative model for unsupervised learning,\nwhere the mini-batch stochastic gradient descent is applied for the update of\nparam- eters. In this work, instead of obtaining a mini-batch randomly, each\nmini-batch in the iterations is selected in a submodular way such that the most\ninformative subset of data is more likely to be chosen. In such a framework,\nthe training objective is reformulated as optimizing a mixed continuous and\nsubmodular function with a cardinality constraint. A Majorization\nMinimization-like algorithm is used to iteratively solve the problem.\nSpecifically, in each iteration of the training process, a mini-batch is first\nselected by solving a submodular maximization problem, and then the mini-batch\nstochastic gradient descent is conducted. Our experiments on the MNIST and\nLabeled Faces in the Wild (LFW) databases show the effectiveness of the\nsubmodular mini-batch training in the GMMN frameworks.", "authors": ["Jun Qi", "Xiaodong He", "Adith Swaminathan", "Li Deng"], "category": "cs.LG", "comment": "Inappropriate text reuse and unattributed ideas", "img": "/static/thumbs/1707.05721v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.05721v2", "num_discussion": 0, "originally_published_time": "7/18/2017", "pid": "1707.05721v2", "published_time": "7/25/2017", "rawpid": "1707.05721", "tags": ["cs.LG"], "title": "Submodular Mini-Batch Training in Generative Moment Matching Networks"}, {"abstract": "We investigate the compositional structure of message vectors computed by a\ndeep network trained on a communication game. By comparing truth-conditional\nrepresentations of encoder-produced message vectors to human-produced referring\nexpressions, we are able to identify aligned (vector, utterance) pairs with the\nsame meaning. We then search for structured relationships among these aligned\npairs to discover simple vector space transformations corresponding to\nnegation, conjunction, and disjunction. Our results suggest that neural\nrepresentations are capable of spontaneously developing a \"syntax\" with\nfunctional analogues to qualitative properties of natural language.", "authors": ["Jacob Andreas", "Dan Klein"], "category": "cs.CL", "comment": "In EMNLP 2017", "img": "/static/thumbs/1707.08139v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08139v1", "num_discussion": 0, "originally_published_time": "7/25/2017", "pid": "1707.08139v1", "published_time": "7/25/2017", "rawpid": "1707.08139", "tags": ["cs.CL", "cs.NE"], "title": "Analogs of Linguistic Structure in Deep Representations"}, {"abstract": "Very large-scale Deep Neural Networks (DNNs) have achieved remarkable\nsuccesses in a large variety of computer vision tasks. However, the high\ncomputation intensity of DNNs makes it challenging to deploy these models on\nresource-limited systems. Some studies used low-rank approaches that\napproximate the filters by low-rank basis to accelerate the testing. Those\nworks directly decomposed the pre-trained DNNs by Low-Rank Approximations\n(LRA). How to train DNNs toward lower-rank space for more efficient DNNs,\nhowever, remains as an open area. To solve the issue, in this work, we propose\nForce Regularization, which uses attractive forces to enforce filters so as to\ncoordinate more weight information into lower-rank space. We mathematically and\nempirically verify that after applying our technique, standard LRA methods can\nreconstruct filters using much lower basis and thus result in faster DNNs. The\neffectiveness of our approach is comprehensively evaluated in ResNets, AlexNet,\nand GoogLeNet. In AlexNet, for example, Force Regularization gains 2x speedup\non modern GPU without accuracy loss and 4.05x speedup on CPU by paying small\naccuracy degradation. Moreover, Force Regularization better initializes the\nlow-rank DNNs such that the fine-tuning can converge faster toward higher\naccuracy. The obtained lower-rank DNNs can be further sparsified, proving that\nForce Regularization can be integrated with state-of-the-art sparsity-based\nacceleration methods. Source code is available in\nhttps://github.com/wenwei202/caffe", "authors": ["Wei Wen", "Cong Xu", "Chunpeng Wu", "Yandan Wang", "Yiran Chen", "Hai Li"], "category": "cs.CV", "comment": "ICCV 2017", "img": "/static/thumbs/1703.09746v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.09746v3", "num_discussion": 0, "originally_published_time": "3/28/2017", "pid": "1703.09746v3", "published_time": "7/25/2017", "rawpid": "1703.09746", "tags": ["cs.CV", "I.2.6; I.5.1"], "title": "Coordinating Filters for Faster Deep Neural Networks"}, {"abstract": "A key challenge for manipulation in unstructured environments is action\nselection. We present a novel neural network-based approach that separates\nunknown objects in clutter by selecting favourable push actions. Our network is\ntrained from data collected through large-scale interaction of a PR2 robot with\nrandomly organized tabletop scenes. The model is designed to propose meaningful\npush actions based on segmented RGB-D images. We evaluate our approach by\nsingulating up to six unknown objects in clutter. We demonstrate that our\nmethod enables the robot to perform the task with a high success rate and a low\nnumber of required push actions. Our results based on real-world experiments\nshow that our network is able to generalize to novel objects of various sizes\nand shapes, as well as to arbitrary object configurations. Highlights of our\nexperiments can be viewed in the following video: https://youtu.be/jWbUJOrSacI .", "authors": ["Andreas Eitel", "Nico Hauff", "Wolfram Burgard"], "category": "cs.RO", "comment": "13 pages, 7 figures, ISRR 2017 submission, video:\n  https://youtu.be/jWbUJOrSacI", "img": "/static/thumbs/1707.08101v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08101v1", "num_discussion": 0, "originally_published_time": "7/25/2017", "pid": "1707.08101v1", "published_time": "7/25/2017", "rawpid": "1707.08101", "tags": ["cs.RO", "cs.LG", "cs.NE"], "title": "Learning to Singulate Objects using a Push Proposal Network"}, {"abstract": "In this paper, we propose a novel approach for text classification based on\nclustering word embeddings, inspired by the bag of visual words model, which is\nwidely used in computer vision. After each word in a collection of documents is\nrepresented as word vector using a pre-trained word embeddings model, a k-means\nalgorithm is applied on the word vectors in order to obtain a fixed-size set of\nclusters. The centroid of each cluster is interpreted as a super word embedding\nthat embodies all the semantically related word vectors in a certain region of\nthe embedding space. Every embedded word in the collection of documents is then\nassigned to the nearest cluster centroid. In the end, each document is\nrepresented as a bag of super word embeddings by computing the frequency of\neach super word embedding in the respective document. We also diverge from the\nidea of building a single vocabulary for the entire collection of documents,\nand propose to build class-specific vocabularies for better performance. Using\nthis kind of representation, we report results on two text mining tasks, namely\ntext categorization by topic and polarity classification. On both tasks, our\nmodel yields better performance than the standard bag of words.", "authors": ["Andrei M. Butnaru", "Radu Tudor Ionescu"], "category": "cs.CL", "comment": "Accepted at KES 2017", "img": "/static/thumbs/1707.08098v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08098v1", "num_discussion": 0, "originally_published_time": "7/25/2017", "pid": "1707.08098v1", "published_time": "7/25/2017", "rawpid": "1707.08098", "tags": ["cs.CL"], "title": "From Image to Text Classification: A Novel Approach based on Clustering\n  Word Embeddings"}, {"abstract": "This paper presents a state-of-the-art approach in object detection for being\napplied in future SLAM problems. Although, many SLAM methods are proposed to\ncreate suitable autonomy for mobile robots namely ground vehicles, they still\nface overconfidence and large computations during entrance to immense spaces\nwith many landmarks. In particular, they suffer from impractical applications\nvia sole reliance on the limited sensors like camera. Proposed method claims\nthat unmanned ground vehicles without having huge amount of database for object\ndefinition and highly advance prediction parameters can deal with incoming\nobjects during straight motion of camera in real-time. Line-Circle (LC) filter\ntries to apply detection, tracking and learning to each defined experts to\nobtain more information for judging scene without over-calculation. In this\nfilter, circle expert let us summarize edges in groups. The Interactive\nfeedback learning between each expert creates minimal error that fights against\noverwhelming landmark signs in crowded scenes without mapping. Our experts\nbasically are dependent on trust factors\u0027 covariance with geometric definitions\nto ignore, emerge and compare detected landmarks. The experiment for validating\nthe model is taken place utilizing a camera beside an IMU sensor for location\nestimation.", "authors": ["Seyed Amir Tafrishi", "Vahid E. Kandjani"], "category": "cs.RO", "comment": "Submitted to 5th International Conference on Robotics and\n  Mechatronics 2017", "img": "/static/thumbs/1707.08095v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08095v1", "num_discussion": 0, "originally_published_time": "7/25/2017", "pid": "1707.08095v1", "published_time": "7/25/2017", "rawpid": "1707.08095", "tags": ["cs.RO", "cs.CV"], "title": "Line-Circle: A Geometric Filter for Single Camera Edge-Based Object\n  Detection"}, {"abstract": "High-dimensional sparse linear regression is a basic problem in machine\nlearning and statistics. Consider a linear model $y = X\\theta^\\star + w$, where\n$y \\in \\mathbb{R}^n$ is the vector of observations, $X \\in \\mathbb{R}^{n \\times\nd}$ is the covariate matrix with $i$th row representing the covariates for the\n$i$th observation, and $w \\in \\mathbb{R}^n$ is an unknown noise vector. In many\napplications, the linear regression model is high-dimensional in nature,\nmeaning that the number of observations $n$ may be substantially smaller than\nthe number of covariates $d$. In these cases, it is common to assume that\n$\\theta^\\star$ is sparse, and the goal in sparse linear regression is to\nestimate this sparse $\\theta^\\star$, given $(X,y)$.\n  In this paper, we study a variant of the traditional sparse linear regression\nproblem where each of the $n$ covariate vectors in $\\mathbb{R}^d$ are\nindividually projected by a random linear transformation to $\\mathbb{R}^m$ with\n$m \\ll d$. Such transformations are commonly applied in practice for\ncomputational savings in resources such as storage space, transmission\nbandwidth, and processing time. Our main result shows that one can estimate\n$\\theta^\\star$ with a low $\\ell_2$-error, even with access to only these\nprojected covariate vectors, under some mild assumptions on the problem\ninstance. Our approach is based on solving a variant of the popular Lasso\noptimization problem. While the conditions (such as the restricted eigenvalue\ncondition on $X$) for success of a Lasso formulation in estimating\n$\\theta^\\star$ are well-understood, we investigate conditions under which this\nvariant of Lasso estimates $\\theta^\\star$. The main technical ingredient of our\nresult, a bound on the restricted eigenvalue on certain projections of a\ndeterministic matrix satisfying a stable rank condition, could be of interest\nbeyond sparse regression.", "authors": ["Shiva Prasad Kasiviswanathan", "Mark Rudelson"], "category": "stat.ML", "comment": "24 pages", "img": "/static/thumbs/1707.08092v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08092v1", "num_discussion": 0, "originally_published_time": "7/25/2017", "pid": "1707.08092v1", "published_time": "7/25/2017", "rawpid": "1707.08092", "tags": ["stat.ML", "cs.DS", "math.ST", "stat.TH"], "title": "Compressed Sparse Linear Regression"}, {"abstract": "In this paper, we present a novel unsupervised algorithm for word sense\ndisambiguation (WSD) at the document level. Our algorithm is inspired by a\nwidely-used approach in the field of genetics for whole genome sequencing,\nknown as the Shotgun sequencing technique. The proposed WSD algorithm is based\non three main steps. First, a brute-force WSD algorithm is applied to short\ncontext windows (up to 10 words) selected from the document in order to\ngenerate a short list of likely sense configurations for each window. In the\nsecond step, these local sense configurations are assembled into longer\ncomposite configurations based on suffix and prefix matching. The resulted\nconfigurations are ranked by their length, and the sense of each word is chosen\nbased on a voting scheme that considers only the top k configurations in which\nthe word appears. We compare our algorithm with other state-of-the-art\nunsupervised WSD algorithms and demonstrate better performance, sometimes by a\nvery large margin. We also show that our algorithm can yield better performance\nthan the Most Common Sense (MCS) baseline on one data set. Moreover, our\nalgorithm has a very small number of parameters, is robust to parameter tuning,\nand, unlike other bio-inspired methods, it gives a deterministic solution (it\ndoes not involve random choices).", "authors": ["Andrei M. Butnaru", "Radu Tudor Ionescu", "Florentina Hristea"], "category": "cs.CL", "comment": "In Proceedings of EACL 2017", "img": "/static/thumbs/1707.08084v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08084v1", "num_discussion": 0, "originally_published_time": "7/25/2017", "pid": "1707.08084v1", "published_time": "7/25/2017", "rawpid": "1707.08084", "tags": ["cs.CL"], "title": "ShotgunWSD: An unsupervised algorithm for global word sense\n  disambiguation inspired by DNA sequencing"}, {"abstract": "Approximate Bayesian Computation (ABC) is a method to obtain a posterior\ndistribution without a likelihood function, using simulations and a set of\ndistance metrics. For that reason, it has recently been gaining popularity as\nan analysis tool in cosmology and astrophysics. Its drawback, however, is a\nslow convergence rate. We propose a novel method, which we call qABC, to\naccelerate ABC with Quantile Regression. In this method, we create a model of\nquantiles of distance measure as a function of input parameters. This model is\ntrained on a small number of simulations and estimates which regions of the\nprior space are likely to be accepted into the posterior. Other regions are\nthen immediately rejected. This procedure is then repeated as more simulations\nare available. We apply it to the practical problem of estimation of redshift\ndistribution of cosmological samples, using forward modelling developed in\nprevious work. The qABC method converges to nearly same posterior as the basic\nABC. It uses, however, only 20\\% of the number of simulations compared to basic\nABC, achieving a fivefold gain in execution time for our problem. For other\nproblems the acceleration rate may vary; it depends on how close the prior is\nto the final posterior. We discuss possible improvements and extensions to this\nmethod.", "authors": ["Tomasz Kacprzak", "J\u00f6rg Herbel", "Adam Amara", "Alexandre R\u00e9fr\u00e9gier"], "category": "astro-ph.CO", "comment": "10 pages, 5 figures, prepared for submission to JCAP", "img": "/static/thumbs/1707.07498v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.07498v2", "num_discussion": 0, "originally_published_time": "7/24/2017", "pid": "1707.07498v2", "published_time": "7/25/2017", "rawpid": "1707.07498", "tags": ["astro-ph.CO", "stat.ML"], "title": "Accelerating Approximate Bayesian Computation with Quantile Regression:\n  Application to Cosmological Redshift Distributions"}, {"abstract": "Search systems are often focused on providing relevant results for the \"now\",\nassuming both corpora and user needs that focus on the present. However, many\ncorpora today reflect significant longitudinal collections ranging from 20\nyears of the Web to hundreds of years of digitized newspapers and books.\nUnderstanding the temporal intent of the user and retrieving the most relevant\nhistorical content has become a significant challenge. Common search features,\nsuch as query expansion, leverage the relationship between terms but cannot\nfunction well across all times when relationships vary temporally. In this\nwork, we introduce a temporal relationship model that is extracted from\nlongitudinal data collections. The model supports the task of identifying,\ngiven two words, when they relate to each other. We present an algorithmic\nframework for this task and show its application for the task of query\nexpansion, achieving high gain.", "authors": ["Guy D. Rosin", "Kira Radinsky", "Eytan Adar"], "category": "cs.CL", "comment": "11 pages, EMNLP 2017", "img": "/static/thumbs/1707.08081v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08081v1", "num_discussion": 0, "originally_published_time": "7/25/2017", "pid": "1707.08081v1", "published_time": "7/25/2017", "rawpid": "1707.08081", "tags": ["cs.CL"], "title": "Learning Word Relatedness over Time"}, {"abstract": "Simultaneously recorded electroencephalography (EEG) and functional magnetic\nresonance imaging (fMRI) can be used to non-invasively measure the\nspatiotemporal dynamics of the human brain. One challenge is dealing with the\nartifacts that each modality introduces into the other when the two are\nrecorded concurrently, for example the ballistocardiogram (BCG). We conducted a\npreliminary comparison of three different MR compatible EEG recording systems\nand assessed their performance in terms of single-trial classification of the\nEEG when simultaneously collecting fMRI. We found tradeoffs across all three\nsystems, for example varied ease of setup and improved classification accuracy\nwith reference electrodes (REF) but not for pulse artifact subtraction (PAS) or\nreference layer adaptive filtering (RLAF).", "authors": ["Josef Faller", "Linbi Hong", "Jennifer Cummings", "Paul Sajda"], "category": "q-bio.NC", "comment": "1 Page, IEEE EMBS Conference 2017, Korea", "img": "/static/thumbs/1707.08077v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08077v1", "num_discussion": 0, "originally_published_time": "7/25/2017", "pid": "1707.08077v1", "published_time": "7/25/2017", "rawpid": "1707.08077", "tags": ["q-bio.NC", "stat.ML"], "title": "A comparison of single-trial EEG classification and EEG-informed fMRI\n  across three MR compatible EEG recording systems"}, {"abstract": "We interpret the predictions of any black-box structured input-structured\noutput model around a specific input-output pair. Our method returns an\n\"explanation\" consisting of groups of input-output tokens that are causally\nrelated. These dependencies are inferred by querying the black-box model with\nperturbed inputs, generating a graph over tokens from the responses, and\nsolving a partitioning problem to select the most relevant components. We focus\nthe general approach on sequence-to-sequence problems, adopting a variational\nautoencoder to yield meaningful input perturbations. We test our method across\nseveral NLP sequence generation tasks.", "authors": ["David Alvarez-Melis", "Tommi S. Jaakkola"], "category": "cs.LG", "comment": "12 Pages, EMNLP 2017", "img": "/static/thumbs/1707.01943v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.01943v2", "num_discussion": 0, "originally_published_time": "7/6/2017", "pid": "1707.01943v2", "published_time": "7/25/2017", "rawpid": "1707.01943", "tags": ["cs.LG"], "title": "A causal framework for explaining the predictions of black-box\n  sequence-to-sequence models"}, {"abstract": "Identifying and interpreting fetal standard scan planes during 2D ultrasound\nmid-pregnancy examinations are highly complex tasks which require years of\ntraining. Apart from guiding the probe to the correct location, it can be\nequally difficult for a non-expert to identify relevant structures within the\nimage. Automatic image processing can provide tools to help experienced as well\nas inexperienced operators with these tasks. In this paper, we propose a novel\nmethod based on convolutional neural networks which can automatically detect 13\nfetal standard views in freehand 2D ultrasound data as well as provide a\nlocalisation of the fetal structures via a bounding box. An important\ncontribution is that the network learns to localise the target anatomy using\nweak supervision based on image-level labels only. The network architecture is\ndesigned to operate in real-time while providing optimal output for the\nlocalisation task. We present results for real-time annotation, retrospective\nframe retrieval from saved videos, and localisation on a very large and\nchallenging dataset consisting of images and video recordings of full clinical\nanomaly screenings. We found that the proposed method achieved an average\nF1-score of 0.798 in a realistic classification experiment modelling real-time\ndetection, and obtained a 90.09% accuracy for retrospective frame retrieval.\nMoreover, an accuracy of 77.8% was achieved on the localisation task.", "authors": ["Christian F. Baumgartner", "Konstantinos Kamnitsas", "Jacqueline Matthew", "Tara P. Fletcher", "Sandra Smith", "Lisa M. Koch", "Bernhard Kainz", "Daniel Rueckert"], "category": "cs.CV", "comment": "12 pages, 8 figures, published in IEEE Transactions in Medical\n  Imaging", "img": "/static/thumbs/1612.05601v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1612.05601v2", "num_discussion": 0, "originally_published_time": "12/16/2016", "pid": "1612.05601v2", "published_time": "7/25/2017", "rawpid": "1612.05601", "tags": ["cs.CV"], "title": "SonoNet: Real-Time Detection and Localisation of Fetal Standard Scan\n  Planes in Freehand Ultrasound"}, {"abstract": "Recent neural models have shown significant progress on the problem of\ngenerating short descriptive texts conditioned on a small number of database\nrecords. In this work, we suggest a slightly more difficult data-to-text\ngeneration task, and investigate how effective current approaches are on this\ntask. In particular, we introduce a new, large-scale corpus of data records\npaired with descriptive documents, propose a series of extractive evaluation\nmethods for analyzing performance, and obtain baseline results using current\nneural generation methods. Experiments show that these models produce fluent\ntext, but fail to convincingly approximate human-generated documents. Moreover,\neven templated baselines exceed the performance of these neural models on some\nmetrics, though copy- and reconstruction-based extensions lead to noticeable\nimprovements.", "authors": ["Sam Wiseman", "Stuart M. Shieber", "Alexander M. Rush"], "category": "cs.CL", "comment": "EMNLP 2017", "img": "/static/thumbs/1707.08052v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08052v1", "num_discussion": 0, "originally_published_time": "7/25/2017", "pid": "1707.08052v1", "published_time": "7/25/2017", "rawpid": "1707.08052", "tags": ["cs.CL"], "title": "Challenges in Data-to-Document Generation"}, {"abstract": "In reinforcement learning, we often define goals by specifying rewards within\ndesirable states. One problem with this approach is that we typically need to\nredefine the rewards each time the goal changes, which often requires some\nunderstanding of the solution in the agents environment. When humans are\nlearning to complete tasks, we regularly utilize alternative sources that guide\nour understanding of the problem. Such task representations allow one to\nspecify goals on their own terms, thus providing specifications that can be\nappropriately interpreted across various environments. This motivates our own\nwork, in which we represent goals in environments that are different from the\nagents. We introduce Cross-Domain Perceptual Reward (CDPR) functions, learned\nrewards that represent the visual similarity between an agents state and a\ncross-domain goal image. We report results for learning the CDPRs with a deep\nneural network and using them to solve two tasks with deep reinforcement\nlearning.", "authors": ["Ashley D. Edwards", "Srijan Sood", "Charles L. Isbell Jr"], "category": "cs.AI", "comment": "A shorter version of this paper was accepted to RLDM\n  (http://rldm.org/rldm2017/)", "img": "/static/thumbs/1705.09045v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1705.09045v3", "num_discussion": 0, "originally_published_time": "5/25/2017", "pid": "1705.09045v3", "published_time": "7/25/2017", "rawpid": "1705.09045", "tags": ["cs.AI"], "title": "Cross-Domain Perceptual Reward Functions"}, {"abstract": "We present a Sign Language modelling approach allowing to build grammars and\ncreate linguistic input for Sign synthesis through avatars. We comment on the\ntype of grammar it allows to build, and observe a resemblance between the\nresulting expressions and traditional semantic representations. Comparing the\nways in which the paradigms are designed, we name and contrast two essentially\ndifferent strategies for building higher-level linguistic input:\n\"source-and-forward\" vs. \"target-and-back\". We conclude by favouring the\nlatter, acknowledging the power of being able to automatically generate output\nfrom semantically relevant input straight into articulations of the target\nlanguage.", "authors": ["Michael Filhol", "Gilles Falquet"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1707.08041v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08041v1", "num_discussion": 0, "originally_published_time": "7/25/2017", "pid": "1707.08041v1", "published_time": "7/25/2017", "rawpid": "1707.08041", "tags": ["cs.CL"], "title": "Synthesising Sign Language from semantics, approaching \"from the target\n  and back\""}, {"abstract": "Automatic liver segmentation in 3D medical images is essential in many\nclinical applications, such as pathological diagnosis of hepatic diseases,\nsurgical planning, and postoperative assessment. However, it is still a very\nchallenging task due to the complex background, fuzzy boundary, and various\nappearance of liver. In this paper, we propose an automatic and efficient\nalgorithm to segment liver from 3D CT volumes. A deep image-to-image network\n(DI2IN) is first deployed to generate the liver segmentation, employing a\nconvolutional encoder-decoder architecture combined with multi-level feature\nconcatenation and deep supervision. Then an adversarial network is utilized\nduring training process to discriminate the output of DI2IN from ground truth,\nwhich further boosts the performance of DI2IN. The proposed method is trained\non an annotated dataset of 1000 CT volumes with various different scanning\nprotocols (e.g., contrast and non-contrast, various resolution and position)\nand large variations in populations (e.g., ages and pathology). Our approach\noutperforms the state-of-the-art solutions in terms of segmentation accuracy\nand computing efficiency.", "authors": ["Dong Yang", "Daguang Xu", "S. Kevin Zhou", "Bogdan Georgescu", "Mingqing Chen", "Sasa Grbic", "Dimitris Metaxas", "Dorin Comaniciu"], "category": "cs.CV", "comment": "Accepted by MICCAI 2017", "img": "/static/thumbs/1707.08037v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08037v1", "num_discussion": 0, "originally_published_time": "7/25/2017", "pid": "1707.08037v1", "published_time": "7/25/2017", "rawpid": "1707.08037", "tags": ["cs.CV"], "title": "Automatic Liver Segmentation Using an Adversarial Image-to-Image Network"}, {"abstract": "Academic research in the field of recommender systems mainly focuses on the\nproblem of maximizing the users\u0027 utility by trying to identify the most\nrelevant items for each user. However, such items are not necessarily the ones\nthat maximize the utility of the service provider (e.g., an online retailer) in\nterms of the business value, such as profit. One approach to increasing the\nproviders\u0027 utility is to incorporate purchase-oriented information, e.g., the\nprice, sales probabilities, and the resulting profit, into the recommendation\nalgorithms. In this paper we specifically focus on price- and profit-aware\nrecommender systems. We provide a brief overview of the relevant literature and\nuse numerical simulations to illustrate the potential business benefit of such\napproaches.", "authors": ["Dietmar Jannach", "Gediminas Adomavicius"], "category": "cs.IR", "comment": "Presented at the 2017 Workshop on Value-Aware and Multi-Stakeholder\n  Recommendation (VAMS) collocat...", "img": "/static/thumbs/1707.08029v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08029v1", "num_discussion": 0, "originally_published_time": "7/25/2017", "pid": "1707.08029v1", "published_time": "7/25/2017", "rawpid": "1707.08029", "tags": ["cs.IR", "cs.AI"], "title": "Price and Profit Awareness in Recommender Systems"}, {"abstract": "Each year, thousands of software vulnerabilities are discovered and reported\nto the public. Unpatched known vulnerabilities are a significant security risk.\nIt is imperative that software vendors quickly provide patches once\nvulnerabilities are known and users quickly install those patches as soon as\nthey are available. However, most vulnerabilities are never actually exploited.\nSince writing, testing, and installing software patches can involve\nconsiderable resources, it would be desirable to prioritize the remediation of\nvulnerabilities that are likely to be exploited. Several published research\nstudies have reported moderate success in applying machine learning techniques\nto the task of predicting whether a vulnerability will be exploited. These\napproaches typically use features derived from vulnerability databases (such as\nthe summary text describing the vulnerability) or social media posts that\nmention the vulnerability by name. However, these prior studies share multiple\nmethodological shortcomings that inflate predictive power of these approaches.\nWe replicate key portions of the prior work, compare their approaches, and show\nhow selection of training and test data critically affect the estimated\nperformance of predictive models. The results of this study point to important\nmethodological considerations that should be taken into account so that results\nreflect real-world utility.", "authors": ["Benjamin L. Bullough", "Anna K. Yanchenko", "Christopher L. Smith", "Joseph R. Zipkin"], "category": "cs.CR", "comment": "", "img": "/static/thumbs/1707.08015v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08015v1", "num_discussion": 0, "originally_published_time": "7/25/2017", "pid": "1707.08015v1", "published_time": "7/25/2017", "rawpid": "1707.08015", "tags": ["cs.CR", "stat.AP", "stat.ML"], "title": "Predicting Exploitation of Disclosed Software Vulnerabilities Using\n  Open-source Data"}, {"abstract": "We present a novel data-driven algorithm to synthesize high-resolution flow\nsimulations with reusable repositories of space-time flow data. In our work, we\nemploy a descriptor learning approach to encode the similarity between fluid\nregions with differences in resolution and numerical viscosity. We use\nconvolutional neural networks to generate the descriptors from fluid data such\nas smoke density and flow velocity. At the same time, we present a deformation\nlimiting patch advection method which allows us to robustly track deformable\nfluid regions. With the help of this patch advection, we generate stable\nspace-time data sets from detailed fluids for our repositories. We can then use\nour learned descriptors to quickly localize a suitable data set when running a\nnew simulation. This makes our approach very efficient, and resolution\nindependent. We will demonstrate with several examples that our method yields\nvolumes with very high effective resolutions, and non-dissipative small scale\ndetails that naturally integrate into the motions of the underlying flow.", "authors": ["Mengyu Chu", "Nils Thuerey"], "category": "cs.GR", "comment": "14 pages, 17 figures, to appear at SIGGRAPH 2017, v2 only fixes small\n  typos", "img": "/static/thumbs/1705.01425v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1705.01425v2", "num_discussion": 0, "originally_published_time": "5/3/2017", "pid": "1705.01425v2", "published_time": "7/25/2017", "rawpid": "1705.01425", "tags": ["cs.GR", "cs.LG"], "title": "Data-Driven Synthesis of Smoke Flows with CNN-based Feature Descriptors"}, {"abstract": "We study zero-shot learning (ZSL) as a transfer learning problem, and focus\non the two key aspects of ZSL, model effectiveness and model adaptation. For\neffective modeling, we adopt the boosting strategy to learn a zero-shot\nclassifier from weak models to a strong model. For adaptable knowledge\ntransfer, we devise a Semantic Correlation Regularization (SCR) approach to\nregularize the boosted model to be consistent with the inter-class semantic\ncorrelations. With SCR embedded in the boosting objective, and with a\nself-controlled sample selection for learning robustness, we propose a unified\nframework, Boosted Zero-shot classification with Semantic Correlation\nRegularization (BZ-SCR). By balancing the SCR-regularized boosted model\nselection and the self-controlled sample selection, BZ-SCR is capable of\ncapturing both discriminative and adaptable feature-to-class semantic\nalignments, while ensuring the reliability and adaptability of the learned\nsamples. The experiments on two ZSL datasets show the superiority of BZ-SCR\nover the state-of-the-arts.", "authors": ["Te Pi", "Xi Li", "Zhongfei", "Zhang"], "category": "cs.LG", "comment": "7 pages; IJCAI 2017", "img": "/static/thumbs/1707.08008v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08008v1", "num_discussion": 0, "originally_published_time": "7/25/2017", "pid": "1707.08008v1", "published_time": "7/25/2017", "rawpid": "1707.08008", "tags": ["cs.LG"], "title": "Boosted Zero-Shot Learning with Semantic Correlation Regularization"}, {"abstract": "We propose a novel framework for abnormal event detection in video that\nrequires no training sequences. Our framework is based on unmasking, a\ntechnique previously used for authorship verification in text documents, which\nwe adapt to our task. We iteratively train a binary classifier to distinguish\nbetween two consecutive video sequences while removing at each step the most\ndiscriminant features. Higher training accuracy rates of the intermediately\nobtained classifiers represent abnormal events. To the best of our knowledge,\nthis is the first work to apply unmasking for a computer vision task. We\ncompare our method with several state-of-the-art supervised and unsupervised\nmethods on four benchmark data sets. The empirical results indicate that our\nabnormal event detection framework can achieve state-of-the-art results, while\nrunning in real-time at 20 frames per second.", "authors": ["Radu Tudor Ionescu", "Sorina Smeureanu", "Bogdan Alexe", "Marius Popescu"], "category": "cs.CV", "comment": "Accepted at the 2017 International Conference on Computer Vision\n  (ICCV 2017)", "img": "/static/thumbs/1705.08182v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1705.08182v3", "num_discussion": 0, "originally_published_time": "5/23/2017", "pid": "1705.08182v3", "published_time": "7/25/2017", "rawpid": "1705.08182", "tags": ["cs.CV"], "title": "Unmasking the abnormal events in video"}, {"abstract": "Compressing convolutional neural networks (CNNs) is essential for\ntransferring the success of CNNs to a wide variety of applications to mobile\ndevices. In contrast to directly recognizing subtle weights or filters as\nredundant in a given CNN, this paper presents an evolutionary method to\nautomatically eliminate redundant convolution filters. We represent each\ncompressed network as a binary individual of specific fitness. Then, the\npopulation is upgraded at each evolutionary iteration using genetic operations.\nAs a result, an extremely compact CNN is generated using the fittest\nindividual. In this approach, either large or small convolution filters can be\nredundant, and filters in the compressed network are more distinct. In\naddition, since the number of filters in each convolutional layer is reduced,\nthe number of filter channels and the size of feature maps are also decreased,\nnaturally improving both the compression and speed-up ratios. Experiments on\nbenchmark deep CNN models suggest the superiority of the proposed algorithm\nover the state-of-the-art compression methods.", "authors": ["Yunhe Wang", "Chang Xu", "Jiayan Qiu", "Chao Xu", "Dacheng Tao"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1707.08005v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08005v1", "num_discussion": 0, "originally_published_time": "7/25/2017", "pid": "1707.08005v1", "published_time": "7/25/2017", "rawpid": "1707.08005", "tags": ["stat.ML", "cs.LG"], "title": "Towards Evolutional Compression"}, {"abstract": "Processing and publishing the data of the historical sciences in the semantic\nweb is an interesting challenge in which the representation of temporal aspects\nplays a key role. We propose in this paper a model of temporal knowledge\nrepresentation adapted to work on historical documents. This model is based on\nthe notion of fluent that is represented in RDF graphs. We show how this model\nallows to represent the knowledge necessary to the historians and how it can be\nused to reason on this knowledge using the SWRL and SPARQL languages. This\nmodel is being used in a project to digitize, study and publish the manuscripts\nof linguist Ferdinand de Saussure.", "authors": ["Sahar Aljalbout", "Gilles Falquet"], "category": "cs.AI", "comment": "in French, IC\\_2017 - 28\\`emes Journ\\\u0027ees francophones d\u0027Ing\\\u0027enierie\n  des Connaissances, Jul 2017,...", "img": "/static/thumbs/1707.08000v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08000v1", "num_discussion": 0, "originally_published_time": "7/25/2017", "pid": "1707.08000v1", "published_time": "7/25/2017", "rawpid": "1707.08000", "tags": ["cs.AI"], "title": "Un mod\u00e8le pour la repr\u00e9sentation des connaissances temporelles dans\n  les documents historiques"}, {"abstract": "The theory of belief functions is an effective tool to deal with the multiple\nuncertain information. In recent years, many evidence combination rules have\nbeen proposed in this framework, such as the conjunctive rule, the cautious\nrule, the PCR (Proportional Conflict Redistribution) rules and so on. These\nrules can be adopted for different types of sources. However, most of these\nrules are not applicable when the number of sources is large. This is due to\neither the complexity or the existence of an absorbing element (such as the\ntotal conflict mass function for the conjunctive-based rules when applied on\nunreliable evidence). In this paper, based on the assumption that the majority\nof sources are reliable, a combination rule for a large number of sources,\nnamed LNS (stands for Large Number of Sources), is proposed on the basis of a\nsimple idea: the more common ideas one source shares with others, the\nmorereliable the source is. This rule is adaptable for aggregating a large\nnumber of sources among which some are unreliable. It will keep the spirit of\nthe conjunctive rule to reinforce the belief on the focal elements with which\nthe sources are in agreement. The mass on the empty set will be kept as an\nindicator of the conflict. Moreover, it can be used to elicit the major opinion\namong the experts. The experimental results on synthetic mass functionsverify\nthat the rule can be effectively used to combine a large number of mass\nfunctions and to elicit the major opinion.", "authors": ["Kuang Zhou", "Arnaud Martin", "Quan Pan"], "category": "cs.AI", "comment": "2017 20th International Conference on Information Fusion (FUSION),\n  Jul 2017, Xi\u0027an, China", "img": "/static/thumbs/1707.07999v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.07999v1", "num_discussion": 0, "originally_published_time": "7/25/2017", "pid": "1707.07999v1", "published_time": "7/25/2017", "rawpid": "1707.07999", "tags": ["cs.AI"], "title": "Evidence combination for a large number of sources"}, {"abstract": "Top-down visual attention mechanisms have been used extensively in image\ncaptioning and visual question answering (VQA) to enable deeper image\nunderstanding through fine-grained analysis and even multiple steps of\nreasoning. In this work, we propose a combined bottom-up and top-down attention\nmechanism that enables attention to be calculated at the level of objects and\nother salient image regions. This is the natural basis for attention to be\nconsidered. Within our approach, the bottom-up mechanism (based on Faster\nR-CNN) proposes image regions, each with an associated feature vector, while\nthe top-down mechanism determines feature weightings. Applying this approach to\nimage captioning, our results on the MSCOCO test server establish a new\nstate-of-the-art for the task, improving the best published result in terms of\nCIDEr score from 114.7 to 117.9 and BLEU-4 from 35.2 to 36.9. Demonstrating the\nbroad applicability of the method, applying the same approach to VQA we obtain\na new state-of-the-art on the VQA v2.0 dataset with 70.2% overall accuracy.", "authors": ["Peter Anderson", "Xiaodong He", "Chris Buehler", "Damien Teney", "Mark Johnson", "Stephen Gould", "Lei Zhang"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1707.07998v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.07998v1", "num_discussion": 0, "originally_published_time": "7/25/2017", "pid": "1707.07998v1", "published_time": "7/25/2017", "rawpid": "1707.07998", "tags": ["cs.CV"], "title": "Bottom-Up and Top-Down Attention for Image Captioning and VQA"}, {"abstract": "Minimizing the nuclear norm of a matrix has been shown to be very efficient\nin reconstructing a low-rank sampled matrix. Furthermore, minimizing the sum of\nnuclear norms of matricizations of a tensor has been shown to be very efficient\nin recovering a low-Tucker-rank sampled tensor. In this paper, we propose to\nrecover a low-TT-rank sampled tensor by minimizing a weighted sum of nuclear\nnorms of unfoldings of the tensor. We provide numerical results to show that\nour proposed method requires significantly less number of samples to recover to\nthe original tensor in comparison with simply minimizing the sum of nuclear\nnorms since the structure of the unfoldings in the TT tensor model is\nfundamentally different from that of matricizations in the Tucker tensor model.", "authors": ["Morteza Ashraphijuo", "Xiaodong Wang"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1707.07976v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.07976v1", "num_discussion": 0, "originally_published_time": "7/25/2017", "pid": "1707.07976v1", "published_time": "7/25/2017", "rawpid": "1707.07976", "tags": ["stat.ML", "cs.NA"], "title": "Scaled Nuclear Norm Minimization for Low-Rank Tensor Completion"}, {"abstract": "This work falls within the context of predicting the value of a real function\nat some input locations given a limited number of observations of this\nfunction. The Kriging interpolation technique (or Gaussian process regression)\nis often considered to tackle such a problem but the method suffers from its\ncomputational burden when the number of observation points is large. We\nintroduce in this article nested Kriging predictors which are constructed by\naggregating sub-models based on subsets of observation points. This approach is\nproven to have better theoretical properties than other aggregation methods\nthat can be found in the literature. Contrarily to some other methods it can be\nshown that the proposed aggregation method is consistent. Finally, the\npractical interest of the proposed method is illustrated on simulated datasets\nand on an industrial test case with $10^4$ observations in a 6-dimensional\nspace.", "authors": ["Didier Rulli\u00e8re", "Nicolas Durrande", "Fran\u00e7ois Bachoc", "Cl\u00e9ment Chevalier"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1607.05432v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1607.05432v3", "num_discussion": 0, "originally_published_time": "7/19/2016", "pid": "1607.05432v3", "published_time": "7/25/2017", "rawpid": "1607.05432", "tags": ["stat.ML"], "title": "Nested Kriging predictions for datasets with large number of\n  observations"}, {"abstract": "We introduce $\\mathcal{DLR}^+$, an extension of the n-ary propositionally\nclosed description logic $\\mathcal{DLR}$ to deal with attribute-labelled tuples\n(generalising the positional notation), projections of relations, and global\nand local objectification of relations, able to express inclusion, functional,\nkey, and external uniqueness dependencies. The logic is equipped with both TBox\nand ABox axioms. We show how a simple syntactic restriction on the appearance\nof projections sharing common attributes in a $\\mathcal{DLR}^+$ knowledge base\nmakes reasoning in the language decidable with the same computational\ncomplexity as $\\mathcal{DLR}$. The obtained $\\mathcal{DLR}^\\pm$ n-ary\ndescription logic is able to encode more thoroughly conceptual data models such\nas EER, UML, and ORM.", "authors": ["Alessandro Artale", "Enrico Franconi", "Rafael Pe\u00f1aloza", "Francesco Sportelli"], "category": "cs.AI", "comment": "20 pages. Extended version of paper appearing in the International\n  Semantic Web Conference (ISWC 2...", "img": "/static/thumbs/1707.08468v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08468v1", "num_discussion": 0, "originally_published_time": "7/25/2017", "pid": "1707.08468v1", "published_time": "7/25/2017", "rawpid": "1707.08468", "tags": ["cs.AI"], "title": "A Decidable Very Expressive Description Logic for Databases (Extended\n  Version)"}];
var pid_to_users = {};
var msg = "Showing most recent Arxiv papers:";
var render_format = "recent";
var username = "";
var numresults = "33031";
var show_prompt = "no";

var urlq = ''; // global will be read in to QueryString when load is done

// when page loads...
$(document).ready(function(){

	urlq = QueryString.q;

  // display message, if any
  if(msg !== '') { d3.select("#rtable").append('div').classed('msg', true).html(msg); }

  // add papers to #rtable
	var done = addPapers(10, false);
  if(done) { $("#loadmorebtn").hide(); }

  // set up inifinite scrolling for adding more papers
  $(window).on('scroll', function(){
    var scrollTop = $(document).scrollTop();
    var windowHeight = $(window).height();
    var bodyHeight = $(document).height() - windowHeight;
    var scrollPercentage = (scrollTop / bodyHeight);
    if(scrollPercentage > 0.9) {
      var done = addPapers(5, true);
      if(done) { $("#loadmorebtn").hide(); }
    }
  });

  // just in case scrolling is broken somehow, provide a button handler explicit
  $("#loadmorebtn").on('click', function(){
    var done = addPapers(5, true);
    if(done) { $("#loadmorebtn").hide(); }
  });

  if(papers.length === 0) { $("#loadmorebtn").hide(); }

	if(!(typeof urlq == 'undefined')) {
		d3.select("#qfield").attr('value', urlq.replace(/\+/g, " "));
	}

  var vf = QueryString.vfilter; if(typeof vf === 'undefined') { vf = 'all'; }
  var tf = QueryString.timefilter; if(typeof tf === 'undefined') { tf = 'week'; }
  var link_endpoint = '/';
  if(render_format === 'recent') { link_endpoint = ''; }
  if(render_format === 'top') { link_endpoint = 'top'; }
  if(render_format === 'recommend') { link_endpoint = 'recommend'; }
  if(render_format === 'friends') { link_endpoint = 'friends'; }
  if(render_format === 'toptwtr') { link_endpoint = 'toptwtr'; }
  if(render_format === 'discussions') { link_endpoint = 'discussions'; }

  var time_ranges = ['day', '3days', 'week', 'month', 'year', 'alltime'];
  var time_txt = {'day':'Last day', '3days': 'Last 3 days', 'week': 'Last week', 'month': 'Last month', 'year': 'Last year', 'alltime': 'All time'}
  var time_range = tf;

  // set up time filtering options
  if(render_format === 'recommend' || render_format === 'top' || render_format === 'recent' || render_format === 'friends') {
    // insert version filtering options for these views
    var elt = d3.select('#recommend-time-choice');
    var vflink = vf === 'all' ? '1' : 'all'; // toggle only showing v1 or not
    if(render_format === 'recent') {
      var aelt = elt.append('a').attr('href', '/'+link_endpoint+'?'+'&vfilter='+vflink); // leave out timefilter from this page
    } else {
      var aelt = elt.append('a').attr('href', '/'+link_endpoint+'?'+'timefilter='+time_range+'&vfilter='+vflink);
    }
    var delt = aelt.append('div').classed('vchoice', true).html('Only show v1');
    if(vf === '1') { delt.classed('vchoice-selected', true); }
  }

  // time choices for recommend/top
  if(render_format === 'recommend' || render_format === 'top' || render_format === 'friends') {
    // insert time filtering options for these two views
    var elt = d3.select('#recommend-time-choice');
    elt.append('div').classed('fdivider', true).html('|');
    for(var i=0;i<time_ranges.length;i++) {
      var time_range = time_ranges[i];
      var aelt = elt.append('a').attr('href', '/'+link_endpoint+'?'+'timefilter='+time_range+'&vfilter='+vf);
      var delt = aelt.append('div').classed('timechoice', true).html(time_txt[time_range]);
      if(tf == time_range) { delt.classed('timechoice-selected', true); } // also render as chosen
    }
  }

  // time choices for top tweets
  if(render_format === 'toptwtr') {
    var tf = QueryString.timefilter; if(typeof tf === 'undefined') { tf = 'day'; } // default here is day
    var time_ranges = ['day', 'week', 'month'];
    var elt = d3.select('#recommend-time-choice');
    for(var i=0;i<time_ranges.length;i++) {
      var time_range = time_ranges[i];
      var aelt = elt.append('a').attr('href', '/'+link_endpoint+'?'+'timefilter='+time_range);
      var delt = aelt.append('div').classed('timechoice', true).html(time_txt[time_range]);
      if(tf == time_range) { delt.classed('timechoice-selected', true); } // also render as chosen
    }
  }

  var xb = $("#xbanner");
  if(xb.length !== 0) {
    xb.click(function(){ $("#banner").slideUp('fast'); })
  }

  // in top tab: color current choice
  if( render_format === 'recent') { d3.select('#tabrecent').classed('tab-selected', true); }
  if( render_format === 'top') { d3.select('#tabtop').classed('tab-selected', true); }
  if( render_format === 'toptwtr') { d3.select('#tabtwtr').classed('tab-selected', true); }
  if( render_format === 'friends') { d3.select('#tabfriends').classed('tab-selected', true); }
  if( render_format === 'discussions') { d3.select('#tabdiscussions').classed('tab-selected', true); }
  if( render_format === 'recommend') { d3.select('#tabrec').classed('tab-selected', true); }
  if( render_format === 'library') { d3.select('#tablib').classed('tab-selected', true); }

  $("#goaway").on('click', function(){
    $("#prompt").slideUp('fast');
    $.post("/goaway", {}).done(function(data){ });
  });
});

</script>
</head>

<body>
<a href="https://github.com/karpathy/arxiv-sanity-preserver"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://camo.githubusercontent.com/38ef81f8aca64bb9a64448d0d70f1308ef5341ab/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f72696768745f6461726b626c75655f3132313632312e706e67" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png"></a>

<div id ="titdiv">

  <!-- User account information on top right -->
  <div id="userinfo">
    
    <form action="/login" method="post">
      User:
      <input type="text" name="username" class="input-no-border">
      Pass:
      <input type="password" name="password" class="input-no-border">
      <input type="submit" value="Login or Create" class="btn-fancy">
    </form>
    
  </div>

  <!-- Site information/banner on top left -->
	<a href="/">
	<div id="tittxt">
		<h1>Arxiv Sanity Preserver</h1>
		Built in spare time by <a href="https://twitter.com/karpathy">@karpathy</a> to accelerate research.<br>
		Serving last 33031 papers from cs.[CV|CL|LG|AI|NE]/stat.ML
	</div>
	</a>
</div>

<div id="flashesdiv">

    

</div>


<div id="banner">
  <div style="float:right;cursor:pointer;" id="xbanner">X</div>
  New to arxiv-sanity? Check out the <a href="https://youtu.be/S2GY3gh6qC8" target="_blank">introduction video</a>.
</div>


<div id="sbox">
  <form action="/search" method="get">
  	<input name="q" type="text" id="qfield">
  </form>
  <div id="search_hint"></div>
</div>



<div id="pagebar">
  <div class="pagelink" id="tabrecent"><a href="/">most recent</a></div>
  <div class="pagelink" id="tabtop"><a href="/top">top recent</a></div>
  <div class="pagelink" id="tabtwtr"><a href="/toptwtr">top hype</a></div>
  <div class="pagelink" id="tabfriends"><a href="/friends">friends</a></div>
  <div class="pagelink" id="tabdiscussions"><a href="/discussions">discussions</a></div>
  <div class="pagelink" id="tabrec"><a href="/recommend">recommended</a></div>
  <div class="pagelink" id="tablib"><a href="/library">library</a></div>
</div>

<!-- this div will be rendered into dynamcially at init with JS -->
<div id="recommend-time-choice" class="centerdiv"></div>

<div id="maindiv">

<div id="rtable"></div>

<div id="loadmore">
  <button id="loadmorebtn">Load more</button>
</div>

</div>

<br><br><br><br><br><br>
</body>

</html>