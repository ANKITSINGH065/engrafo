<!DOCTYPE HTML>
<html>

<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Arxiv Sanity Preserver</title>

<!-- MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML">
</script>

<!-- CSS -->
<link rel="stylesheet" type="text/css" href="/static/style.css">

<!-- Favicon -->
<link rel="shortcut icon" type="image/png" href="/static/favicon.png" />

<!-- JS -->
<script src="/static/jquery-1.8.3.min.js"></script>
<script src="/static/d3.min.js"></script>
<script src="/static/as-common.js"></script>

<!-- Google Analytics JS -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-3698471-25', 'auto');
  ga('send', 'pageview');

</script>

<script>

// passed in from flask as json
var tweets = [];
var papers = [{"abstract": "This report summarizes the tutorial presented by the author at NIPS 2016 on\ngenerative adversarial networks (GANs). The tutorial describes: (1) Why\ngenerative modeling is a topic worth studying, (2) how generative models work,\nand how GANs compare to other generative models, (3) the details of how GANs\nwork, (4) research frontiers in GANs, and (5) state-of-the-art image models\nthat combine GANs with other methods. Finally, the tutorial contains three\nexercises for readers to complete, and the solutions to these exercises.", "authors": ["Ian Goodfellow"], "category": "cs.LG", "comment": "v2-v4 are all typo fixes. No substantive changes relative to v1", "img": "/static/thumbs/1701.00160v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1701.00160v4", "num_discussion": 0, "originally_published_time": "12/31/2016", "pid": "1701.00160v4", "published_time": "4/3/2017", "rawpid": "1701.00160", "tags": ["cs.LG"], "title": "NIPS 2016 Tutorial: Generative Adversarial Networks"}, {"abstract": "Very deep convolutional networks have been central to the largest advances in\nimage recognition performance in recent years. One example is the Inception\narchitecture that has been shown to achieve very good performance at relatively\nlow computational cost. Recently, the introduction of residual connections in\nconjunction with a more traditional architecture has yielded state-of-the-art\nperformance in the 2015 ILSVRC challenge; its performance was similar to the\nlatest generation Inception-v3 network. This raises the question of whether\nthere are any benefit in combining the Inception architecture with residual\nconnections. Here we give clear empirical evidence that training with residual\nconnections accelerates the training of Inception networks significantly. There\nis also some evidence of residual Inception networks outperforming similarly\nexpensive Inception networks without residual connections by a thin margin. We\nalso present several new streamlined architectures for both residual and\nnon-residual Inception networks. These variations improve the single-frame\nrecognition performance on the ILSVRC 2012 classification task significantly.\nWe further demonstrate how proper activation scaling stabilizes the training of\nvery wide residual Inception networks. With an ensemble of three residual and\none Inception-v4, we achieve 3.08 percent top-5 error on the test set of the\nImageNet classification (CLS) challenge", "authors": ["Christian Szegedy", "Sergey Ioffe", "Vincent Vanhoucke", "Alex Alemi"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1602.07261v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1602.07261v2", "num_discussion": 0, "originally_published_time": "2/23/2016", "pid": "1602.07261v2", "published_time": "8/23/2016", "rawpid": "1602.07261", "tags": ["cs.CV"], "title": "Inception-v4, Inception-ResNet and the Impact of Residual Connections on\n  Learning"}, {"abstract": "We present a conceptually simple, flexible, and general framework for object\ninstance segmentation. Our approach efficiently detects objects in an image\nwhile simultaneously generating a high-quality segmentation mask for each\ninstance. The method, called Mask R-CNN, extends Faster R-CNN by adding a\nbranch for predicting an object mask in parallel with the existing branch for\nbounding box recognition. Mask R-CNN is simple to train and adds only a small\noverhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to\ngeneralize to other tasks, e.g., allowing us to estimate human poses in the\nsame framework. We show top results in all three tracks of the COCO suite of\nchallenges, including instance segmentation, bounding-box object detection, and\nperson keypoint detection. Without tricks, Mask R-CNN outperforms all existing,\nsingle-model entries on every task, including the COCO 2016 challenge winners.\nWe hope our simple and effective approach will serve as a solid baseline and\nhelp ease future research in instance-level recognition. Code will be made\navailable.", "authors": ["Kaiming He", "Georgia Gkioxari", "Piotr Doll\u00e1r", "Ross Girshick"], "category": "cs.CV", "comment": "Technical report", "img": "/static/thumbs/1703.06870v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.06870v2", "num_discussion": 0, "originally_published_time": "3/20/2017", "pid": "1703.06870v2", "published_time": "4/5/2017", "rawpid": "1703.06870", "tags": ["cs.CV"], "title": "Mask R-CNN"}, {"abstract": "Deeper neural networks are more difficult to train. We present a residual\nlearning framework to ease the training of networks that are substantially\ndeeper than those used previously. We explicitly reformulate the layers as\nlearning residual functions with reference to the layer inputs, instead of\nlearning unreferenced functions. We provide comprehensive empirical evidence\nshowing that these residual networks are easier to optimize, and can gain\naccuracy from considerably increased depth. On the ImageNet dataset we evaluate\nresidual nets with a depth of up to 152 layers---8x deeper than VGG nets but\nstill having lower complexity. An ensemble of these residual nets achieves\n3.57% error on the ImageNet test set. This result won the 1st place on the\nILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100\nand 1000 layers.\n  The depth of representations is of central importance for many visual\nrecognition tasks. Solely due to our extremely deep representations, we obtain\na 28% relative improvement on the COCO object detection dataset. Deep residual\nnets are foundations of our submissions to ILSVRC \u0026 COCO 2015 competitions,\nwhere we also won the 1st places on the tasks of ImageNet detection, ImageNet\nlocalization, COCO detection, and COCO segmentation.", "authors": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "category": "cs.CV", "comment": "Tech report", "img": "/static/thumbs/1512.03385v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1512.03385v1", "num_discussion": 0, "originally_published_time": "12/10/2015", "pid": "1512.03385v1", "published_time": "12/10/2015", "rawpid": "1512.03385", "tags": ["cs.CV"], "title": "Deep Residual Learning for Image Recognition"}, {"abstract": "Deep Learning has revolutionized vision via convolutional neural networks\n(CNNs) and natural language processing via recurrent neural networks (RNNs).\nHowever, success stories of Deep Learning with standard feed-forward neural\nnetworks (FNNs) are rare. FNNs that perform well are typically shallow and,\ntherefore cannot exploit many levels of abstract representations. We introduce\nself-normalizing neural networks (SNNs) to enable high-level abstract\nrepresentations. While batch normalization requires explicit normalization,\nneuron activations of SNNs automatically converge towards zero mean and unit\nvariance. The activation function of SNNs are \"scaled exponential linear units\"\n(SELUs), which induce self-normalizing properties. Using the Banach fixed-point\ntheorem, we prove that activations close to zero mean and unit variance that\nare propagated through many network layers will converge towards zero mean and\nunit variance -- even under the presence of noise and perturbations. This\nconvergence property of SNNs allows to (1) train deep networks with many\nlayers, (2) employ strong regularization, and (3) to make learning highly\nrobust. Furthermore, for activations not close to unit variance, we prove an\nupper and lower bound on the variance, thus, vanishing and exploding gradients\nare impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning\nrepository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with\nstandard FNNs and other machine learning methods such as random forests and\nsupport vector machines. SNNs significantly outperformed all competing FNN\nmethods at 121 UCI tasks, outperformed all competing methods at the Tox21\ndataset, and set a new record at an astronomy data set. The winning SNN\narchitectures are often very deep. Implementations are available at:\ngithub.com/bioinf-jku/SNNs.", "authors": ["G\u00fcnter Klambauer", "Thomas Unterthiner", "Andreas Mayr", "Sepp Hochreiter"], "category": "cs.LG", "comment": "9 pages (+ 93 pages appendix)", "img": "/static/thumbs/1706.02515v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1706.02515v3", "num_discussion": 0, "originally_published_time": "6/8/2017", "pid": "1706.02515v3", "published_time": "6/22/2017", "rawpid": "1706.02515", "tags": ["cs.LG", "stat.ML"], "title": "Self-Normalizing Neural Networks"}, {"abstract": "The move from hand-designed features to learned features in machine learning\nhas been wildly successful. In spite of this, optimization algorithms are still\ndesigned by hand. In this paper we show how the design of an optimization\nalgorithm can be cast as a learning problem, allowing the algorithm to learn to\nexploit structure in the problems of interest in an automatic way. Our learned\nalgorithms, implemented by LSTMs, outperform generic, hand-designed competitors\non the tasks for which they are trained, and also generalize well to new tasks\nwith similar structure. We demonstrate this on a number of tasks, including\nsimple convex problems, training neural networks, and styling images with\nneural art.", "authors": ["Marcin Andrychowicz", "Misha Denil", "Sergio Gomez", "Matthew W. Hoffman", "David Pfau", "Tom Schaul", "Brendan Shillingford", "Nando de Freitas"], "category": "cs.NE", "comment": "", "img": "/static/thumbs/1606.04474v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1606.04474v2", "num_discussion": 0, "originally_published_time": "6/14/2016", "pid": "1606.04474v2", "published_time": "11/30/2016", "rawpid": "1606.04474", "tags": ["cs.NE", "cs.LG"], "title": "Learning to learn by gradient descent by gradient descent"}, {"abstract": "Recent work has shown that convolutional networks can be substantially\ndeeper, more accurate, and efficient to train if they contain shorter\nconnections between layers close to the input and those close to the output. In\nthis paper, we embrace this observation and introduce the Dense Convolutional\nNetwork (DenseNet), which connects each layer to every other layer in a\nfeed-forward fashion. Whereas traditional convolutional networks with L layers\nhave L connections - one between each layer and its subsequent layer - our\nnetwork has L(L+1)/2 direct connections. For each layer, the feature-maps of\nall preceding layers are used as inputs, and its own feature-maps are used as\ninputs into all subsequent layers. DenseNets have several compelling\nadvantages: they alleviate the vanishing-gradient problem, strengthen feature\npropagation, encourage feature reuse, and substantially reduce the number of\nparameters. We evaluate our proposed architecture on four highly competitive\nobject recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet).\nDenseNets obtain significant improvements over the state-of-the-art on most of\nthem, whilst requiring less memory and computation to achieve high performance.\nCode and models are available at https://github.com/liuzhuang13/DenseNet .", "authors": ["Gao Huang", "Zhuang Liu", "Kilian Q. Weinberger", "Laurens van der Maaten"], "category": "cs.CV", "comment": "12 pages", "img": "/static/thumbs/1608.06993v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1608.06993v3", "num_discussion": 0, "originally_published_time": "8/25/2016", "pid": "1608.06993v3", "published_time": "12/3/2016", "rawpid": "1608.06993", "tags": ["cs.CV", "cs.LG"], "title": "Densely Connected Convolutional Networks"}, {"abstract": "We introduce a method to train Binarized Neural Networks (BNNs) - neural\nnetworks with binary weights and activations at run-time. At training-time the\nbinary weights and activations are used for computing the parameters gradients.\nDuring the forward pass, BNNs drastically reduce memory size and accesses, and\nreplace most arithmetic operations with bit-wise operations, which is expected\nto substantially improve power-efficiency. To validate the effectiveness of\nBNNs we conduct two sets of experiments on the Torch7 and Theano frameworks. On\nboth, BNNs achieved nearly state-of-the-art results over the MNIST, CIFAR-10\nand SVHN datasets. Last but not least, we wrote a binary matrix multiplication\nGPU kernel with which it is possible to run our MNIST BNN 7 times faster than\nwith an unoptimized GPU kernel, without suffering any loss in classification\naccuracy. The code for training and running our BNNs is available on-line.", "authors": ["Matthieu Courbariaux", "Itay Hubara", "Daniel Soudry", "Ran El-Yaniv", "Yoshua Bengio"], "category": "cs.LG", "comment": "11 pages and 3 figures", "img": "/static/thumbs/1602.02830v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1602.02830v3", "num_discussion": 0, "originally_published_time": "2/9/2016", "pid": "1602.02830v3", "published_time": "3/17/2016", "rawpid": "1602.02830", "tags": ["cs.LG"], "title": "Binarized Neural Networks: Training Deep Neural Networks with Weights\n  and Activations Constrained to +1 or -1"}, {"abstract": "Deep residual networks have emerged as a family of extremely deep\narchitectures showing compelling accuracy and nice convergence behaviors. In\nthis paper, we analyze the propagation formulations behind the residual\nbuilding blocks, which suggest that the forward and backward signals can be\ndirectly propagated from one block to any other block, when using identity\nmappings as the skip connections and after-addition activation. A series of\nablation experiments support the importance of these identity mappings. This\nmotivates us to propose a new residual unit, which makes training easier and\nimproves generalization. We report improved results using a 1001-layer ResNet\non CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet.\nCode is available at: https://github.com/KaimingHe/resnet-1k-layers", "authors": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "category": "cs.CV", "comment": "ECCV 2016 camera-ready", "img": "/static/thumbs/1603.05027v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1603.05027v3", "num_discussion": 0, "originally_published_time": "3/16/2016", "pid": "1603.05027v3", "published_time": "7/25/2016", "rawpid": "1603.05027", "tags": ["cs.CV", "cs.LG"], "title": "Identity Mappings in Deep Residual Networks"}, {"abstract": "The dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks in an encoder-decoder configuration. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer, based\nsolely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to be\nsuperior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\nEnglish-to-German translation task, improving over the existing best results,\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\ntranslation task, our model establishes a new single-model state-of-the-art\nBLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction\nof the training costs of the best models from the literature. We show that the\nTransformer generalizes well to other tasks by applying it successfully to\nEnglish constituency parsing both with large and limited training data.", "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin"], "category": "cs.CL", "comment": "15 pages, 5 figures", "img": "/static/thumbs/1706.03762v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1706.03762v4", "num_discussion": 1, "originally_published_time": "6/12/2017", "pid": "1706.03762v4", "published_time": "6/30/2017", "rawpid": "1706.03762", "tags": ["cs.CL", "cs.LG"], "title": "Attention Is All You Need"}, {"abstract": "We propose two efficient approximations to standard convolutional neural\nnetworks: Binary-Weight-Networks and XNOR-Networks. In Binary-Weight-Networks,\nthe filters are approximated with binary values resulting in 32x memory saving.\nIn XNOR-Networks, both the filters and the input to convolutional layers are\nbinary. XNOR-Networks approximate convolutions using primarily binary\noperations. This results in 58x faster convolutional operations and 32x memory\nsavings. XNOR-Nets offer the possibility of running state-of-the-art networks\non CPUs (rather than GPUs) in real-time. Our binary networks are simple,\naccurate, efficient, and work on challenging visual tasks. We evaluate our\napproach on the ImageNet classification task. The classification accuracy with\na Binary-Weight-Network version of AlexNet is only 2.9% less than the\nfull-precision AlexNet (in top-1 measure). We compare our method with recent\nnetwork binarization methods, BinaryConnect and BinaryNets, and outperform\nthese methods by large margins on ImageNet, more than 16% in top-1 accuracy.", "authors": ["Mohammad Rastegari", "Vicente Ordonez", "Joseph Redmon", "Ali Farhadi"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1603.05279v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1603.05279v4", "num_discussion": 0, "originally_published_time": "3/16/2016", "pid": "1603.05279v4", "published_time": "8/2/2016", "rawpid": "1603.05279", "tags": ["cs.CV"], "title": "XNOR-Net: ImageNet Classification Using Binary Convolutional Neural\n  Networks"}, {"abstract": "In the last few years, deep learning has led to very good performance on a\nvariety of problems, such as visual recognition, speech recognition and natural\nlanguage processing. Among different types of deep neural networks,\nconvolutional neural networks have been most extensively studied. Due to the\nlack of training data and computing power in early days, it is hard to train a\nlarge high-capacity convolutional neural network without overfitting. After the\nrapid growth in the amount of the annotated data and the recent improvements in\nthe strengths of graphics processor units (GPUs), the research on convolutional\nneural networks has been emerged swiftly and achieved state-of-the-art results\non various tasks. In this paper, we provide a broad survey of the recent\nadvances in convolutional neural networks. Besides, we also introduce some\napplications of convolutional neural networks in computer vision.", "authors": ["Jiuxiang Gu", "Zhenhua Wang", "Jason Kuen", "Lianyang Ma", "Amir Shahroudy", "Bing Shuai", "Ting Liu", "Xingxing Wang", "Gang Wang"], "category": "cs.CV", "comment": "review, journal", "img": "/static/thumbs/1512.07108v5.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1512.07108v5", "num_discussion": 0, "originally_published_time": "12/22/2015", "pid": "1512.07108v5", "published_time": "1/5/2017", "rawpid": "1512.07108", "tags": ["cs.CV", "cs.LG", "cs.NE"], "title": "Recent Advances in Convolutional Neural Networks"}, {"abstract": "Deep residual networks were shown to be able to scale up to thousands of\nlayers and still have improving performance. However, each fraction of a\npercent of improved accuracy costs nearly doubling the number of layers, and so\ntraining very deep residual networks has a problem of diminishing feature\nreuse, which makes these networks very slow to train. To tackle these problems,\nin this paper we conduct a detailed experimental study on the architecture of\nResNet blocks, based on which we propose a novel architecture where we decrease\ndepth and increase width of residual networks. We call the resulting network\nstructures wide residual networks (WRNs) and show that these are far superior\nover their commonly used thin and very deep counterparts. For example, we\ndemonstrate that even a simple 16-layer-deep wide residual network outperforms\nin accuracy and efficiency all previous deep residual networks, including\nthousand-layer-deep networks, achieving new state-of-the-art results on CIFAR,\nSVHN, COCO, and significant improvements on ImageNet. Our code and models are\navailable at https://github.com/szagoruyko/wide-residual-networks", "authors": ["Sergey Zagoruyko", "Nikos Komodakis"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1605.07146v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1605.07146v4", "num_discussion": 0, "originally_published_time": "5/23/2016", "pid": "1605.07146v4", "published_time": "6/14/2017", "rawpid": "1605.07146", "tags": ["cs.CV", "cs.LG", "cs.NE"], "title": "Wide Residual Networks"}, {"abstract": "Learning both hierarchical and temporal representation has been among the\nlong-standing challenges of recurrent neural networks. Multiscale recurrent\nneural networks have been considered as a promising approach to resolve this\nissue, yet there has been a lack of empirical evidence showing that this type\nof models can actually capture the temporal dependencies by discovering the\nlatent hierarchical structure of the sequence. In this paper, we propose a\nnovel multiscale approach, called the hierarchical multiscale recurrent neural\nnetworks, which can capture the latent hierarchical structure in the sequence\nby encoding the temporal dependencies with different timescales using a novel\nupdate mechanism. We show some evidence that our proposed multiscale\narchitecture can discover underlying hierarchical structure in the sequences\nwithout using explicit boundary information. We evaluate our proposed model on\ncharacter-level language modelling and handwriting sequence modelling.", "authors": ["Junyoung Chung", "Sungjin Ahn", "Yoshua Bengio"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1609.01704v7.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1609.01704v7", "num_discussion": 0, "originally_published_time": "9/6/2016", "pid": "1609.01704v7", "published_time": "3/9/2017", "rawpid": "1609.01704", "tags": ["cs.LG"], "title": "Hierarchical Multiscale Recurrent Neural Networks"}, {"abstract": "The ability of the Generative Adversarial Networks (GANs) framework to learn\ngenerative models mapping from simple latent distributions to arbitrarily\ncomplex data distributions has been demonstrated empirically, with compelling\nresults showing that the latent space of such generators captures semantic\nvariation in the data distribution. Intuitively, models trained to predict\nthese semantic latent representations given data may serve as useful feature\nrepresentations for auxiliary problems where semantics are relevant. However,\nin their existing form, GANs have no means of learning the inverse mapping --\nprojecting data back into the latent space. We propose Bidirectional Generative\nAdversarial Networks (BiGANs) as a means of learning this inverse mapping, and\ndemonstrate that the resulting learned feature representation is useful for\nauxiliary supervised discrimination tasks, competitive with contemporary\napproaches to unsupervised and self-supervised feature learning.", "authors": ["Jeff Donahue", "Philipp Kr\u00e4henb\u00fchl", "Trevor Darrell"], "category": "cs.LG", "comment": "Published as a conference paper at ICLR 2017. Changelog: (v7) Table 2\n  results improved 1-2% due to...", "img": "/static/thumbs/1605.09782v7.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1605.09782v7", "num_discussion": 0, "originally_published_time": "5/31/2016", "pid": "1605.09782v7", "published_time": "4/3/2017", "rawpid": "1605.09782", "tags": ["cs.LG", "cs.AI", "cs.CV", "cs.NE", "stat.ML"], "title": "Adversarial Feature Learning"}, {"abstract": "This paper introduces WaveNet, a deep neural network for generating raw audio\nwaveforms. The model is fully probabilistic and autoregressive, with the\npredictive distribution for each audio sample conditioned on all previous ones;\nnonetheless we show that it can be efficiently trained on data with tens of\nthousands of samples per second of audio. When applied to text-to-speech, it\nyields state-of-the-art performance, with human listeners rating it as\nsignificantly more natural sounding than the best parametric and concatenative\nsystems for both English and Mandarin. A single WaveNet can capture the\ncharacteristics of many different speakers with equal fidelity, and can switch\nbetween them by conditioning on the speaker identity. When trained to model\nmusic, we find that it generates novel and often highly realistic musical\nfragments. We also show that it can be employed as a discriminative model,\nreturning promising results for phoneme recognition.", "authors": ["Aaron van den Oord", "Sander Dieleman", "Heiga Zen", "Karen Simonyan", "Oriol Vinyals", "Alex Graves", "Nal Kalchbrenner", "Andrew Senior", "Koray Kavukcuoglu"], "category": "cs.SD", "comment": "", "img": "/static/thumbs/1609.03499v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1609.03499v2", "num_discussion": 0, "originally_published_time": "9/12/2016", "pid": "1609.03499v2", "published_time": "9/19/2016", "rawpid": "1609.03499", "tags": ["cs.SD", "cs.LG"], "title": "WaveNet: A Generative Model for Raw Audio"}, {"abstract": "We propose a conceptually simple and lightweight framework for deep\nreinforcement learning that uses asynchronous gradient descent for optimization\nof deep neural network controllers. We present asynchronous variants of four\nstandard reinforcement learning algorithms and show that parallel\nactor-learners have a stabilizing effect on training allowing all four methods\nto successfully train neural network controllers. The best performing method,\nan asynchronous variant of actor-critic, surpasses the current state-of-the-art\non the Atari domain while training for half the time on a single multi-core CPU\ninstead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds\non a wide variety of continuous motor control problems as well as on a new task\nof navigating random 3D mazes using a visual input.", "authors": ["Volodymyr Mnih", "Adri\u00e0 Puigdom\u00e8nech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P. Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1602.01783v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1602.01783v2", "num_discussion": 0, "originally_published_time": "2/4/2016", "pid": "1602.01783v2", "published_time": "6/16/2016", "rawpid": "1602.01783", "tags": ["cs.LG"], "title": "Asynchronous Methods for Deep Reinforcement Learning"}, {"abstract": "Training state-of-the-art, deep neural networks is computationally expensive.\nOne way to reduce the training time is to normalize the activities of the\nneurons. A recently introduced technique called batch normalization uses the\ndistribution of the summed input to a neuron over a mini-batch of training\ncases to compute a mean and variance which are then used to normalize the\nsummed input to that neuron on each training case. This significantly reduces\nthe training time in feed-forward neural networks. However, the effect of batch\nnormalization is dependent on the mini-batch size and it is not obvious how to\napply it to recurrent neural networks. In this paper, we transpose batch\nnormalization into layer normalization by computing the mean and variance used\nfor normalization from all of the summed inputs to the neurons in a layer on a\nsingle training case. Like batch normalization, we also give each neuron its\nown adaptive bias and gain which are applied after the normalization but before\nthe non-linearity. Unlike batch normalization, layer normalization performs\nexactly the same computation at training and test times. It is also\nstraightforward to apply to recurrent neural networks by computing the\nnormalization statistics separately at each time step. Layer normalization is\nvery effective at stabilizing the hidden state dynamics in recurrent networks.\nEmpirically, we show that layer normalization can substantially reduce the\ntraining time compared with previously published techniques.", "authors": ["Jimmy Lei Ba", "Jamie Ryan Kiros", "Geoffrey E. Hinton"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1607.06450v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1607.06450v1", "num_discussion": 0, "originally_published_time": "7/21/2016", "pid": "1607.06450v1", "published_time": "7/21/2016", "rawpid": "1607.06450", "tags": ["stat.ML", "cs.LG"], "title": "Layer Normalization"}, {"abstract": "We present a framework for efficient inference in structured image models\nthat explicitly reason about objects. We achieve this by performing\nprobabilistic inference using a recurrent neural network that attends to scene\nelements and processes them one at a time. Crucially, the model itself learns\nto choose the appropriate number of inference steps. We use this scheme to\nlearn to perform inference in partially specified 2D models (variable-sized\nvariational auto-encoders) and fully specified 3D models (probabilistic\nrenderers). We show that such models learn to identify multiple objects -\ncounting, locating and classifying the elements of a scene - without any\nsupervision, e.g., decomposing 3D images with various numbers of objects in a\nsingle forward pass of a neural network. We further show that the networks\nproduce accurate inferences when compared to supervised counterparts, and that\ntheir structure leads to improved generalization.", "authors": ["S. M. Ali Eslami", "Nicolas Heess", "Theophane Weber", "Yuval Tassa", "David Szepesvari", "Koray Kavukcuoglu", "Geoffrey E. Hinton"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1603.08575v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1603.08575v3", "num_discussion": 0, "originally_published_time": "3/28/2016", "pid": "1603.08575v3", "published_time": "8/12/2016", "rawpid": "1603.08575", "tags": ["cs.CV", "cs.LG"], "title": "Attend, Infer, Repeat: Fast Scene Understanding with Generative Models"}, {"abstract": "Very deep convolutional networks with hundreds of layers have led to\nsignificant reductions in error on competitive benchmarks. Although the\nunmatched expressiveness of the many layers can be highly desirable at test\ntime, training very deep networks comes with its own set of challenges. The\ngradients can vanish, the forward flow often diminishes, and the training time\ncan be painfully slow. To address these problems, we propose stochastic depth,\na training procedure that enables the seemingly contradictory setup to train\nshort networks and use deep networks at test time. We start with very deep\nnetworks but during training, for each mini-batch, randomly drop a subset of\nlayers and bypass them with the identity function. This simple approach\ncomplements the recent success of residual networks. It reduces training time\nsubstantially and improves the test error significantly on almost all data sets\nthat we used for evaluation. With stochastic depth we can increase the depth of\nresidual networks even beyond 1200 layers and still yield meaningful\nimprovements in test error (4.91% on CIFAR-10).", "authors": ["Gao Huang", "Yu Sun", "Zhuang Liu", "Daniel Sedra", "Kilian Weinberger"], "category": "cs.LG", "comment": "first two authors contributed equally", "img": "/static/thumbs/1603.09382v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1603.09382v3", "num_discussion": 0, "originally_published_time": "3/30/2016", "pid": "1603.09382v3", "published_time": "7/28/2016", "rawpid": "1603.09382", "tags": ["cs.LG", "cs.CV", "cs.NE"], "title": "Deep Networks with Stochastic Depth"}, {"abstract": "We present weight normalization: a reparameterization of the weight vectors\nin a neural network that decouples the length of those weight vectors from\ntheir direction. By reparameterizing the weights in this way we improve the\nconditioning of the optimization problem and we speed up convergence of\nstochastic gradient descent. Our reparameterization is inspired by batch\nnormalization but does not introduce any dependencies between the examples in a\nminibatch. This means that our method can also be applied successfully to\nrecurrent models such as LSTMs and to noise-sensitive applications such as deep\nreinforcement learning or generative models, for which batch normalization is\nless well suited. Although our method is much simpler, it still provides much\nof the speed-up of full batch normalization. In addition, the computational\noverhead of our method is lower, permitting more optimization steps to be taken\nin the same amount of time. We demonstrate the usefulness of our method on\napplications in supervised image recognition, generative modelling, and deep\nreinforcement learning.", "authors": ["Tim Salimans", "Diederik P. Kingma"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1602.07868v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1602.07868v3", "num_discussion": 0, "originally_published_time": "2/25/2016", "pid": "1602.07868v3", "published_time": "6/4/2016", "rawpid": "1602.07868", "tags": ["cs.LG", "cs.AI", "cs.NE"], "title": "Weight Normalization: A Simple Reparameterization to Accelerate Training\n  of Deep Neural Networks"}, {"abstract": "Recent research on deep neural networks has focused primarily on improving\naccuracy. For a given accuracy level, it is typically possible to identify\nmultiple DNN architectures that achieve that accuracy level. With equivalent\naccuracy, smaller DNN architectures offer at least three advantages: (1)\nSmaller DNNs require less communication across servers during distributed\ntraining. (2) Smaller DNNs require less bandwidth to export a new model from\nthe cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on\nFPGAs and other hardware with limited memory. To provide all of these\nadvantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet\nachieves AlexNet-level accuracy on ImageNet with 50x fewer parameters.\nAdditionally, with model compression techniques we are able to compress\nSqueezeNet to less than 0.5MB (510x smaller than AlexNet).\n  The SqueezeNet architecture is available for download here:\nhttps://github.com/DeepScale/SqueezeNet", "authors": ["Forrest N. Iandola", "Song Han", "Matthew W. Moskewicz", "Khalid Ashraf", "William J. Dally", "Kurt Keutzer"], "category": "cs.CV", "comment": "In ICLR Format", "img": "/static/thumbs/1602.07360v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1602.07360v4", "num_discussion": 0, "originally_published_time": "2/24/2016", "pid": "1602.07360v4", "published_time": "11/4/2016", "rawpid": "1602.07360", "tags": ["cs.CV", "cs.AI"], "title": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and \u003c0.5MB\n  model size"}, {"abstract": "We give an overview of recent exciting achievements of deep reinforcement\nlearning (RL). We discuss six core elements, six important mechanisms, and\ntwelve applications. We start with background of machine learning, deep\nlearning and reinforcement learning. Next we discuss core RL elements,\nincluding value function, in particular, Deep Q-Network (DQN), policy, reward,\nmodel, planning, and exploration. After that, we discuss important mechanisms\nfor RL, including attention and memory, in particular, differentiable neural\ncomputer (DNC), unsupervised learning, transfer learning, semi-supervised\nlearning, hierarchical RL, and learning to learn. Then we discuss various\napplications of RL, including games, in particular, AlphaGo, robotics, natural\nlanguage processing, including dialogue systems (a.k.a. chatbots), machine\ntranslation, and text generation, computer vision, neural architecture design,\nbusiness management, finance, healthcare, Industry 4.0, smart grid, intelligent\ntransportation systems, and computer systems. We mention topics not reviewed\nyet. After listing a collection of RL resources, we present a brief summary,\nand close with discussions.", "authors": ["Yuxi Li"], "category": "cs.LG", "comment": "major updates for both content and organization", "img": "/static/thumbs/1701.07274v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1701.07274v3", "num_discussion": 0, "originally_published_time": "1/25/2017", "pid": "1701.07274v3", "published_time": "7/15/2017", "rawpid": "1701.07274", "tags": ["cs.LG"], "title": "Deep Reinforcement Learning: An Overview"}, {"abstract": "Recent progress in artificial intelligence (AI) has renewed interest in\nbuilding systems that learn and think like people. Many advances have come from\nusing deep neural networks trained end-to-end in tasks such as object\nrecognition, video games, and board games, achieving performance that equals or\neven beats humans in some respects. Despite their biological inspiration and\nperformance achievements, these systems differ from human intelligence in\ncrucial ways. We review progress in cognitive science suggesting that truly\nhuman-like learning and thinking machines will have to reach beyond current\nengineering trends in both what they learn, and how they learn it.\nSpecifically, we argue that these machines should (a) build causal models of\nthe world that support explanation and understanding, rather than merely\nsolving pattern recognition problems; (b) ground learning in intuitive theories\nof physics and psychology, to support and enrich the knowledge that is learned;\nand (c) harness compositionality and learning-to-learn to rapidly acquire and\ngeneralize knowledge to new tasks and situations. We suggest concrete\nchallenges and promising routes towards these goals that can combine the\nstrengths of recent neural network advances with more structured cognitive\nmodels.", "authors": ["Brenden M. Lake", "Tomer D. Ullman", "Joshua B. Tenenbaum", "Samuel J. Gershman"], "category": "cs.AI", "comment": "In press at Behavioral and Brain Sciences. Open call for commentary\n  proposals (until Nov. 22, 2016...", "img": "/static/thumbs/1604.00289v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1604.00289v3", "num_discussion": 0, "originally_published_time": "4/1/2016", "pid": "1604.00289v3", "published_time": "11/2/2016", "rawpid": "1604.00289", "tags": ["cs.AI", "cs.CV", "cs.LG", "cs.NE", "stat.ML"], "title": "Building Machines That Learn and Think Like People"}, {"abstract": "Many sequential processing tasks require complex nonlinear transition\nfunctions from one step to the next. However, recurrent neural networks with\n\u0027deep\u0027 transition functions remain difficult to train, even when using Long\nShort-Term Memory (LSTM) networks. We introduce a novel theoretical analysis of\nrecurrent networks based on Gersgorin\u0027s circle theorem that illuminates several\nmodeling and optimization issues and improves our understanding of the LSTM\ncell. Based on this analysis we propose Recurrent Highway Networks, which\nextend the LSTM architecture to allow step-to-step transition depths larger\nthan one. Several language modeling experiments demonstrate that the proposed\narchitecture results in powerful and efficient models. On the Penn Treebank\ncorpus, solely increasing the transition depth from 1 to 10 improves word-level\nperplexity from 90.6 to 65.4 using the same number of parameters. On the larger\nWikipedia datasets for character prediction (text8 and enwik8), RHNs outperform\nall previous results and achieve an entropy of 1.27 bits per character.", "authors": ["Julian Georg Zilly", "Rupesh Kumar Srivastava", "Jan Koutn\u00edk", "J\u00fcrgen Schmidhuber"], "category": "cs.LG", "comment": "12 pages, 6 figures, 3 tables", "img": "/static/thumbs/1607.03474v5.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1607.03474v5", "num_discussion": 0, "originally_published_time": "7/12/2016", "pid": "1607.03474v5", "published_time": "7/4/2017", "rawpid": "1607.03474", "tags": ["cs.LG", "cs.CL", "cs.NE"], "title": "Recurrent Highway Networks"}, {"abstract": "In recent years, supervised learning with convolutional networks (CNNs) has\nseen huge adoption in computer vision applications. Comparatively, unsupervised\nlearning with CNNs has received less attention. In this work we hope to help\nbridge the gap between the success of CNNs for supervised learning and\nunsupervised learning. We introduce a class of CNNs called deep convolutional\ngenerative adversarial networks (DCGANs), that have certain architectural\nconstraints, and demonstrate that they are a strong candidate for unsupervised\nlearning. Training on various image datasets, we show convincing evidence that\nour deep convolutional adversarial pair learns a hierarchy of representations\nfrom object parts to scenes in both the generator and discriminator.\nAdditionally, we use the learned features for novel tasks - demonstrating their\napplicability as general image representations.", "authors": ["Alec Radford", "Luke Metz", "Soumith Chintala"], "category": "cs.LG", "comment": "Under review as a conference paper at ICLR 2016", "img": "/static/thumbs/1511.06434v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1511.06434v2", "num_discussion": 0, "originally_published_time": "11/19/2015", "pid": "1511.06434v2", "published_time": "1/7/2016", "rawpid": "1511.06434", "tags": ["cs.LG", "cs.CV"], "title": "Unsupervised Representation Learning with Deep Convolutional Generative\n  Adversarial Networks"}, {"abstract": "We propose a new framework for estimating generative models via an\nadversarial process, in which we simultaneously train two models: a generative\nmodel G that captures the data distribution, and a discriminative model D that\nestimates the probability that a sample came from the training data rather than\nG. The training procedure for G is to maximize the probability of D making a\nmistake. This framework corresponds to a minimax two-player game. In the space\nof arbitrary functions G and D, a unique solution exists, with G recovering the\ntraining data distribution and D equal to 1/2 everywhere. In the case where G\nand D are defined by multilayer perceptrons, the entire system can be trained\nwith backpropagation. There is no need for any Markov chains or unrolled\napproximate inference networks during either training or generation of samples.\nExperiments demonstrate the potential of the framework through qualitative and\nquantitative evaluation of the generated samples.", "authors": ["Ian J. Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1406.2661v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1406.2661v1", "num_discussion": 0, "originally_published_time": "6/10/2014", "pid": "1406.2661v1", "published_time": "6/10/2014", "rawpid": "1406.2661", "tags": ["stat.ML", "cs.LG"], "title": "Generative Adversarial Networks"}, {"abstract": "We introduce a new algorithm named WGAN, an alternative to traditional GAN\ntraining. In this new model, we show that we can improve the stability of\nlearning, get rid of problems like mode collapse, and provide meaningful\nlearning curves useful for debugging and hyperparameter searches. Furthermore,\nwe show that the corresponding optimization problem is sound, and provide\nextensive theoretical work highlighting the deep connections to other distances\nbetween distributions.", "authors": ["Martin Arjovsky", "Soumith Chintala", "L\u00e9on Bottou"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1701.07875v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1701.07875v2", "num_discussion": 0, "originally_published_time": "1/26/2017", "pid": "1701.07875v2", "published_time": "3/9/2017", "rawpid": "1701.07875", "tags": ["stat.ML", "cs.LG"], "title": "Wasserstein GAN"}, {"abstract": "State-of-the-art object detection networks depend on region proposal\nalgorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN\nhave reduced the running time of these detection networks, exposing region\nproposal computation as a bottleneck. In this work, we introduce a Region\nProposal Network (RPN) that shares full-image convolutional features with the\ndetection network, thus enabling nearly cost-free region proposals. An RPN is a\nfully convolutional network that simultaneously predicts object bounds and\nobjectness scores at each position. The RPN is trained end-to-end to generate\nhigh-quality region proposals, which are used by Fast R-CNN for detection. We\nfurther merge RPN and Fast R-CNN into a single network by sharing their\nconvolutional features---using the recently popular terminology of neural\nnetworks with \u0027attention\u0027 mechanisms, the RPN component tells the unified\nnetwork where to look. For the very deep VGG-16 model, our detection system has\na frame rate of 5fps (including all steps) on a GPU, while achieving\nstate-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS\nCOCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015\ncompetitions, Faster R-CNN and RPN are the foundations of the 1st-place winning\nentries in several tracks. Code has been made publicly available.", "authors": ["Shaoqing Ren", "Kaiming He", "Ross Girshick", "Jian Sun"], "category": "cs.CV", "comment": "Extended tech report", "img": "/static/thumbs/1506.01497v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1506.01497v3", "num_discussion": 0, "originally_published_time": "6/4/2015", "pid": "1506.01497v3", "published_time": "1/6/2016", "rawpid": "1506.01497", "tags": ["cs.CV"], "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal\n  Networks"}, {"abstract": "We present YOLO, a new approach to object detection. Prior work on object\ndetection repurposes classifiers to perform detection. Instead, we frame object\ndetection as a regression problem to spatially separated bounding boxes and\nassociated class probabilities. A single neural network predicts bounding boxes\nand class probabilities directly from full images in one evaluation. Since the\nwhole detection pipeline is a single network, it can be optimized end-to-end\ndirectly on detection performance.\n  Our unified architecture is extremely fast. Our base YOLO model processes\nimages in real-time at 45 frames per second. A smaller version of the network,\nFast YOLO, processes an astounding 155 frames per second while still achieving\ndouble the mAP of other real-time detectors. Compared to state-of-the-art\ndetection systems, YOLO makes more localization errors but is far less likely\nto predict false detections where nothing exists. Finally, YOLO learns very\ngeneral representations of objects. It outperforms all other detection methods,\nincluding DPM and R-CNN, by a wide margin when generalizing from natural images\nto artwork on both the Picasso Dataset and the People-Art Dataset.", "authors": ["Joseph Redmon", "Santosh Divvala", "Ross Girshick", "Ali Farhadi"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1506.02640v5.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1506.02640v5", "num_discussion": 1, "originally_published_time": "6/8/2015", "pid": "1506.02640v5", "published_time": "5/9/2016", "rawpid": "1506.02640", "tags": ["cs.CV"], "title": "You Only Look Once: Unified, Real-Time Object Detection"}, {"abstract": "We propose a reparameterization of LSTM that brings the benefits of batch\nnormalization to recurrent neural networks. Whereas previous works only apply\nbatch normalization to the input-to-hidden transformation of RNNs, we\ndemonstrate that it is both possible and beneficial to batch-normalize the\nhidden-to-hidden transition, thereby reducing internal covariate shift between\ntime steps. We evaluate our proposal on various sequential problems such as\nsequence classification, language modeling and question answering. Our\nempirical results show that our batch-normalized LSTM consistently leads to\nfaster convergence and improved generalization.", "authors": ["Tim Cooijmans", "Nicolas Ballas", "C\u00e9sar Laurent", "\u00c7a\u011flar G\u00fcl\u00e7ehre", "Aaron Courville"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1603.09025v5.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1603.09025v5", "num_discussion": 0, "originally_published_time": "3/30/2016", "pid": "1603.09025v5", "published_time": "2/28/2017", "rawpid": "1603.09025", "tags": ["cs.LG"], "title": "Recurrent Batch Normalization"}, {"abstract": "Training Deep Neural Networks is complicated by the fact that the\ndistribution of each layer\u0027s inputs changes during training, as the parameters\nof the previous layers change. This slows down the training by requiring lower\nlearning rates and careful parameter initialization, and makes it notoriously\nhard to train models with saturating nonlinearities. We refer to this\nphenomenon as internal covariate shift, and address the problem by normalizing\nlayer inputs. Our method draws its strength from making normalization a part of\nthe model architecture and performing the normalization for each training\nmini-batch. Batch Normalization allows us to use much higher learning rates and\nbe less careful about initialization. It also acts as a regularizer, in some\ncases eliminating the need for Dropout. Applied to a state-of-the-art image\nclassification model, Batch Normalization achieves the same accuracy with 14\ntimes fewer training steps, and beats the original model by a significant\nmargin. Using an ensemble of batch-normalized networks, we improve upon the\nbest published result on ImageNet classification: reaching 4.9% top-5\nvalidation error (and 4.8% test error), exceeding the accuracy of human raters.", "authors": ["Sergey Ioffe", "Christian Szegedy"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1502.03167v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1502.03167v3", "num_discussion": 0, "originally_published_time": "2/11/2015", "pid": "1502.03167v3", "published_time": "3/2/2015", "rawpid": "1502.03167", "tags": ["cs.LG"], "title": "Batch Normalization: Accelerating Deep Network Training by Reducing\n  Internal Covariate Shift"}, {"abstract": "We explore the use of Evolution Strategies, a class of black box optimization\nalgorithms, as an alternative to popular RL techniques such as Q-learning and\nPolicy Gradients. Experiments on MuJoCo and Atari show that ES is a viable\nsolution strategy that scales extremely well with the number of CPUs available:\nBy using hundreds to thousands of parallel workers, ES can solve 3D humanoid\nwalking in 10 minutes and obtain competitive results on most Atari games after\none hour of training time. In addition, we highlight several advantages of ES\nas a black box optimization technique: it is invariant to action frequency and\ndelayed rewards, tolerant of extremely long horizons, and does not need\ntemporal discounting or value function approximation.", "authors": ["Tim Salimans", "Jonathan Ho", "Xi Chen", "Ilya Sutskever"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1703.03864v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.03864v1", "num_discussion": 0, "originally_published_time": "3/10/2017", "pid": "1703.03864v1", "published_time": "3/10/2017", "rawpid": "1703.03864", "tags": ["stat.ML", "cs.AI", "cs.LG", "cs.NE"], "title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning"}, {"abstract": "We present a method for detecting objects in images using a single deep\nneural network. Our approach, named SSD, discretizes the output space of\nbounding boxes into a set of default boxes over different aspect ratios and\nscales per feature map location. At prediction time, the network generates\nscores for the presence of each object category in each default box and\nproduces adjustments to the box to better match the object shape. Additionally,\nthe network combines predictions from multiple feature maps with different\nresolutions to naturally handle objects of various sizes. Our SSD model is\nsimple relative to methods that require object proposals because it completely\neliminates proposal generation and subsequent pixel or feature resampling stage\nand encapsulates all computation in a single network. This makes SSD easy to\ntrain and straightforward to integrate into systems that require a detection\ncomponent. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets\nconfirm that SSD has comparable accuracy to methods that utilize an additional\nobject proposal step and is much faster, while providing a unified framework\nfor both training and inference. Compared to other single stage methods, SSD\nhas much better accuracy, even with a smaller input image size. For $300\\times\n300$ input, SSD achieves 72.1% mAP on VOC2007 test at 58 FPS on a Nvidia Titan\nX and for $500\\times 500$ input, SSD achieves 75.1% mAP, outperforming a\ncomparable state of the art Faster R-CNN model. Code is available at\nhttps://github.com/weiliu89/caffe/tree/ssd .", "authors": ["Wei Liu", "Dragomir Anguelov", "Dumitru Erhan", "Christian Szegedy", "Scott Reed", "Cheng-Yang Fu", "Alexander C. Berg"], "category": "cs.CV", "comment": "ECCV 2016", "img": "/static/thumbs/1512.02325v5.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1512.02325v5", "num_discussion": 0, "originally_published_time": "12/8/2015", "pid": "1512.02325v5", "published_time": "12/29/2016", "rawpid": "1512.02325", "tags": ["cs.CV"], "title": "SSD: Single Shot MultiBox Detector"}, {"abstract": "We extend the capabilities of neural networks by coupling them to external\nmemory resources, which they can interact with by attentional processes. The\ncombined system is analogous to a Turing Machine or Von Neumann architecture\nbut is differentiable end-to-end, allowing it to be efficiently trained with\ngradient descent. Preliminary results demonstrate that Neural Turing Machines\ncan infer simple algorithms such as copying, sorting, and associative recall\nfrom input and output examples.", "authors": ["Alex Graves", "Greg Wayne", "Ivo Danihelka"], "category": "cs.NE", "comment": "", "img": "/static/thumbs/1410.5401v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1410.5401v2", "num_discussion": 0, "originally_published_time": "10/20/2014", "pid": "1410.5401v2", "published_time": "12/10/2014", "rawpid": "1410.5401", "tags": ["cs.NE"], "title": "Neural Turing Machines"}, {"abstract": "In just three years, Variational Autoencoders (VAEs) have emerged as one of\nthe most popular approaches to unsupervised learning of complicated\ndistributions. VAEs are appealing because they are built on top of standard\nfunction approximators (neural networks), and can be trained with stochastic\ngradient descent. VAEs have already shown promise in generating many kinds of\ncomplicated data, including handwritten digits, faces, house numbers, CIFAR\nimages, physical models of scenes, segmentation, and predicting the future from\nstatic images. This tutorial introduces the intuitions behind VAEs, explains\nthe mathematics behind them, and describes some empirical behavior. No prior\nknowledge of variational Bayesian methods is assumed.", "authors": ["Carl Doersch"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1606.05908v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1606.05908v2", "num_discussion": 0, "originally_published_time": "6/19/2016", "pid": "1606.05908v2", "published_time": "8/13/2016", "rawpid": "1606.05908", "tags": ["stat.ML", "cs.LG"], "title": "Tutorial on Variational Autoencoders"}, {"abstract": "Deep learning yields great results across many fields, from speech\nrecognition, image classification, to translation. But for each problem,\ngetting a deep model to work well involves research into the architecture and a\nlong period of tuning. We present a single model that yields good results on a\nnumber of problems spanning multiple domains. In particular, this single model\nis trained concurrently on ImageNet, multiple translation tasks, image\ncaptioning (COCO dataset), a speech recognition corpus, and an English parsing\ntask. Our model architecture incorporates building blocks from multiple\ndomains. It contains convolutional layers, an attention mechanism, and\nsparsely-gated layers. Each of these computational blocks is crucial for a\nsubset of the tasks we train on. Interestingly, even if a block is not crucial\nfor a task, we observe that adding it never hurts performance and in most cases\nimproves it on all tasks. We also show that tasks with less data benefit\nlargely from joint training with other tasks, while performance on large tasks\ndegrades only slightly if at all.", "authors": ["Lukasz Kaiser", "Aidan N. Gomez", "Noam Shazeer", "Ashish Vaswani", "Niki Parmar", "Llion Jones", "Jakob Uszkoreit"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1706.05137v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1706.05137v1", "num_discussion": 0, "originally_published_time": "6/16/2017", "pid": "1706.05137v1", "published_time": "6/16/2017", "rawpid": "1706.05137", "tags": ["cs.LG", "stat.ML"], "title": "One Model To Learn Them All"}, {"abstract": "Generative Adversarial Networks (GANs) are powerful generative models, but\nsuffer from training instability. The recently proposed Wasserstein GAN (WGAN)\nmakes progress toward stable training of GANs, but can still generate\nlow-quality samples or fail to converge in some settings. We find that these\nproblems are often due to the use of weight clipping in WGAN to enforce a\nLipschitz constraint on the critic, which can lead to pathological behavior. We\npropose an alternative to clipping weights: penalize the norm of gradient of\nthe critic with respect to its input. Our proposed method performs better than\nstandard WGAN and enables stable training of a wide variety of GAN\narchitectures with almost no hyperparameter tuning, including 101-layer ResNets\nand language models over discrete data. We also achieve high quality\ngenerations on CIFAR-10 and LSUN bedrooms.", "authors": ["Ishaan Gulrajani", "Faruk Ahmed", "Martin Arjovsky", "Vincent Dumoulin", "Aaron Courville"], "category": "cs.LG", "comment": "New CIFAR-10 and LSUN image generation experiments", "img": "/static/thumbs/1704.00028v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00028v2", "num_discussion": 0, "originally_published_time": "3/31/2017", "pid": "1704.00028v2", "published_time": "5/29/2017", "rawpid": "1704.00028", "tags": ["cs.LG", "stat.ML"], "title": "Improved Training of Wasserstein GANs"}, {"abstract": "Despite the breakthroughs in accuracy and speed of single image\nsuper-resolution using faster and deeper convolutional neural networks, one\ncentral problem remains largely unsolved: how do we recover the finer texture\ndetails when we super-resolve at large upscaling factors? The behavior of\noptimization-based super-resolution methods is principally driven by the choice\nof the objective function. Recent work has largely focused on minimizing the\nmean squared reconstruction error. The resulting estimates have high peak\nsignal-to-noise ratios, but they are often lacking high-frequency details and\nare perceptually unsatisfying in the sense that they fail to match the fidelity\nexpected at the higher resolution. In this paper, we present SRGAN, a\ngenerative adversarial network (GAN) for image super-resolution (SR). To our\nknowledge, it is the first framework capable of inferring photo-realistic\nnatural images for 4x upscaling factors. To achieve this, we propose a\nperceptual loss function which consists of an adversarial loss and a content\nloss. The adversarial loss pushes our solution to the natural image manifold\nusing a discriminator network that is trained to differentiate between the\nsuper-resolved images and original photo-realistic images. In addition, we use\na content loss motivated by perceptual similarity instead of similarity in\npixel space. Our deep residual network is able to recover photo-realistic\ntextures from heavily downsampled images on public benchmarks. An extensive\nmean-opinion-score (MOS) test shows hugely significant gains in perceptual\nquality using SRGAN. The MOS scores obtained with SRGAN are closer to those of\nthe original high-resolution images than to those obtained with any\nstate-of-the-art method.", "authors": ["Christian Ledig", "Lucas Theis", "Ferenc Huszar", "Jose Caballero", "Andrew Cunningham", "Alejandro Acosta", "Andrew Aitken", "Alykhan Tejani", "Johannes Totz", "Zehan Wang", "Wenzhe Shi"], "category": "cs.CV", "comment": "19 pages, 15 figures, 2 tables, accepted for oral presentation at\n  CVPR, main paper + some suppleme...", "img": "/static/thumbs/1609.04802v5.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1609.04802v5", "num_discussion": 0, "originally_published_time": "9/15/2016", "pid": "1609.04802v5", "published_time": "5/25/2017", "rawpid": "1609.04802", "tags": ["cs.CV", "stat.ML"], "title": "Photo-Realistic Single Image Super-Resolution Using a Generative\n  Adversarial Network"}, {"abstract": "Despite their massive size, successful deep artificial neural networks can\nexhibit a remarkably small difference between training and test performance.\nConventional wisdom attributes small generalization error either to properties\nof the model family, or to the regularization techniques used during training.\n  Through extensive systematic experiments, we show how these traditional\napproaches fail to explain why large neural networks generalize well in\npractice. Specifically, our experiments establish that state-of-the-art\nconvolutional networks for image classification trained with stochastic\ngradient methods easily fit a random labeling of the training data. This\nphenomenon is qualitatively unaffected by explicit regularization, and occurs\neven if we replace the true images by completely unstructured random noise. We\ncorroborate these experimental findings with a theoretical construction showing\nthat simple depth two neural networks already have perfect finite sample\nexpressivity as soon as the number of parameters exceeds the number of data\npoints as it usually does in practice.\n  We interpret our experimental findings by comparison with traditional models.", "authors": ["Chiyuan Zhang", "Samy Bengio", "Moritz Hardt", "Benjamin Recht", "Oriol Vinyals"], "category": "cs.LG", "comment": "Published in ICLR 2017", "img": "/static/thumbs/1611.03530v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1611.03530v2", "num_discussion": 0, "originally_published_time": "11/10/2016", "pid": "1611.03530v2", "published_time": "2/26/2017", "rawpid": "1611.03530", "tags": ["cs.LG"], "title": "Understanding deep learning requires rethinking generalization"}, {"abstract": "Relational reasoning is a central component of generally intelligent\nbehavior, but has proven difficult for neural networks to learn. In this paper\nwe describe how to use Relation Networks (RNs) as a simple plug-and-play module\nto solve problems that fundamentally hinge on relational reasoning. We tested\nRN-augmented networks on three tasks: visual question answering using a\nchallenging dataset called CLEVR, on which we achieve state-of-the-art,\nsuper-human performance; text-based question answering using the bAbI suite of\ntasks; and complex reasoning about dynamic physical systems. Then, using a\ncurated dataset called Sort-of-CLEVR we show that powerful convolutional\nnetworks do not have a general capacity to solve relational questions, but can\ngain this capacity when augmented with RNs. Our work shows how a deep learning\narchitecture equipped with an RN module can implicitly discover and learn to\nreason about entities and their relations.", "authors": ["Adam Santoro", "David Raposo", "David G. T. Barrett", "Mateusz Malinowski", "Razvan Pascanu", "Peter Battaglia", "Timothy Lillicrap"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1706.01427v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1706.01427v1", "num_discussion": 0, "originally_published_time": "6/5/2017", "pid": "1706.01427v1", "published_time": "6/5/2017", "rawpid": "1706.01427", "tags": ["cs.CL", "cs.LG"], "title": "A simple neural network module for relational reasoning"}, {"abstract": "Inspired by recent work in machine translation and object detection, we\nintroduce an attention based model that automatically learns to describe the\ncontent of images. We describe how we can train this model in a deterministic\nmanner using standard backpropagation techniques and stochastically by\nmaximizing a variational lower bound. We also show through visualization how\nthe model is able to automatically learn to fix its gaze on salient objects\nwhile generating the corresponding words in the output sequence. We validate\nthe use of attention with state-of-the-art performance on three benchmark\ndatasets: Flickr8k, Flickr30k and MS COCO.", "authors": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1502.03044v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1502.03044v3", "num_discussion": 2, "originally_published_time": "2/10/2015", "pid": "1502.03044v3", "published_time": "4/19/2016", "rawpid": "1502.03044", "tags": ["cs.LG", "cs.CV"], "title": "Show, Attend and Tell: Neural Image Caption Generation with Visual\n  Attention"}, {"abstract": "We present the first deep learning model to successfully learn control\npolicies directly from high-dimensional sensory input using reinforcement\nlearning. The model is a convolutional neural network, trained with a variant\nof Q-learning, whose input is raw pixels and whose output is a value function\nestimating future rewards. We apply our method to seven Atari 2600 games from\nthe Arcade Learning Environment, with no adjustment of the architecture or\nlearning algorithm. We find that it outperforms all previous approaches on six\nof the games and surpasses a human expert on three of them.", "authors": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin Riedmiller"], "category": "cs.LG", "comment": "NIPS Deep Learning Workshop 2013", "img": "/static/thumbs/1312.5602v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1312.5602v1", "num_discussion": 0, "originally_published_time": "12/19/2013", "pid": "1312.5602v1", "published_time": "12/19/2013", "rawpid": "1312.5602", "tags": ["cs.LG"], "title": "Playing Atari with Deep Reinforcement Learning"}, {"abstract": "We introduce YOLO9000, a state-of-the-art, real-time object detection system\nthat can detect over 9000 object categories. First we propose various\nimprovements to the YOLO detection method, both novel and drawn from prior\nwork. The improved model, YOLOv2, is state-of-the-art on standard detection\ntasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At\n40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like\nFaster RCNN with ResNet and SSD while still running significantly faster.\nFinally we propose a method to jointly train on object detection and\nclassification. Using this method we train YOLO9000 simultaneously on the COCO\ndetection dataset and the ImageNet classification dataset. Our joint training\nallows YOLO9000 to predict detections for object classes that don\u0027t have\nlabelled detection data. We validate our approach on the ImageNet detection\ntask. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite\nonly having detection data for 44 of the 200 classes. On the 156 classes not in\nCOCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes;\nit predicts detections for more than 9000 different object categories. And it\nstill runs in real-time.", "authors": ["Joseph Redmon", "Ali Farhadi"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1612.08242v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1612.08242v1", "num_discussion": 1, "originally_published_time": "12/25/2016", "pid": "1612.08242v1", "published_time": "12/25/2016", "rawpid": "1612.08242", "tags": ["cs.CV"], "title": "YOLO9000: Better, Faster, Stronger"}, {"abstract": "We explore the method of style transfer presented in the article \"A Neural\nAlgorithm of Artistic Style\" by Leon A. Gatys, Alexander S. Ecker and Matthias\nBethge (arXiv:1508.06576).\n  We first demonstrate the power of the suggested style space on a few\nexamples. We then vary different hyper-parameters and program properties that\nwere not discussed in the original paper, among which are the recognition\nnetwork used, starting point of the gradient descent and different ways to\npartition style and content layers. We also give a brief comparison of some of\nthe existing algorithm implementations and deep learning frameworks used.\n  To study the style space further we attempt to generate synthetic images by\nmaximizing a single entry in one of the Gram matrices $\\mathcal{G}_l$ and some\ninteresting results are observed. Next, we try to mimic the sparsity and\nintensity distribution of Gram matrices obtained from a real painting and\ngenerate more complex textures.\n  Finally, we propose two new style representations built on top of network\u0027s\nfeatures and discuss how one could be used to achieve local and potentially\ncontent-aware style transfer.", "authors": ["Yaroslav Nikulin", "Roman Novak"], "category": "cs.CV", "comment": "A short class project report (14 pages, 14 figures)", "img": "/static/thumbs/1602.07188v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1602.07188v2", "num_discussion": 0, "originally_published_time": "2/23/2016", "pid": "1602.07188v2", "published_time": "3/13/2016", "rawpid": "1602.07188", "tags": ["cs.CV"], "title": "Exploring the Neural Algorithm of Artistic Style"}, {"abstract": "We present a variety of new architectural features and training procedures\nthat we apply to the generative adversarial networks (GANs) framework. We focus\non two applications of GANs: semi-supervised learning, and the generation of\nimages that humans find visually realistic. Unlike most work on generative\nmodels, our primary goal is not to train a model that assigns high likelihood\nto test data, nor do we require the model to be able to learn well without\nusing any labels. Using our new techniques, we achieve state-of-the-art results\nin semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated\nimages are of high quality as confirmed by a visual Turing test: our model\ngenerates MNIST samples that humans cannot distinguish from real data, and\nCIFAR-10 samples that yield a human error rate of 21.3%. We also present\nImageNet samples with unprecedented resolution and show that our methods enable\nthe model to learn recognizable features of ImageNet classes.", "authors": ["Tim Salimans", "Ian Goodfellow", "Wojciech Zaremba", "Vicki Cheung", "Alec Radford", "Xi Chen"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1606.03498v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1606.03498v1", "num_discussion": 0, "originally_published_time": "6/10/2016", "pid": "1606.03498v1", "published_time": "6/10/2016", "rawpid": "1606.03498", "tags": ["cs.LG", "cs.CV", "cs.NE"], "title": "Improved Techniques for Training GANs"}, {"abstract": "Very deep convolutional neural networks introduced new problems like\nvanishing gradient and degradation. The recent successful contributions towards\nsolving these problems are Residual and Highway Networks. These networks\nintroduce skip connections that allow the information (from the input or those\nlearned in earlier layers) to flow more into the deeper layers. These very deep\nmodels have lead to a considerable decrease in test errors, on benchmarks like\nImageNet and COCO. In this paper, we propose the use of exponential linear unit\ninstead of the combination of ReLU and Batch Normalization in Residual\nNetworks. We show that this not only speeds up learning in Residual Networks\nbut also improves the accuracy as the depth increases. It improves the test\nerror on almost all data sets, like CIFAR-10 and CIFAR-100", "authors": ["Anish Shah", "Eashan Kadam", "Hena Shah", "Sameer Shinde", "Sandip Shingade"], "category": "cs.CV", "comment": "submitted in Vision Net 2016, Jaipur, India", "img": "/static/thumbs/1604.04112v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1604.04112v4", "num_discussion": 0, "originally_published_time": "4/14/2016", "pid": "1604.04112v4", "published_time": "10/5/2016", "rawpid": "1604.04112", "tags": ["cs.CV"], "title": "Deep Residual Networks with Exponential Linear Unit"}, {"abstract": "Until recently, research on artificial neural networks was largely restricted\nto systems with only two types of variable: Neural activities that represent\nthe current or recent input and weights that learn to capture regularities\namong inputs, outputs and payoffs. There is no good reason for this\nrestriction. Synapses have dynamics at many different time-scales and this\nsuggests that artificial neural networks might benefit from variables that\nchange slower than activities but much faster than the standard weights. These\n\"fast weights\" can be used to store temporary memories of the recent past and\nthey provide a neurally plausible way of implementing the type of attention to\nthe past that has recently proved very helpful in sequence-to-sequence models.\nBy using fast weights we can avoid the need to store copies of neural activity\npatterns.", "authors": ["Jimmy Ba", "Geoffrey Hinton", "Volodymyr Mnih", "Joel Z. Leibo", "Catalin Ionescu"], "category": "stat.ML", "comment": "Added [Schmidhuber 1993] citation to the last paragraph of the\n  introduction. Fixed typo appendix A...", "img": "/static/thumbs/1610.06258v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1610.06258v3", "num_discussion": 0, "originally_published_time": "10/20/2016", "pid": "1610.06258v3", "published_time": "12/5/2016", "rawpid": "1610.06258", "tags": ["stat.ML", "cs.LG", "cs.NE"], "title": "Using Fast Weights to Attend to the Recent Past"}, {"abstract": "This paper introduces Adaptive Computation Time (ACT), an algorithm that\nallows recurrent neural networks to learn how many computational steps to take\nbetween receiving an input and emitting an output. ACT requires minimal changes\nto the network architecture, is deterministic and differentiable, and does not\nadd any noise to the parameter gradients. Experimental results are provided for\nfour synthetic problems: determining the parity of binary vectors, applying\nbinary logic operations, adding integers, and sorting real numbers. Overall,\nperformance is dramatically improved by the use of ACT, which successfully\nadapts the number of computational steps to the requirements of the problem. We\nalso present character-level language modelling results on the Hutter prize\nWikipedia dataset. In this case ACT does not yield large gains in performance;\nhowever it does provide intriguing insight into the structure of the data, with\nmore computation allocated to harder-to-predict transitions, such as spaces\nbetween words and ends of sentences. This suggests that ACT or other adaptive\ncomputation methods could provide a generic method for inferring segment\nboundaries in sequence data.", "authors": ["Alex Graves"], "category": "cs.NE", "comment": "", "img": "/static/thumbs/1603.08983v6.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1603.08983v6", "num_discussion": 0, "originally_published_time": "3/29/2016", "pid": "1603.08983v6", "published_time": "2/21/2017", "rawpid": "1603.08983", "tags": ["cs.NE"], "title": "Adaptive Computation Time for Recurrent Neural Networks"}, {"abstract": "Humans have an impressive ability to reason about new concepts and\nexperiences from just a single example. In particular, humans have an ability\nfor one-shot generalization: an ability to encounter a new concept, understand\nits structure, and then be able to generate compelling alternative variations\nof the concept. We develop machine learning systems with this important\ncapacity by developing new deep generative models, models that combine the\nrepresentational power of deep learning with the inferential power of Bayesian\nreasoning. We develop a class of sequential generative models that are built on\nthe principles of feedback and attention. These two characteristics lead to\ngenerative models that are among the state-of-the art in density estimation and\nimage generation. We demonstrate the one-shot generalization ability of our\nmodels using three tasks: unconditional sampling, generating new exemplars of a\ngiven concept, and generating new exemplars of a family of concepts. In all\ncases our models are able to generate compelling and diverse samples---having\nseen new examples just once---providing an important class of general-purpose\nmodels for one-shot machine learning.", "authors": ["Danilo Jimenez Rezende", "Shakir Mohamed", "Ivo Danihelka", "Karol Gregor", "Daan Wierstra"], "category": "stat.ML", "comment": "8pgs, 1pg references, 1pg appendix, In Proceedings of the 33rd\n  International Conference on Machine...", "img": "/static/thumbs/1603.05106v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1603.05106v2", "num_discussion": 0, "originally_published_time": "3/16/2016", "pid": "1603.05106v2", "published_time": "5/25/2016", "rawpid": "1603.05106", "tags": ["stat.ML", "cs.AI", "cs.LG"], "title": "One-Shot Generalization in Deep Generative Models"}, {"abstract": "In this paper, we propose gcForest, a decision tree ensemble approach with\nperformance highly competitive to deep neural networks. In contrast to deep\nneural networks which require great effort in hyper-parameter tuning, gcForest\nis much easier to train. Actually, even when gcForest is applied to different\ndata from different domains, excellent performance can be achieved by almost\nsame settings of hyper-parameters. The training process of gcForest is\nefficient and scalable. In our experiments its training time running on a PC is\ncomparable to that of deep neural networks running with GPU facilities, and the\nefficiency advantage may be more apparent because gcForest is naturally apt to\nparallel implementation. Furthermore, in contrast to deep neural networks which\nrequire large-scale training data, gcForest can work well even when there are\nonly small-scale training data. Moreover, as a tree-based approach, gcForest\nshould be easier for theoretical analysis than deep neural networks.", "authors": ["Zhi-Hua Zhou", "Ji Feng"], "category": "cs.LG", "comment": "IJCAI 2017", "img": "/static/thumbs/1702.08835v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1702.08835v2", "num_discussion": 4, "originally_published_time": "2/28/2017", "pid": "1702.08835v2", "published_time": "5/31/2017", "rawpid": "1702.08835", "tags": ["cs.LG", "stat.ML"], "title": "Deep Forest: Towards An Alternative to Deep Neural Networks"}, {"abstract": "In recent years deep reinforcement learning (RL) systems have attained\nsuperhuman performance in a number of challenging task domains. However, a\nmajor limitation of such applications is their demand for massive amounts of\ntraining data. A critical present objective is thus to develop deep RL methods\nthat can adapt rapidly to new tasks. In the present work we introduce a novel\napproach to this challenge, which we refer to as deep meta-reinforcement\nlearning. Previous work has shown that recurrent networks can support\nmeta-learning in a fully supervised context. We extend this approach to the RL\nsetting. What emerges is a system that is trained using one RL algorithm, but\nwhose recurrent dynamics implement a second, quite separate RL procedure. This\nsecond, learned RL algorithm can differ from the original one in arbitrary\nways. Importantly, because it is learned, it is configured to exploit structure\nin the training domain. We unpack these points in a series of seven\nproof-of-concept experiments, each of which examines a key aspect of deep\nmeta-RL. We consider prospects for extending and scaling up the approach, and\nalso point out some potentially important implications for neuroscience.", "authors": ["Jane X Wang", "Zeb Kurth-Nelson", "Dhruva Tirumala", "Hubert Soyer", "Joel Z Leibo", "Remi Munos", "Charles Blundell", "Dharshan Kumaran", "Matt Botvinick"], "category": "cs.LG", "comment": "17 pages, 7 figures, 1 table", "img": "/static/thumbs/1611.05763v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1611.05763v3", "num_discussion": 0, "originally_published_time": "11/17/2016", "pid": "1611.05763v3", "published_time": "1/23/2017", "rawpid": "1611.05763", "tags": ["cs.LG", "cs.AI", "stat.ML"], "title": "Learning to reinforcement learn"}, {"abstract": "Deep reinforcement learning agents have achieved state-of-the-art results by\ndirectly maximising cumulative reward. However, environments contain a much\nwider variety of possible training signals. In this paper, we introduce an\nagent that also maximises many other pseudo-reward functions simultaneously by\nreinforcement learning. All of these tasks share a common representation that,\nlike unsupervised learning, continues to develop in the absence of extrinsic\nrewards. We also introduce a novel mechanism for focusing this representation\nupon extrinsic rewards, so that learning can rapidly adapt to the most relevant\naspects of the actual task. Our agent significantly outperforms the previous\nstate-of-the-art on Atari, averaging 880\\% expert human performance, and a\nchallenging suite of first-person, three-dimensional \\emph{Labyrinth} tasks\nleading to a mean speedup in learning of 10$\\times$ and averaging 87\\% expert\nhuman performance on Labyrinth.", "authors": ["Max Jaderberg", "Volodymyr Mnih", "Wojciech Marian Czarnecki", "Tom Schaul", "Joel Z Leibo", "David Silver", "Koray Kavukcuoglu"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1611.05397v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1611.05397v1", "num_discussion": 0, "originally_published_time": "11/16/2016", "pid": "1611.05397v1", "published_time": "11/16/2016", "rawpid": "1611.05397", "tags": ["cs.LG", "cs.NE"], "title": "Reinforcement Learning with Unsupervised Auxiliary Tasks"}, {"abstract": "In this paper we address the question of how to render sequence-level\nnetworks better at handling structured input. We propose a machine reading\nsimulator which processes text incrementally from left to right and performs\nshallow reasoning with memory and attention. The reader extends the Long\nShort-Term Memory architecture with a memory network in place of a single\nmemory cell. This enables adaptive memory usage during recurrence with neural\nattention, offering a way to weakly induce relations among tokens. The system\nis initially designed to process a single sequence but we also demonstrate how\nto integrate it with an encoder-decoder architecture. Experiments on language\nmodeling, sentiment analysis, and natural language inference show that our\nmodel matches or outperforms the state of the art.", "authors": ["Jianpeng Cheng", "Li Dong", "Mirella Lapata"], "category": "cs.CL", "comment": "Published as a conference paper at EMNLP 2016", "img": "/static/thumbs/1601.06733v7.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1601.06733v7", "num_discussion": 0, "originally_published_time": "1/25/2016", "pid": "1601.06733v7", "published_time": "9/20/2016", "rawpid": "1601.06733", "tags": ["cs.CL", "cs.NE"], "title": "Long Short-Term Memory-Networks for Machine Reading"}, {"abstract": "Image-to-image translation is a class of vision and graphics problems where\nthe goal is to learn the mapping between an input image and an output image\nusing a training set of aligned image pairs. However, for many tasks, paired\ntraining data will not be available. We present an approach for learning to\ntranslate an image from a source domain $X$ to a target domain $Y$ in the\nabsence of paired examples. Our goal is to learn a mapping $G: X \\rightarrow Y$\nsuch that the distribution of images from $G(X)$ is indistinguishable from the\ndistribution $Y$ using an adversarial loss. Because this mapping is highly\nunder-constrained, we couple it with an inverse mapping $F: Y \\rightarrow X$\nand introduce a cycle consistency loss to push $F(G(X)) \\approx X$ (and vice\nversa). Qualitative results are presented on several tasks where paired\ntraining data does not exist, including collection style transfer, object\ntransfiguration, season transfer, photo enhancement, etc. Quantitative\ncomparisons against several prior methods demonstrate the superiority of our\napproach.", "authors": ["Jun-Yan Zhu", "Taesung Park", "Phillip Isola", "Alexei A. Efros"], "category": "cs.CV", "comment": "Submitted to ICCV 2017", "img": "/static/thumbs/1703.10593v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.10593v1", "num_discussion": 0, "originally_published_time": "3/30/2017", "pid": "1703.10593v1", "published_time": "3/30/2017", "rawpid": "1703.10593", "tags": ["cs.CV"], "title": "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial\n  Networks"}, {"abstract": "In this paper, we propose the \"adversarial autoencoder\" (AAE), which is a\nprobabilistic autoencoder that uses the recently proposed generative\nadversarial networks (GAN) to perform variational inference by matching the\naggregated posterior of the hidden code vector of the autoencoder with an\narbitrary prior distribution. Matching the aggregated posterior to the prior\nensures that generating from any part of prior space results in meaningful\nsamples. As a result, the decoder of the adversarial autoencoder learns a deep\ngenerative model that maps the imposed prior to the data distribution. We show\nhow the adversarial autoencoder can be used in applications such as\nsemi-supervised classification, disentangling style and content of images,\nunsupervised clustering, dimensionality reduction and data visualization. We\nperformed experiments on MNIST, Street View House Numbers and Toronto Face\ndatasets and show that adversarial autoencoders achieve competitive results in\ngenerative modeling and semi-supervised classification tasks.", "authors": ["Alireza Makhzani", "Jonathon Shlens", "Navdeep Jaitly", "Ian Goodfellow", "Brendan Frey"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1511.05644v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1511.05644v2", "num_discussion": 0, "originally_published_time": "11/18/2015", "pid": "1511.05644v2", "published_time": "5/25/2016", "rawpid": "1511.05644", "tags": ["cs.LG"], "title": "Adversarial Autoencoders"}, {"abstract": "We present region-based, fully convolutional networks for accurate and\nefficient object detection. In contrast to previous region-based detectors such\nas Fast/Faster R-CNN that apply a costly per-region subnetwork hundreds of\ntimes, our region-based detector is fully convolutional with almost all\ncomputation shared on the entire image. To achieve this goal, we propose\nposition-sensitive score maps to address a dilemma between\ntranslation-invariance in image classification and translation-variance in\nobject detection. Our method can thus naturally adopt fully convolutional image\nclassifier backbones, such as the latest Residual Networks (ResNets), for\nobject detection. We show competitive results on the PASCAL VOC datasets (e.g.,\n83.6% mAP on the 2007 set) with the 101-layer ResNet. Meanwhile, our result is\nachieved at a test-time speed of 170ms per image, 2.5-20x faster than the\nFaster R-CNN counterpart. Code is made publicly available at:\nhttps://github.com/daijifeng001/r-fcn", "authors": ["Jifeng Dai", "Yi Li", "Kaiming He", "Jian Sun"], "category": "cs.CV", "comment": "Tech report", "img": "/static/thumbs/1605.06409v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1605.06409v2", "num_discussion": 0, "originally_published_time": "5/20/2016", "pid": "1605.06409v2", "published_time": "6/21/2016", "rawpid": "1605.06409", "tags": ["cs.CV"], "title": "R-FCN: Object Detection via Region-based Fully Convolutional Networks"}, {"abstract": "In recent years, Deep Learning has become the go-to solution for a broad\nrange of applications, often outperforming state-of-the-art. However, it is\nimportant, for both theoreticians and practitioners, to gain a deeper\nunderstanding of the difficulties and limitations associated with common\napproaches and algorithms. We describe four types of simple problems, for which\nthe gradient-based algorithms commonly used in deep learning either fail or\nsuffer from significant difficulties. We illustrate the failures through\npractical experiments, and provide theoretical insights explaining their\nsource, and how they might be remedied.", "authors": ["Shai Shalev-Shwartz", "Ohad Shamir", "Shaked Shammah"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1703.07950v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.07950v2", "num_discussion": 0, "originally_published_time": "3/23/2017", "pid": "1703.07950v2", "published_time": "4/26/2017", "rawpid": "1703.07950", "tags": ["cs.LG"], "title": "Failures of Gradient-Based Deep Learning"}, {"abstract": "Training directed neural networks typically requires forward-propagating data\nthrough a computation graph, followed by backpropagating error signal, to\nproduce weight updates. All layers, or more generally, modules, of the network\nare therefore locked, in the sense that they must wait for the remainder of the\nnetwork to execute forwards and propagate error backwards before they can be\nupdated. In this work we break this constraint by decoupling modules by\nintroducing a model of the future computation of the network graph. These\nmodels predict what the result of the modelled subgraph will produce using only\nlocal information. In particular we focus on modelling error gradients: by\nusing the modelled synthetic gradient in place of true backpropagated error\ngradients we decouple subgraphs, and can update them independently and\nasynchronously i.e. we realise decoupled neural interfaces. We show results for\nfeed-forward models, where every layer is trained asynchronously, recurrent\nneural networks (RNNs) where predicting one\u0027s future gradient extends the time\nover which the RNN can effectively model, and also a hierarchical RNN system\nwith ticking at different timescales. Finally, we demonstrate that in addition\nto predicting gradients, the same framework can be used to predict inputs,\nresulting in models which are decoupled in both the forward and backwards pass\n-- amounting to independent networks which co-learn such that they can be\ncomposed into a single functioning corporation.", "authors": ["Max Jaderberg", "Wojciech Marian Czarnecki", "Simon Osindero", "Oriol Vinyals", "Alex Graves", "David Silver", "Koray Kavukcuoglu"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1608.05343v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1608.05343v2", "num_discussion": 0, "originally_published_time": "8/18/2016", "pid": "1608.05343v2", "published_time": "7/3/2017", "rawpid": "1608.05343", "tags": ["cs.LG"], "title": "Decoupled Neural Interfaces using Synthetic Gradients"}, {"abstract": "We introduce a guide to help deep learning practitioners understand and\nmanipulate convolutional neural network architectures. The guide clarifies the\nrelationship between various properties (input shape, kernel shape, zero\npadding, strides and output shape) of convolutional, pooling and transposed\nconvolutional layers, as well as the relationship between convolutional and\ntransposed convolutional layers. Relationships are derived for various cases,\nand are illustrated in order to make them intuitive.", "authors": ["Vincent Dumoulin", "Francesco Visin"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1603.07285v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1603.07285v1", "num_discussion": 0, "originally_published_time": "3/23/2016", "pid": "1603.07285v1", "published_time": "3/23/2016", "rawpid": "1603.07285", "tags": ["stat.ML", "cs.LG"], "title": "A guide to convolution arithmetic for deep learning"}, {"abstract": "This paper introduces a deep-learning approach to photographic style transfer\nthat handles a large variety of image content while faithfully transferring the\nreference style. Our approach builds upon the recent work on painterly transfer\nthat separates style from the content of an image by considering different\nlayers of a neural network. However, as is, this approach is not suitable for\nphotorealistic style transfer. Even when both the input and reference images\nare photographs, the output still exhibits distortions reminiscent of a\npainting. Our contribution is to constrain the transformation from the input to\nthe output to be locally affine in colorspace, and to express this constraint\nas a custom fully differentiable energy term. We show that this approach\nsuccessfully suppresses distortion and yields satisfying photorealistic style\ntransfers in a broad variety of scenarios, including transfer of the time of\nday, weather, season, and artistic edits.", "authors": ["Fujun Luan", "Sylvain Paris", "Eli Shechtman", "Kavita Bala"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1703.07511v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.07511v3", "num_discussion": 0, "originally_published_time": "3/22/2017", "pid": "1703.07511v3", "published_time": "4/11/2017", "rawpid": "1703.07511", "tags": ["cs.CV"], "title": "Deep Photo Style Transfer"}, {"abstract": "Neural Machine Translation (NMT) is an end-to-end learning approach for\nautomated translation, with the potential to overcome many of the weaknesses of\nconventional phrase-based translation systems. Unfortunately, NMT systems are\nknown to be computationally expensive both in training and in translation\ninference. Also, most NMT systems have difficulty with rare words. These issues\nhave hindered NMT\u0027s use in practical deployments and services, where both\naccuracy and speed are essential. In this work, we present GNMT, Google\u0027s\nNeural Machine Translation system, which attempts to address many of these\nissues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder\nlayers using attention and residual connections. To improve parallelism and\ntherefore decrease training time, our attention mechanism connects the bottom\nlayer of the decoder to the top layer of the encoder. To accelerate the final\ntranslation speed, we employ low-precision arithmetic during inference\ncomputations. To improve handling of rare words, we divide words into a limited\nset of common sub-word units (\"wordpieces\") for both input and output. This\nmethod provides a good balance between the flexibility of \"character\"-delimited\nmodels and the efficiency of \"word\"-delimited models, naturally handles\ntranslation of rare words, and ultimately improves the overall accuracy of the\nsystem. Our beam search technique employs a length-normalization procedure and\nuses a coverage penalty, which encourages generation of an output sentence that\nis most likely to cover all the words in the source sentence. On the WMT\u002714\nEnglish-to-French and English-to-German benchmarks, GNMT achieves competitive\nresults to state-of-the-art. Using a human side-by-side evaluation on a set of\nisolated simple sentences, it reduces translation errors by an average of 60%\ncompared to Google\u0027s phrase-based production system.", "authors": ["Yonghui Wu", "Mike Schuster", "Zhifeng Chen", "Quoc V. Le", "Mohammad Norouzi", "Wolfgang Macherey", "Maxim Krikun", "Yuan Cao", "Qin Gao", "Klaus Macherey", "Jeff Klingner", "Apurva Shah", "Melvin Johnson", "Xiaobing Liu", "\u0141ukasz Kaiser", "Stephan Gouws", "Yoshikiyo Kato", "Taku Kudo", "Hideto Kazawa", "Keith Stevens", "George Kurian", "Nishant Patil", "Wei Wang", "Cliff Young", "Jason Smith", "Jason Riesa", "Alex Rudnick", "Oriol Vinyals", "Greg Corrado", "Macduff Hughes", "Jeffrey Dean"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1609.08144v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1609.08144v2", "num_discussion": 0, "originally_published_time": "9/26/2016", "pid": "1609.08144v2", "published_time": "10/8/2016", "rawpid": "1609.08144", "tags": ["cs.CL", "cs.AI", "cs.LG"], "title": "Google\u0027s Neural Machine Translation System: Bridging the Gap between\n  Human and Machine Translation"}, {"abstract": "We present a method for visualising the response of a deep neural network to\na specific input. For image data for instance our method will highlight areas\nthat provide evidence in favor of, and against choosing a certain class. The\nmethod overcomes several shortcomings of previous methods and provides great\nadditional insight into the decision making process of convolutional networks,\nwhich is important both to improve models and to accelerate the adoption of\nsuch methods in e.g. medicine. In experiments on ImageNet data, we illustrate\nhow the method works and can be applied in different ways to understand deep\nneural nets.", "authors": ["Luisa M. Zintgraf", "Taco S. Cohen", "Max Welling"], "category": "cs.CV", "comment": "Please note that this version of the article is outdated. The new\n  version (published at ICLR2017) ...", "img": "/static/thumbs/1603.02518v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1603.02518v3", "num_discussion": 0, "originally_published_time": "3/8/2016", "pid": "1603.02518v3", "published_time": "6/12/2017", "rawpid": "1603.02518", "tags": ["cs.CV"], "title": "A New Method to Visualize Deep Neural Networks"}, {"abstract": "Recurrent Neural Networks (RNNs), and specifically a variant with Long\nShort-Term Memory (LSTM), are enjoying renewed interest as a result of\nsuccessful applications in a wide range of machine learning problems that\ninvolve sequential data. However, while LSTMs provide exceptional results in\npractice, the source of their performance and their limitations remain rather\npoorly understood. Using character-level language models as an interpretable\ntestbed, we aim to bridge this gap by providing an analysis of their\nrepresentations, predictions and error types. In particular, our experiments\nreveal the existence of interpretable cells that keep track of long-range\ndependencies such as line lengths, quotes and brackets. Moreover, our\ncomparative analysis with finite horizon n-gram models traces the source of the\nLSTM improvements to long-range structural dependencies. Finally, we provide\nanalysis of the remaining errors and suggests areas for further study.", "authors": ["Andrej Karpathy", "Justin Johnson", "Li Fei-Fei"], "category": "cs.LG", "comment": "changing style, adding references, minor changes to text", "img": "/static/thumbs/1506.02078v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1506.02078v2", "num_discussion": 0, "originally_published_time": "6/5/2015", "pid": "1506.02078v2", "published_time": "11/17/2015", "rawpid": "1506.02078", "tags": ["cs.LG", "cs.CL", "cs.NE"], "title": "Visualizing and Understanding Recurrent Networks"}, {"abstract": "Batch Normalization is quite effective at accelerating and improving the\ntraining of deep models. However, its effectiveness diminishes when the\ntraining minibatches are small, or do not consist of independent samples. We\nhypothesize that this is due to the dependence of model layer inputs on all the\nexamples in the minibatch, and different activations being produced between\ntraining and inference. We propose Batch Renormalization, a simple and\neffective extension to ensure that the training and inference models generate\nthe same outputs that depend on individual examples rather than the entire\nminibatch. Models trained with Batch Renormalization perform substantially\nbetter than batchnorm when training with small or non-i.i.d. minibatches. At\nthe same time, Batch Renormalization retains the benefits of batchnorm such as\ninsensitivity to initialization and training efficiency.", "authors": ["Sergey Ioffe"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1702.03275v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1702.03275v2", "num_discussion": 0, "originally_published_time": "2/10/2017", "pid": "1702.03275v2", "published_time": "3/30/2017", "rawpid": "1702.03275", "tags": ["cs.LG"], "title": "Batch Renormalization: Towards Reducing Minibatch Dependence in\n  Batch-Normalized Models"}, {"abstract": "Despite recent breakthroughs in the applications of deep neural networks, one\nsetting that presents a persistent challenge is that of \"one-shot learning.\"\nTraditional gradient-based networks require a lot of data to learn, often\nthrough extensive iterative training. When new data is encountered, the models\nmust inefficiently relearn their parameters to adequately incorporate the new\ninformation without catastrophic interference. Architectures with augmented\nmemory capacities, such as Neural Turing Machines (NTMs), offer the ability to\nquickly encode and retrieve new information, and hence can potentially obviate\nthe downsides of conventional models. Here, we demonstrate the ability of a\nmemory-augmented neural network to rapidly assimilate new data, and leverage\nthis data to make accurate predictions after only a few samples. We also\nintroduce a new method for accessing an external memory that focuses on memory\ncontent, unlike previous methods that additionally use memory location-based\nfocusing mechanisms.", "authors": ["Adam Santoro", "Sergey Bartunov", "Matthew Botvinick", "Daan Wierstra", "Timothy Lillicrap"], "category": "cs.LG", "comment": "13 pages, 8 figures", "img": "/static/thumbs/1605.06065v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1605.06065v1", "num_discussion": 0, "originally_published_time": "5/19/2016", "pid": "1605.06065v1", "published_time": "5/19/2016", "rawpid": "1605.06065", "tags": ["cs.LG"], "title": "One-shot Learning with Memory-Augmented Neural Networks"}, {"abstract": "Learning from a few examples remains a key challenge in machine learning.\nDespite recent advances in important domains such as vision and language, the\nstandard supervised deep learning paradigm does not offer a satisfactory\nsolution for learning new concepts rapidly from little data. In this work, we\nemploy ideas from metric learning based on deep neural features and from recent\nadvances that augment neural networks with external memories. Our framework\nlearns a network that maps a small labelled support set and an unlabelled\nexample to its label, obviating the need for fine-tuning to adapt to new class\ntypes. We then define one-shot learning problems on vision (using Omniglot,\nImageNet) and language tasks. Our algorithm improves one-shot accuracy on\nImageNet from 87.6% to 93.2% and from 88.0% to 93.8% on Omniglot compared to\ncompeting approaches. We also demonstrate the usefulness of the same model on\nlanguage modeling by introducing a one-shot task on the Penn Treebank.", "authors": ["Oriol Vinyals", "Charles Blundell", "Timothy Lillicrap", "Koray Kavukcuoglu", "Daan Wierstra"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1606.04080v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1606.04080v1", "num_discussion": 0, "originally_published_time": "6/13/2016", "pid": "1606.04080v1", "published_time": "6/13/2016", "rawpid": "1606.04080", "tags": ["cs.LG", "stat.ML"], "title": "Matching Networks for One Shot Learning"}, {"abstract": "We investigate conditional adversarial networks as a general-purpose solution\nto image-to-image translation problems. These networks not only learn the\nmapping from input image to output image, but also learn a loss function to\ntrain this mapping. This makes it possible to apply the same generic approach\nto problems that traditionally would require very different loss formulations.\nWe demonstrate that this approach is effective at synthesizing photos from\nlabel maps, reconstructing objects from edge maps, and colorizing images, among\nother tasks. As a community, we no longer hand-engineer our mapping functions,\nand this work suggests we can achieve reasonable results without\nhand-engineering our loss functions either.", "authors": ["Phillip Isola", "Jun-Yan Zhu", "Tinghui Zhou", "Alexei A. Efros"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1611.07004v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1611.07004v1", "num_discussion": 0, "originally_published_time": "11/21/2016", "pid": "1611.07004v1", "published_time": "11/21/2016", "rawpid": "1611.07004", "tags": ["cs.CV"], "title": "Image-to-Image Translation with Conditional Adversarial Networks"}, {"abstract": "Modeling the distribution of natural images is a landmark problem in\nunsupervised learning. This task requires an image model that is at once\nexpressive, tractable and scalable. We present a deep neural network that\nsequentially predicts the pixels in an image along the two spatial dimensions.\nOur method models the discrete probability of the raw pixel values and encodes\nthe complete set of dependencies in the image. Architectural novelties include\nfast two-dimensional recurrent layers and an effective use of residual\nconnections in deep recurrent networks. We achieve log-likelihood scores on\nnatural images that are considerably better than the previous state of the art.\nOur main results also provide benchmarks on the diverse ImageNet dataset.\nSamples generated from the model appear crisp, varied and globally coherent.", "authors": ["Aaron van den Oord", "Nal Kalchbrenner", "Koray Kavukcuoglu"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1601.06759v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1601.06759v3", "num_discussion": 0, "originally_published_time": "1/25/2016", "pid": "1601.06759v3", "published_time": "8/19/2016", "rawpid": "1601.06759", "tags": ["cs.CV", "cs.LG", "cs.NE"], "title": "Pixel Recurrent Neural Networks"}, {"abstract": "The prevalent approach to sequence to sequence learning maps an input\nsequence to a variable length output sequence via recurrent neural networks. We\nintroduce an architecture based entirely on convolutional neural networks.\nCompared to recurrent models, computations over all elements can be fully\nparallelized during training and optimization is easier since the number of\nnon-linearities is fixed and independent of the input length. Our use of gated\nlinear units eases gradient propagation and we equip each decoder layer with a\nseparate attention module. We outperform the accuracy of the deep LSTM setup of\nWu et al. (2016) on both WMT\u002714 English-German and WMT\u002714 English-French\ntranslation at an order of magnitude faster speed, both on GPU and CPU.", "authors": ["Jonas Gehring", "Michael Auli", "David Grangier", "Denis Yarats", "Yann N. Dauphin"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1705.03122v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1705.03122v3", "num_discussion": 0, "originally_published_time": "5/8/2017", "pid": "1705.03122v3", "published_time": "7/25/2017", "rawpid": "1705.03122", "tags": ["cs.CL"], "title": "Convolutional Sequence to Sequence Learning"}, {"abstract": "The capacity of a neural network to absorb information is limited by its\nnumber of parameters. Conditional computation, where parts of the network are\nactive on a per-example basis, has been proposed in theory as a way of\ndramatically increasing model capacity without a proportional increase in\ncomputation. In practice, however, there are significant algorithmic and\nperformance challenges. In this work, we address these challenges and finally\nrealize the promise of conditional computation, achieving greater than 1000x\nimprovements in model capacity with only minor losses in computational\nefficiency on modern GPU clusters. We introduce a Sparsely-Gated\nMixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward\nsub-networks. A trainable gating network determines a sparse combination of\nthese experts to use for each example. We apply the MoE to the tasks of\nlanguage modeling and machine translation, where model capacity is critical for\nabsorbing the vast quantities of knowledge available in the training corpora.\nWe present model architectures in which a MoE with up to 137 billion parameters\nis applied convolutionally between stacked LSTM layers. On large language\nmodeling and machine translation benchmarks, these models achieve significantly\nbetter results than state-of-the-art at lower computational cost.", "authors": ["Noam Shazeer", "Azalia Mirhoseini", "Krzysztof Maziarz", "Andy Davis", "Quoc Le", "Geoffrey Hinton", "Jeff Dean"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1701.06538v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1701.06538v1", "num_discussion": 0, "originally_published_time": "1/23/2017", "pid": "1701.06538v1", "published_time": "1/23/2017", "rawpid": "1701.06538", "tags": ["cs.LG", "cs.CL", "cs.NE", "stat.ML"], "title": "Outrageously Large Neural Networks: The Sparsely-Gated\n  Mixture-of-Experts Layer"}, {"abstract": "We introduce the \"Energy-based Generative Adversarial Network\" model (EBGAN)\nwhich views the discriminator as an energy function that attributes low\nenergies to the regions near the data manifold and higher energies to other\nregions. Similar to the probabilistic GANs, a generator is seen as being\ntrained to produce contrastive samples with minimal energies, while the\ndiscriminator is trained to assign high energies to these generated samples.\nViewing the discriminator as an energy function allows to use a wide variety of\narchitectures and loss functionals in addition to the usual binary classifier\nwith logistic output. Among them, we show one instantiation of EBGAN framework\nas using an auto-encoder architecture, with the energy being the reconstruction\nerror, in place of the discriminator. We show that this form of EBGAN exhibits\nmore stable behavior than regular GANs during training. We also show that a\nsingle-scale architecture can be trained to generate high-resolution images.", "authors": ["Junbo Zhao", "Michael Mathieu", "Yann LeCun"], "category": "cs.LG", "comment": "Submitted to ICLR 2017", "img": "/static/thumbs/1609.03126v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1609.03126v4", "num_discussion": 0, "originally_published_time": "9/11/2016", "pid": "1609.03126v4", "published_time": "3/6/2017", "rawpid": "1609.03126", "tags": ["cs.LG", "stat.ML"], "title": "Energy-based Generative Adversarial Network"}, {"abstract": "Given a grayscale photograph as input, this paper attacks the problem of\nhallucinating a plausible color version of the photograph. This problem is\nclearly underconstrained, so previous approaches have either relied on\nsignificant user interaction or resulted in desaturated colorizations. We\npropose a fully automatic approach that produces vibrant and realistic\ncolorizations. We embrace the underlying uncertainty of the problem by posing\nit as a classification task and use class-rebalancing at training time to\nincrease the diversity of colors in the result. The system is implemented as a\nfeed-forward pass in a CNN at test time and is trained on over a million color\nimages. We evaluate our algorithm using a \"colorization Turing test,\" asking\nhuman participants to choose between a generated and ground truth color image.\nOur method successfully fools humans on 32% of the trials, significantly higher\nthan previous methods. Moreover, we show that colorization can be a powerful\npretext task for self-supervised feature learning, acting as a cross-channel\nencoder. This approach results in state-of-the-art performance on several\nfeature learning benchmarks.", "authors": ["Richard Zhang", "Phillip Isola", "Alexei A. Efros"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1603.08511v5.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1603.08511v5", "num_discussion": 0, "originally_published_time": "3/28/2016", "pid": "1603.08511v5", "published_time": "10/5/2016", "rawpid": "1603.08511", "tags": ["cs.CV"], "title": "Colorful Image Colorization"}, {"abstract": "One of the core problems of modern statistics is to approximate\ndifficult-to-compute probability densities. This problem is especially\nimportant in Bayesian statistics, which frames all inference about unknown\nquantities as a calculation involving the posterior density. In this paper, we\nreview variational inference (VI), a method from machine learning that\napproximates probability densities through optimization. VI has been used in\nmany applications and tends to be faster than classical methods, such as Markov\nchain Monte Carlo sampling. The idea behind VI is to first posit a family of\ndensities and then to find the member of that family which is close to the\ntarget. Closeness is measured by Kullback-Leibler divergence. We review the\nideas behind mean-field variational inference, discuss the special case of VI\napplied to exponential family models, present a full example with a Bayesian\nmixture of Gaussians, and derive a variant that uses stochastic optimization to\nscale up to massive data. We discuss modern research in VI and highlight\nimportant open problems. VI is powerful, but it is not yet well understood. Our\nhope in writing this paper is to catalyze statistical research on this class of\nalgorithms.", "authors": ["David M. Blei", "Alp Kucukelbir", "Jon D. McAuliffe"], "category": "stat.CO", "comment": "", "img": "/static/thumbs/1601.00670v5.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1601.00670v5", "num_discussion": 0, "originally_published_time": "1/4/2016", "pid": "1601.00670v5", "published_time": "6/14/2017", "rawpid": "1601.00670", "tags": ["stat.CO", "cs.LG", "stat.ML"], "title": "Variational Inference: A Review for Statisticians"}, {"abstract": "Convolutional Neural Networks define an exceptionally powerful class of\nmodels, but are still limited by the lack of ability to be spatially invariant\nto the input data in a computationally and parameter efficient manner. In this\nwork we introduce a new learnable module, the Spatial Transformer, which\nexplicitly allows the spatial manipulation of data within the network. This\ndifferentiable module can be inserted into existing convolutional\narchitectures, giving neural networks the ability to actively spatially\ntransform feature maps, conditional on the feature map itself, without any\nextra training supervision or modification to the optimisation process. We show\nthat the use of spatial transformers results in models which learn invariance\nto translation, scale, rotation and more generic warping, resulting in\nstate-of-the-art performance on several benchmarks, and for a number of classes\nof transformations.", "authors": ["Max Jaderberg", "Karen Simonyan", "Andrew Zisserman", "Koray Kavukcuoglu"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1506.02025v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1506.02025v3", "num_discussion": 0, "originally_published_time": "6/5/2015", "pid": "1506.02025v3", "published_time": "2/4/2016", "rawpid": "1506.02025", "tags": ["cs.CV"], "title": "Spatial Transformer Networks"}, {"abstract": "This paper explores a simple and efficient baseline for text classification.\nOur experiments show that our fast text classifier fastText is often on par\nwith deep learning classifiers in terms of accuracy, and many orders of\nmagnitude faster for training and evaluation. We can train fastText on more\nthan one billion words in less than ten minutes using a standard multicore~CPU,\nand classify half a million sentences among~312K classes in less than a minute.", "authors": ["Armand Joulin", "Edouard Grave", "Piotr Bojanowski", "Tomas Mikolov"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1607.01759v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1607.01759v3", "num_discussion": 0, "originally_published_time": "7/6/2016", "pid": "1607.01759v3", "published_time": "8/9/2016", "rawpid": "1607.01759", "tags": ["cs.CL"], "title": "Bag of Tricks for Efficient Text Classification"}, {"abstract": "This paper describes InfoGAN, an information-theoretic extension to the\nGenerative Adversarial Network that is able to learn disentangled\nrepresentations in a completely unsupervised manner. InfoGAN is a generative\nadversarial network that also maximizes the mutual information between a small\nsubset of the latent variables and the observation. We derive a lower bound to\nthe mutual information objective that can be optimized efficiently, and show\nthat our training procedure can be interpreted as a variation of the Wake-Sleep\nalgorithm. Specifically, InfoGAN successfully disentangles writing styles from\ndigit shapes on the MNIST dataset, pose from lighting of 3D rendered images,\nand background digits from the central digit on the SVHN dataset. It also\ndiscovers visual concepts that include hair styles, presence/absence of\neyeglasses, and emotions on the CelebA face dataset. Experiments show that\nInfoGAN learns interpretable representations that are competitive with\nrepresentations learned by existing fully supervised methods.", "authors": ["Xi Chen", "Yan Duan", "Rein Houthooft", "John Schulman", "Ilya Sutskever", "Pieter Abbeel"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1606.03657v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1606.03657v1", "num_discussion": 0, "originally_published_time": "6/12/2016", "pid": "1606.03657v1", "published_time": "6/12/2016", "rawpid": "1606.03657", "tags": ["cs.LG", "stat.ML"], "title": "InfoGAN: Interpretable Representation Learning by Information Maximizing\n  Generative Adversarial Nets"}, {"abstract": "Both generative adversarial networks (GAN) in unsupervised learning and\nactor-critic methods in reinforcement learning (RL) have gained a reputation\nfor being difficult to optimize. Practitioners in both fields have amassed a\nlarge number of strategies to mitigate these instabilities and improve\ntraining. Here we show that GANs can be viewed as actor-critic methods in an\nenvironment where the actor cannot affect the reward. We review the strategies\nfor stabilizing training for each class of models, both those that generalize\nbetween the two and those that are particular to that model. We also review a\nnumber of extensions to GANs and RL algorithms with even more complicated\ninformation flow. We hope that by highlighting this formal connection we will\nencourage both GAN and RL communities to develop general, scalable, and stable\nalgorithms for multilevel optimization with deep networks, and to draw\ninspiration across communities.", "authors": ["David Pfau", "Oriol Vinyals"], "category": "cs.LG", "comment": "Added comments on inverse reinforcement learning", "img": "/static/thumbs/1610.01945v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1610.01945v3", "num_discussion": 0, "originally_published_time": "10/6/2016", "pid": "1610.01945v3", "published_time": "1/18/2017", "rawpid": "1610.01945", "tags": ["cs.LG", "stat.ML"], "title": "Connecting Generative Adversarial Networks and Actor-Critic Methods"}, {"abstract": "This work explores hypernetworks: an approach of using a one network, also\nknown as a hypernetwork, to generate the weights for another network.\nHypernetworks provide an abstraction that is similar to what is found in\nnature: the relationship between a genotype - the hypernetwork - and a\nphenotype - the main network. Though they are also reminiscent of HyperNEAT in\nevolution, our hypernetworks are trained end-to-end with backpropagation and\nthus are usually faster. The focus of this work is to make hypernetworks useful\nfor deep convolutional networks and long recurrent networks, where\nhypernetworks can be viewed as relaxed form of weight-sharing across layers.\nOur main result is that hypernetworks can generate non-shared weights for LSTM\nand achieve near state-of-the-art results on a variety of sequence modelling\ntasks including character-level language modelling, handwriting generation and\nneural machine translation, challenging the weight-sharing paradigm for\nrecurrent networks. Our results also show that hypernetworks applied to\nconvolutional networks still achieve respectable results for image recognition\ntasks compared to state-of-the-art baseline models while requiring fewer\nlearnable parameters.", "authors": ["David Ha", "Andrew Dai", "Quoc V. Le"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1609.09106v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1609.09106v4", "num_discussion": 1, "originally_published_time": "9/27/2016", "pid": "1609.09106v4", "published_time": "12/1/2016", "rawpid": "1609.09106", "tags": ["cs.LG"], "title": "HyperNetworks"}, {"abstract": "Neural networks are powerful and flexible models that work well for many\ndifficult learning tasks in image, speech and natural language understanding.\nDespite their success, neural networks are still hard to design. In this paper,\nwe use a recurrent network to generate the model descriptions of neural\nnetworks and train this RNN with reinforcement learning to maximize the\nexpected accuracy of the generated architectures on a validation set. On the\nCIFAR-10 dataset, our method, starting from scratch, can design a novel network\narchitecture that rivals the best human-invented architecture in terms of test\nset accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is\n0.09 percent better and 1.05x faster than the previous state-of-the-art model\nthat used a similar architectural scheme. On the Penn Treebank dataset, our\nmodel can compose a novel recurrent cell that outperforms the widely-used LSTM\ncell, and other state-of-the-art baselines. Our cell achieves a test set\nperplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than\nthe previous state-of-the-art model. The cell can also be transferred to the\ncharacter language modeling task on PTB and achieves a state-of-the-art\nperplexity of 1.214.", "authors": ["Barret Zoph", "Quoc V. Le"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1611.01578v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1611.01578v2", "num_discussion": 1, "originally_published_time": "11/5/2016", "pid": "1611.01578v2", "published_time": "2/15/2017", "rawpid": "1611.01578", "tags": ["cs.LG", "cs.AI", "cs.NE"], "title": "Neural Architecture Search with Reinforcement Learning"}, {"abstract": "TensorFlow is an interface for expressing machine learning algorithms, and an\nimplementation for executing such algorithms. A computation expressed using\nTensorFlow can be executed with little or no change on a wide variety of\nheterogeneous systems, ranging from mobile devices such as phones and tablets\nup to large-scale distributed systems of hundreds of machines and thousands of\ncomputational devices such as GPU cards. The system is flexible and can be used\nto express a wide variety of algorithms, including training and inference\nalgorithms for deep neural network models, and it has been used for conducting\nresearch and for deploying machine learning systems into production across more\nthan a dozen areas of computer science and other fields, including speech\nrecognition, computer vision, robotics, information retrieval, natural language\nprocessing, geographic information extraction, and computational drug\ndiscovery. This paper describes the TensorFlow interface and an implementation\nof that interface that we have built at Google. The TensorFlow API and a\nreference implementation were released as an open-source package under the\nApache 2.0 license in November, 2015 and are available at www.tensorflow.org.", "authors": ["Mart\u00edn Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S. Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin", "Sanjay Ghemawat", "Ian Goodfellow", "Andrew Harp", "Geoffrey Irving", "Michael Isard", "Yangqing Jia", "Rafal Jozefowicz", "Lukasz Kaiser", "Manjunath Kudlur", "Josh Levenberg", "Dan Mane", "Rajat Monga", "Sherry Moore", "Derek Murray", "Chris Olah", "Mike Schuster", "Jonathon Shlens", "Benoit Steiner", "Ilya Sutskever", "Kunal Talwar", "Paul Tucker", "Vincent Vanhoucke", "Vijay Vasudevan", "Fernanda Viegas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "category": "cs.DC", "comment": "Version 2 updates only the metadata, to correct the formatting of\n  Mart\\\u0027in Abadi\u0027s name", "img": "/static/thumbs/1603.04467v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1603.04467v2", "num_discussion": 0, "originally_published_time": "3/14/2016", "pid": "1603.04467v2", "published_time": "3/16/2016", "rawpid": "1603.04467", "tags": ["cs.DC", "cs.LG"], "title": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed\n  Systems"}, {"abstract": "Deep learning thrives with large neural networks and large datasets. However,\nlarger networks and larger datasets result in longer training times that impede\nresearch and development progress. Distributed synchronous SGD offers a\npotential solution to this problem by dividing SGD minibatches over a pool of\nparallel workers. Yet to make this scheme efficient, the per-worker workload\nmust be large, which implies nontrivial growth in the SGD minibatch size. In\nthis paper, we empirically show that on the ImageNet dataset large minibatches\ncause optimization difficulties, but when these are addressed the trained\nnetworks exhibit good generalization. Specifically, we show no loss of accuracy\nwhen training with large minibatch sizes up to 8192 images. To achieve this\nresult, we adopt a linear scaling rule for adjusting learning rates as a\nfunction of minibatch size and develop a new warmup scheme that overcomes\noptimization challenges early in training. With these simple techniques, our\nCaffe2-based system trains ResNet-50 with a minibatch size of 8192 on 256 GPUs\nin one hour, while matching small minibatch accuracy. Using commodity hardware,\nour implementation achieves ~90% scaling efficiency when moving from 8 to 256\nGPUs. This system enables us to train visual recognition models on\ninternet-scale data with high efficiency.", "authors": ["Priya Goyal", "Piotr Doll\u00e1r", "Ross Girshick", "Pieter Noordhuis", "Lukasz Wesolowski", "Aapo Kyrola", "Andrew Tulloch", "Yangqing Jia", "Kaiming He"], "category": "cs.CV", "comment": "Tech report", "img": "/static/thumbs/1706.02677v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1706.02677v1", "num_discussion": 0, "originally_published_time": "6/8/2017", "pid": "1706.02677v1", "published_time": "6/8/2017", "rawpid": "1706.02677", "tags": ["cs.CV", "cs.DC", "cs.LG"], "title": "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour"}, {"abstract": "Most tasks in natural language processing can be cast into question answering\n(QA) problems over language input. We introduce the dynamic memory network\n(DMN), a neural network architecture which processes input sequences and\nquestions, forms episodic memories, and generates relevant answers. Questions\ntrigger an iterative attention process which allows the model to condition its\nattention on the inputs and the result of previous iterations. These results\nare then reasoned over in a hierarchical recurrent sequence model to generate\nanswers. The DMN can be trained end-to-end and obtains state-of-the-art results\non several types of tasks and datasets: question answering (Facebook\u0027s bAbI\ndataset), text classification for sentiment analysis (Stanford Sentiment\nTreebank) and sequence modeling for part-of-speech tagging (WSJ-PTB). The\ntraining for these different tasks relies exclusively on trained word vector\nrepresentations and input-question-answer triplets.", "authors": ["Ankit Kumar", "Ozan Irsoy", "Peter Ondruska", "Mohit Iyyer", "James Bradbury", "Ishaan Gulrajani", "Victor Zhong", "Romain Paulus", "Richard Socher"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1506.07285v5.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1506.07285v5", "num_discussion": 0, "originally_published_time": "6/24/2015", "pid": "1506.07285v5", "published_time": "3/5/2016", "rawpid": "1506.07285", "tags": ["cs.CL", "cs.LG", "cs.NE"], "title": "Ask Me Anything: Dynamic Memory Networks for Natural Language Processing"}, {"abstract": "For artificial general intelligence (AGI) it would be efficient if multiple\nusers trained the same giant neural network, permitting parameter reuse,\nwithout catastrophic forgetting. PathNet is a first step in this direction. It\nis a neural network algorithm that uses agents embedded in the neural network\nwhose task is to discover which parts of the network to re-use for new tasks.\nAgents are pathways (views) through the network which determine the subset of\nparameters that are used and updated by the forwards and backwards passes of\nthe backpropogation algorithm. During learning, a tournament selection genetic\nalgorithm is used to select pathways through the neural network for replication\nand mutation. Pathway fitness is the performance of that pathway measured\naccording to a cost function. We demonstrate successful transfer learning;\nfixing the parameters along a path learned on task A and re-evolving a new\npopulation of paths for task B, allows task B to be learned faster than it\ncould be learned from scratch or after fine-tuning. Paths evolved on task B\nre-use parts of the optimal path evolved on task A. Positive transfer was\ndemonstrated for binary MNIST, CIFAR, and SVHN supervised learning\nclassification tasks, and a set of Atari and Labyrinth reinforcement learning\ntasks, suggesting PathNets have general applicability for neural network\ntraining. Finally, PathNet also significantly improves the robustness to\nhyperparameter choices of a parallel asynchronous reinforcement learning\nalgorithm (A3C).", "authors": ["Chrisantha Fernando", "Dylan Banarse", "Charles Blundell", "Yori Zwols", "David Ha", "Andrei A. Rusu", "Alexander Pritzel", "Daan Wierstra"], "category": "cs.NE", "comment": "", "img": "/static/thumbs/1701.08734v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1701.08734v1", "num_discussion": 0, "originally_published_time": "1/30/2017", "pid": "1701.08734v1", "published_time": "1/30/2017", "rawpid": "1701.08734", "tags": ["cs.NE", "cs.LG"], "title": "PathNet: Evolution Channels Gradient Descent in Super Neural Networks"}, {"abstract": "Generative adversarial networks (GANs) are a recently proposed class of\ngenerative models in which a generator is trained to optimize a cost function\nthat is being simultaneously learned by a discriminator. While the idea of\nlearning cost functions is relatively new to the field of generative modeling,\nlearning costs has long been studied in control and reinforcement learning (RL)\ndomains, typically for imitation learning from demonstrations. In these fields,\nlearning cost function underlying observed behavior is known as inverse\nreinforcement learning (IRL) or inverse optimal control. While at first the\nconnection between cost learning in RL and cost learning in generative modeling\nmay appear to be a superficial one, we show in this paper that certain IRL\nmethods are in fact mathematically equivalent to GANs. In particular, we\ndemonstrate an equivalence between a sample-based algorithm for maximum entropy\nIRL and a GAN in which the generator\u0027s density can be evaluated and is provided\nas an additional input to the discriminator. Interestingly, maximum entropy IRL\nis a special case of an energy-based model. We discuss the interpretation of\nGANs as an algorithm for training energy-based models, and relate this\ninterpretation to other recent work that seeks to connect GANs and EBMs. By\nformally highlighting the connection between GANs, IRL, and EBMs, we hope that\nresearchers in all three communities can better identify and apply transferable\nideas from one domain to another, particularly for developing more stable and\nscalable algorithms: a major challenge in all three domains.", "authors": ["Chelsea Finn", "Paul Christiano", "Pieter Abbeel", "Sergey Levine"], "category": "cs.LG", "comment": "NIPS 2016 Workshop on Adversarial Training. First two authors\n  contributed equally", "img": "/static/thumbs/1611.03852v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1611.03852v3", "num_discussion": 0, "originally_published_time": "11/11/2016", "pid": "1611.03852v3", "published_time": "11/25/2016", "rawpid": "1611.03852", "tags": ["cs.LG", "cs.AI"], "title": "A Connection between Generative Adversarial Networks, Inverse\n  Reinforcement Learning, and Energy-Based Models"}, {"abstract": "As a new way of training generative models, Generative Adversarial Nets (GAN)\nthat uses a discriminative model to guide the training of the generative model\nhas enjoyed considerable success in generating real-valued data. However, it\nhas limitations when the goal is for generating sequences of discrete tokens. A\nmajor reason lies in that the discrete outputs from the generative model make\nit difficult to pass the gradient update from the discriminative model to the\ngenerative model. Also, the discriminative model can only assess a complete\nsequence, while for a partially generated sequence, it is non-trivial to\nbalance its current score and the future one once the entire sequence has been\ngenerated. In this paper, we propose a sequence generation framework, called\nSeqGAN, to solve the problems. Modeling the data generator as a stochastic\npolicy in reinforcement learning (RL), SeqGAN bypasses the generator\ndifferentiation problem by directly performing gradient policy update. The RL\nreward signal comes from the GAN discriminator judged on a complete sequence,\nand is passed back to the intermediate state-action steps using Monte Carlo\nsearch. Extensive experiments on synthetic data and real-world tasks\ndemonstrate significant improvements over strong baselines.", "authors": ["Lantao Yu", "Weinan Zhang", "Jun Wang", "Yong Yu"], "category": "cs.LG", "comment": "The Thirty-First AAAI Conference on Artificial Intelligence (AAAI\n  2017)", "img": "/static/thumbs/1609.05473v5.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1609.05473v5", "num_discussion": 0, "originally_published_time": "9/18/2016", "pid": "1609.05473v5", "published_time": "12/9/2016", "rawpid": "1609.05473", "tags": ["cs.LG", "cs.AI"], "title": "SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient"}, {"abstract": "Learning goal-directed behavior in environments with sparse feedback is a\nmajor challenge for reinforcement learning algorithms. The primary difficulty\narises due to insufficient exploration, resulting in an agent being unable to\nlearn robust value functions. Intrinsically motivated agents can explore new\nbehavior for its own sake rather than to directly solve problems. Such\nintrinsic behaviors could eventually help the agent solve tasks posed by the\nenvironment. We present hierarchical-DQN (h-DQN), a framework to integrate\nhierarchical value functions, operating at different temporal scales, with\nintrinsically motivated deep reinforcement learning. A top-level value function\nlearns a policy over intrinsic goals, and a lower-level function learns a\npolicy over atomic actions to satisfy the given goals. h-DQN allows for\nflexible goal specifications, such as functions over entities and relations.\nThis provides an efficient space for exploration in complicated environments.\nWe demonstrate the strength of our approach on two problems with very sparse,\ndelayed feedback: (1) a complex discrete stochastic decision process, and (2)\nthe classic ATARI game `Montezuma\u0027s Revenge\u0027.", "authors": ["Tejas D. Kulkarni", "Karthik R. Narasimhan", "Ardavan Saeedi", "Joshua B. Tenenbaum"], "category": "cs.LG", "comment": "14 pages, 7 figures", "img": "/static/thumbs/1604.06057v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1604.06057v2", "num_discussion": 0, "originally_published_time": "4/20/2016", "pid": "1604.06057v2", "published_time": "5/31/2016", "rawpid": "1604.06057", "tags": ["cs.LG", "cs.AI", "cs.CV", "cs.NE", "stat.ML"], "title": "Hierarchical Deep Reinforcement Learning: Integrating Temporal\n  Abstraction and Intrinsic Motivation"}, {"abstract": "We adapt the ideas underlying the success of Deep Q-Learning to the\ncontinuous action domain. We present an actor-critic, model-free algorithm\nbased on the deterministic policy gradient that can operate over continuous\naction spaces. Using the same learning algorithm, network architecture and\nhyper-parameters, our algorithm robustly solves more than 20 simulated physics\ntasks, including classic problems such as cartpole swing-up, dexterous\nmanipulation, legged locomotion and car driving. Our algorithm is able to find\npolicies whose performance is competitive with those found by a planning\nalgorithm with full access to the dynamics of the domain and its derivatives.\nWe further demonstrate that for many of the tasks the algorithm can learn\npolicies end-to-end: directly from raw pixel inputs.", "authors": ["Timothy P. Lillicrap", "Jonathan J. Hunt", "Alexander Pritzel", "Nicolas Heess", "Tom Erez", "Yuval Tassa", "David Silver", "Daan Wierstra"], "category": "cs.LG", "comment": "10 pages + supplementary", "img": "/static/thumbs/1509.02971v5.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1509.02971v5", "num_discussion": 0, "originally_published_time": "9/9/2015", "pid": "1509.02971v5", "published_time": "2/29/2016", "rawpid": "1509.02971", "tags": ["cs.LG", "stat.ML"], "title": "Continuous control with deep reinforcement learning"}, {"abstract": "We present a class of efficient models called MobileNets for mobile and\nembedded vision applications. MobileNets are based on a streamlined\narchitecture that uses depth-wise separable convolutions to build light weight\ndeep neural networks. We introduce two simple global hyper-parameters that\nefficiently trade off between latency and accuracy. These hyper-parameters\nallow the model builder to choose the right sized model for their application\nbased on the constraints of the problem. We present extensive experiments on\nresource and accuracy tradeoffs and show strong performance compared to other\npopular models on ImageNet classification. We then demonstrate the\neffectiveness of MobileNets across a wide range of applications and use cases\nincluding object detection, finegrain classification, face attributes and large\nscale geo-localization.", "authors": ["Andrew G. Howard", "Menglong Zhu", "Bo Chen", "Dmitry Kalenichenko", "Weijun Wang", "Tobias Weyand", "Marco Andreetto", "Hartwig Adam"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1704.04861v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.04861v1", "num_discussion": 1, "originally_published_time": "4/17/2017", "pid": "1704.04861v1", "published_time": "4/17/2017", "rawpid": "1704.04861", "tags": ["cs.CV"], "title": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision\n  Applications"}, {"abstract": "This report is targeted to groups who are subject matter experts in their\napplication but deep learning novices. It contains practical advice for those\ninterested in testing the use of deep neural networks on applications that are\nnovel for deep learning. We suggest making your project more manageable by\ndividing it into phases. For each phase this report contains numerous\nrecommendations and insights to assist novice practitioners.", "authors": ["Leslie N. Smith"], "category": "cs.SE", "comment": "", "img": "/static/thumbs/1704.01568v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.01568v1", "num_discussion": 0, "originally_published_time": "4/5/2017", "pid": "1704.01568v1", "published_time": "4/5/2017", "rawpid": "1704.01568", "tags": ["cs.SE", "cs.AI", "cs.NE"], "title": "Best Practices for Applying Deep Learning to Novel Applications"}, {"abstract": "Deep reinforcement learning methods attain super-human performance in a wide\nrange of environments. Such methods are grossly inefficient, often taking\norders of magnitudes more data than humans to achieve reasonable performance.\nWe propose Neural Episodic Control: a deep reinforcement learning agent that is\nable to rapidly assimilate new experiences and act upon them. Our agent uses a\nsemi-tabular representation of the value function: a buffer of past experience\ncontaining slowly changing state representations and rapidly updated estimates\nof the value function. We show across a wide range of environments that our\nagent learns significantly faster than other state-of-the-art, general purpose\ndeep reinforcement learning agents.", "authors": ["Alexander Pritzel", "Benigno Uria", "Sriram Srinivasan", "Adri\u00e0 Puigdom\u00e8nech", "Oriol Vinyals", "Demis Hassabis", "Daan Wierstra", "Charles Blundell"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1703.01988v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.01988v1", "num_discussion": 1, "originally_published_time": "3/6/2017", "pid": "1703.01988v1", "published_time": "3/6/2017", "rawpid": "1703.01988", "tags": ["cs.LG", "stat.ML"], "title": "Neural Episodic Control"}, {"abstract": "This paper is a review of the evolutionary history of deep learning models.\nIt covers from the genesis of neural networks when associationism modeling of\nthe brain is studied, to the models that dominate the last decade of research\nin deep learning like convolutional neural networks, deep belief networks, and\nrecurrent neural networks. In addition to a review of these models, this paper\nprimarily focuses on the precedents of the models above, examining how the\ninitial ideas are assembled to construct the early models and how these\npreliminary models are developed into their current forms. Many of these\nevolutionary paths last more than half a century and have a diversity of\ndirections. For example, CNN is built on prior knowledge of biological vision\nsystem; DBN is evolved from a trade-off of modeling power and computation\ncomplexity of graphical models and many nowadays models are neural counterparts\nof ancient linear models. This paper reviews these evolutionary paths and\noffers a concise thought flow of how these models are developed, and aims to\nprovide a thorough background for deep learning. More importantly, along with\nthe path, this paper summarizes the gist behind these milestones and proposes\nmany directions to guide the future research of deep learning.", "authors": ["Haohan Wang", "Bhiksha Raj"], "category": "cs.LG", "comment": "70 pages, 200 references", "img": "/static/thumbs/1702.07800v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1702.07800v4", "num_discussion": 0, "originally_published_time": "2/24/2017", "pid": "1702.07800v4", "published_time": "3/3/2017", "rawpid": "1702.07800", "tags": ["cs.LG", "cs.NE", "stat.ML"], "title": "On the Origin of Deep Learning"}, {"abstract": "We introduce Equilibrium Propagation, a learning framework for energy-based\nmodels. It involves only one kind of neural computation, performed in both the\nfirst phase (when the prediction is made) and the second phase of training\n(after the target or prediction error is revealed). Although this algorithm\ncomputes the gradient of an objective function just like Backpropagation, it\ndoes not need a special computation or circuit for the second phase, where\nerrors are implicitly propagated. Equilibrium Propagation shares similarities\nwith Contrastive Hebbian Learning and Contrastive Divergence while solving the\ntheoretical issues of both algorithms: our algorithm computes the gradient of a\nwell defined objective function. Because the objective function is defined in\nterms of local perturbations, the second phase of Equilibrium Propagation\ncorresponds to only nudging the prediction (fixed point, or stationary\ndistribution) towards a configuration that reduces prediction error. In the\ncase of a recurrent multi-layer supervised network, the output units are\nslightly nudged towards their target in the second phase, and the perturbation\nintroduced at the output layer propagates backward in the hidden layers. We\nshow that the signal \u0027back-propagated\u0027 during this second phase corresponds to\nthe propagation of error derivatives and encodes the gradient of the objective\nfunction, when the synaptic update corresponds to a standard form of\nspike-timing dependent plasticity. This work makes it more plausible that a\nmechanism similar to Backpropagation could be implemented by brains, since\nleaky integrator neural computation performs both inference and error\nback-propagation in our model. The only local difference between the two phases\nis whether synaptic changes are allowed or not.", "authors": ["Benjamin Scellier", "Yoshua Bengio"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1602.05179v5.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1602.05179v5", "num_discussion": 0, "originally_published_time": "2/16/2016", "pid": "1602.05179v5", "published_time": "3/28/2017", "rawpid": "1602.05179", "tags": ["cs.LG"], "title": "Equilibrium Propagation: Bridging the Gap Between Energy-Based Models\n  and Backpropagation"}, {"abstract": "Gatys et al. (2015) showed that optimizing pixels to match features in a\nconvolutional network with respect reference image features is a way to render\nimages of high visual quality. We show that unrolling this gradient-based\noptimization yields a recurrent computation that creates images by\nincrementally adding onto a visual \"canvas\". We propose a recurrent generative\nmodel inspired by this view, and show that it can be trained using adversarial\ntraining to generate very good image samples. We also propose a way to\nquantitatively compare adversarial networks by having the generators and\ndiscriminators of these networks compete against each other.", "authors": ["Daniel Jiwoong Im", "Chris Dongjoo Kim", "Hui Jiang", "Roland Memisevic"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1602.05110v5.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1602.05110v5", "num_discussion": 0, "originally_published_time": "2/16/2016", "pid": "1602.05110v5", "published_time": "12/13/2016", "rawpid": "1602.05110", "tags": ["cs.LG", "cs.CV"], "title": "Generating images with recurrent adversarial networks"}, {"abstract": "We investigate a new method to augment recurrent neural networks with extra\nmemory without increasing the number of network parameters. The system has an\nassociative memory based on complex-valued vectors and is closely related to\nHolographic Reduced Representations and Long Short-Term Memory networks.\nHolographic Reduced Representations have limited capacity: as they store more\ninformation, each retrieval becomes noisier due to interference. Our system in\ncontrast creates redundant copies of stored information, which enables\nretrieval with reduced noise. Experiments demonstrate faster learning on\nmultiple memorization tasks.", "authors": ["Ivo Danihelka", "Greg Wayne", "Benigno Uria", "Nal Kalchbrenner", "Alex Graves"], "category": "cs.NE", "comment": "ICML-2016", "img": "/static/thumbs/1602.03032v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1602.03032v2", "num_discussion": 0, "originally_published_time": "2/9/2016", "pid": "1602.03032v2", "published_time": "5/19/2016", "rawpid": "1602.03032", "tags": ["cs.NE"], "title": "Associative Long Short-Term Memory"}, {"abstract": "We propose a new technique for visual attribute transfer across images that\nmay have very different appearance but have perceptually similar semantic\nstructure. By visual attribute transfer, we mean transfer of visual information\n(such as color, tone, texture, and style) from one image to another. For\nexample, one image could be that of a painting or a sketch while the other is a\nphoto of a real scene, and both depict the same type of scene.\n  Our technique finds semantically-meaningful dense correspondences between two\ninput images. To accomplish this, it adapts the notion of \"image analogy\" with\nfeatures extracted from a Deep Convolutional Neutral Network for matching; we\ncall our technique Deep Image Analogy. A coarse-to-fine strategy is used to\ncompute the nearest-neighbor field for generating the results. We validate the\neffectiveness of our proposed method in a variety of cases, including\nstyle/texture transfer, color/style swap, sketch/painting to photo, and time\nlapse.", "authors": ["Jing Liao", "Yuan Yao", "Lu Yuan", "Gang Hua", "Sing Bing Kang"], "category": "cs.CV", "comment": "Accepted by SIGGRAPH 2017", "img": "/static/thumbs/1705.01088v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1705.01088v2", "num_discussion": 2, "originally_published_time": "5/2/2017", "pid": "1705.01088v2", "published_time": "6/6/2017", "rawpid": "1705.01088", "tags": ["cs.CV"], "title": "Visual Attribute Transfer through Deep Image Analogy"}, {"abstract": "How can we perform efficient inference and learning in directed probabilistic\nmodels, in the presence of continuous latent variables with intractable\nposterior distributions, and large datasets? We introduce a stochastic\nvariational inference and learning algorithm that scales to large datasets and,\nunder some mild differentiability conditions, even works in the intractable\ncase. Our contributions is two-fold. First, we show that a reparameterization\nof the variational lower bound yields a lower bound estimator that can be\nstraightforwardly optimized using standard stochastic gradient methods. Second,\nwe show that for i.i.d. datasets with continuous latent variables per\ndatapoint, posterior inference can be made especially efficient by fitting an\napproximate inference model (also called a recognition model) to the\nintractable posterior using the proposed lower bound estimator. Theoretical\nadvantages are reflected in experimental results.", "authors": ["Diederik P Kingma", "Max Welling"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1312.6114v10.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1312.6114v10", "num_discussion": 0, "originally_published_time": "12/20/2013", "pid": "1312.6114v10", "published_time": "5/1/2014", "rawpid": "1312.6114", "tags": ["stat.ML", "cs.LG"], "title": "Auto-Encoding Variational Bayes"}, {"abstract": "We introduce FeUdal Networks (FuNs): a novel architecture for hierarchical\nreinforcement learning. Our approach is inspired by the feudal reinforcement\nlearning proposal of Dayan and Hinton, and gains power and efficacy by\ndecoupling end-to-end learning across multiple levels -- allowing it to utilise\ndifferent resolutions of time. Our framework employs a Manager module and a\nWorker module. The Manager operates at a lower temporal resolution and sets\nabstract goals which are conveyed to and enacted by the Worker. The Worker\ngenerates primitive actions at every tick of the environment. The decoupled\nstructure of FuN conveys several benefits -- in addition to facilitating very\nlong timescale credit assignment it also encourages the emergence of\nsub-policies associated with different goals set by the Manager. These\nproperties allow FuN to dramatically outperform a strong baseline agent on\ntasks that involve long-term credit assignment or memorisation. We demonstrate\nthe performance of our proposed system on a range of tasks from the ATARI suite\nand also from a 3D DeepMind Lab environment.", "authors": ["Alexander Sasha Vezhnevets", "Simon Osindero", "Tom Schaul", "Nicolas Heess", "Max Jaderberg", "David Silver", "Koray Kavukcuoglu"], "category": "cs.AI", "comment": "", "img": "/static/thumbs/1703.01161v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.01161v2", "num_discussion": 0, "originally_published_time": "3/3/2017", "pid": "1703.01161v2", "published_time": "3/6/2017", "rawpid": "1703.01161", "tags": ["cs.AI"], "title": "FeUdal Networks for Hierarchical Reinforcement Learning"}, {"abstract": "We introduce a design strategy for neural network macro-architecture based on\nself-similarity. Repeated application of a simple expansion rule generates deep\nnetworks whose structural layouts are precisely truncated fractals. These\nnetworks contain interacting subpaths of different lengths, but do not include\nany pass-through or residual connections; every internal signal is transformed\nby a filter and nonlinearity before being seen by subsequent layers. In\nexperiments, fractal networks match the excellent performance of standard\nresidual networks on both CIFAR and ImageNet classification tasks, thereby\ndemonstrating that residual representations may not be fundamental to the\nsuccess of extremely deep convolutional neural networks. Rather, the key may be\nthe ability to transition, during training, from effectively shallow to deep.\nWe note similarities with student-teacher behavior and develop drop-path, a\nnatural extension of dropout, to regularize co-adaptation of subpaths in\nfractal architectures. Such regularization allows extraction of\nhigh-performance fixed-depth subnetworks. Additionally, fractal networks\nexhibit an anytime property: shallow subnetworks provide a quick answer, while\ndeeper subnetworks, with higher latency, provide a more accurate answer.", "authors": ["Gustav Larsson", "Michael Maire", "Gregory Shakhnarovich"], "category": "cs.CV", "comment": "updated with ImageNet results; published as a conference paper at\n  ICLR 2017; project page at http:...", "img": "/static/thumbs/1605.07648v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1605.07648v4", "num_discussion": 0, "originally_published_time": "5/24/2016", "pid": "1605.07648v4", "published_time": "5/26/2017", "rawpid": "1605.07648", "tags": ["cs.CV"], "title": "FractalNet: Ultra-Deep Neural Networks without Residuals"}, {"abstract": "We explore the use of deep learning hierarchical models for problems in\nfinancial prediction and classification. Financial prediction problems -- such\nas those presented in designing and pricing securities, constructing\nportfolios, and risk management -- often involve large data sets with complex\ndata interactions that currently are difficult or impossible to specify in a\nfull economic model. Applying deep learning methods to these problems can\nproduce more useful results than standard methods in finance. In particular,\ndeep learning can detect and exploit interactions in the data that are, at\nleast currently, invisible to any existing financial economic theory.", "authors": ["J. B. Heaton", "N. G. Polson", "J. H. Witte"], "category": "cs.LG", "comment": "20 Pages, 5 Figures", "img": "/static/thumbs/1602.06561v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1602.06561v2", "num_discussion": 0, "originally_published_time": "2/21/2016", "pid": "1602.06561v2", "published_time": "2/23/2016", "rawpid": "1602.06561", "tags": ["cs.LG"], "title": "Deep Learning in Finance"}, {"abstract": "A very simple way to improve the performance of almost any machine learning\nalgorithm is to train many different models on the same data and then to\naverage their predictions. Unfortunately, making predictions using a whole\nensemble of models is cumbersome and may be too computationally expensive to\nallow deployment to a large number of users, especially if the individual\nmodels are large neural nets. Caruana and his collaborators have shown that it\nis possible to compress the knowledge in an ensemble into a single model which\nis much easier to deploy and we develop this approach further using a different\ncompression technique. We achieve some surprising results on MNIST and we show\nthat we can significantly improve the acoustic model of a heavily used\ncommercial system by distilling the knowledge in an ensemble of models into a\nsingle model. We also introduce a new type of ensemble composed of one or more\nfull models and many specialist models which learn to distinguish fine-grained\nclasses that the full models confuse. Unlike a mixture of experts, these\nspecialist models can be trained rapidly and in parallel.", "authors": ["Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean"], "category": "stat.ML", "comment": "NIPS 2014 Deep Learning Workshop", "img": "/static/thumbs/1503.02531v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1503.02531v1", "num_discussion": 0, "originally_published_time": "3/9/2015", "pid": "1503.02531v1", "published_time": "3/9/2015", "rawpid": "1503.02531", "tags": ["stat.ML", "cs.LG", "cs.NE"], "title": "Distilling the Knowledge in a Neural Network"}, {"abstract": "This tutorial introduces a new and powerful set of techniques variously\ncalled \"neural machine translation\" or \"neural sequence-to-sequence models\".\nThese techniques have been used in a number of tasks regarding the handling of\nhuman language, and can be a powerful tool in the toolbox of anyone who wants\nto model sequential data of some sort. The tutorial assumes that the reader\nknows the basics of math and programming, but does not assume any particular\nexperience with neural networks or natural language processing. It attempts to\nexplain the intuition behind the various methods covered, then delves into them\nwith enough mathematical detail to understand them concretely, and culiminates\nwith a suggestion for an implementation exercise, where readers can test that\nthey understood the content in practice.", "authors": ["Graham Neubig"], "category": "cs.CL", "comment": "65 Pages", "img": "/static/thumbs/1703.01619v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.01619v1", "num_discussion": 0, "originally_published_time": "3/5/2017", "pid": "1703.01619v1", "published_time": "3/5/2017", "rawpid": "1703.01619", "tags": ["cs.CL", "cs.LG", "stat.ML"], "title": "Neural Machine Translation and Sequence-to-sequence Models: A Tutorial"}, {"abstract": "The ability to learn tasks in a sequential fashion is crucial to the\ndevelopment of artificial intelligence. Neural networks are not, in general,\ncapable of this and it has been widely thought that catastrophic forgetting is\nan inevitable feature of connectionist models. We show that it is possible to\novercome this limitation and train networks that can maintain expertise on\ntasks which they have not experienced for a long time. Our approach remembers\nold tasks by selectively slowing down learning on the weights important for\nthose tasks. We demonstrate our approach is scalable and effective by solving a\nset of classification tasks based on the MNIST hand written digit dataset and\nby learning several Atari 2600 games sequentially.", "authors": ["James Kirkpatrick", "Razvan Pascanu", "Neil Rabinowitz", "Joel Veness", "Guillaume Desjardins", "Andrei A. Rusu", "Kieran Milan", "John Quan", "Tiago Ramalho", "Agnieszka Grabska-Barwinska", "Demis Hassabis", "Claudia Clopath", "Dharshan Kumaran", "Raia Hadsell"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1612.00796v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1612.00796v2", "num_discussion": 0, "originally_published_time": "12/2/2016", "pid": "1612.00796v2", "published_time": "1/25/2017", "rawpid": "1612.00796", "tags": ["cs.LG", "cs.AI", "stat.ML"], "title": "Overcoming catastrophic forgetting in neural networks"}, {"abstract": "Learning to solve complex sequences of tasks--while both leveraging transfer\nand avoiding catastrophic forgetting--remains a key obstacle to achieving\nhuman-level intelligence. The progressive networks approach represents a step\nforward in this direction: they are immune to forgetting and can leverage prior\nknowledge via lateral connections to previously learned features. We evaluate\nthis architecture extensively on a wide variety of reinforcement learning tasks\n(Atari and 3D maze games), and show that it outperforms common baselines based\non pretraining and finetuning. Using a novel sensitivity measure, we\ndemonstrate that transfer occurs at both low-level sensory and high-level\ncontrol layers of the learned policy.", "authors": ["Andrei A. Rusu", "Neil C. Rabinowitz", "Guillaume Desjardins", "Hubert Soyer", "James Kirkpatrick", "Koray Kavukcuoglu", "Razvan Pascanu", "Raia Hadsell"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1606.04671v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1606.04671v3", "num_discussion": 0, "originally_published_time": "6/15/2016", "pid": "1606.04671v3", "published_time": "9/7/2016", "rawpid": "1606.04671", "tags": ["cs.LG"], "title": "Progressive Neural Networks"}, {"abstract": "Convolutional networks are powerful visual models that yield hierarchies of\nfeatures. We show that convolutional networks by themselves, trained\nend-to-end, pixels-to-pixels, improve on the previous best result in semantic\nsegmentation. Our key insight is to build \"fully convolutional\" networks that\ntake input of arbitrary size and produce correspondingly-sized output with\nefficient inference and learning. We define and detail the space of fully\nconvolutional networks, explain their application to spatially dense prediction\ntasks, and draw connections to prior models. We adapt contemporary\nclassification networks (AlexNet, the VGG net, and GoogLeNet) into fully\nconvolutional networks and transfer their learned representations by\nfine-tuning to the segmentation task. We then define a skip architecture that\ncombines semantic information from a deep, coarse layer with appearance\ninformation from a shallow, fine layer to produce accurate and detailed\nsegmentations. Our fully convolutional network achieves improved segmentation\nof PASCAL VOC (30% relative improvement to 67.2% mean IU on 2012), NYUDv2, SIFT\nFlow, and PASCAL-Context, while inference takes one tenth of a second for a\ntypical image.", "authors": ["Evan Shelhamer", "Jonathan Long", "Trevor Darrell"], "category": "cs.CV", "comment": "to appear in PAMI (accepted May, 2016); journal edition of\n  arXiv:1411.4038", "img": "/static/thumbs/1605.06211v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1605.06211v1", "num_discussion": 0, "originally_published_time": "5/20/2016", "pid": "1605.06211v1", "published_time": "5/20/2016", "rawpid": "1605.06211", "tags": ["cs.CV"], "title": "Fully Convolutional Networks for Semantic Segmentation"}, {"abstract": "Many real-world applications can be described as large-scale games of\nimperfect information. To deal with these challenging domains, prior work has\nfocused on computing Nash equilibria in a handcrafted abstraction of the\ndomain. In this paper we introduce the first scalable end-to-end approach to\nlearning approximate Nash equilibria without prior domain knowledge. Our method\ncombines fictitious self-play with deep reinforcement learning. When applied to\nLeduc poker, Neural Fictitious Self-Play (NFSP) approached a Nash equilibrium,\nwhereas common reinforcement learning methods diverged. In Limit Texas Holdem,\na poker game of real-world scale, NFSP learnt a strategy that approached the\nperformance of state-of-the-art, superhuman algorithms based on significant\ndomain expertise.", "authors": ["Johannes Heinrich", "David Silver"], "category": "cs.LG", "comment": "updated version, incorporating conference feedback", "img": "/static/thumbs/1603.01121v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1603.01121v2", "num_discussion": 0, "originally_published_time": "3/3/2016", "pid": "1603.01121v2", "published_time": "6/28/2016", "rawpid": "1603.01121", "tags": ["cs.LG", "cs.AI", "cs.GT"], "title": "Deep Reinforcement Learning from Self-Play in Imperfect-Information\n  Games"}, {"abstract": "Imitation learning has been commonly applied to solve different tasks in\nisolation. This usually requires either careful feature engineering, or a\nsignificant number of samples. This is far from what we desire: ideally, robots\nshould be able to learn from very few demonstrations of any given task, and\ninstantly generalize to new situations of the same task, without requiring\ntask-specific engineering. In this paper, we propose a meta-learning framework\nfor achieving such capability, which we call one-shot imitation learning.\n  Specifically, we consider the setting where there is a very large set of\ntasks, and each task has many instantiations. For example, a task could be to\nstack all blocks on a table into a single tower, another task could be to place\nall blocks on a table into two-block towers, etc. In each case, different\ninstances of the task would consist of different sets of blocks with different\ninitial states. At training time, our algorithm is presented with pairs of\ndemonstrations for a subset of all tasks. A neural net is trained that takes as\ninput one demonstration and the current state (which initially is the initial\nstate of the other demonstration of the pair), and outputs an action with the\ngoal that the resulting sequence of states and actions matches as closely as\npossible with the second demonstration. At test time, a demonstration of a\nsingle instance of a new task is presented, and the neural net is expected to\nperform well on new instances of this new task. The use of soft attention\nallows the model to generalize to conditions and tasks unseen in the training\ndata. We anticipate that by training this model on a much greater variety of\ntasks and settings, we will obtain a general system that can turn any\ndemonstrations into robust policies that can accomplish an overwhelming variety\nof tasks.\n  Videos available at https://bit.ly/one-shot-imitation .", "authors": ["Yan Duan", "Marcin Andrychowicz", "Bradly C. Stadie", "Jonathan Ho", "Jonas Schneider", "Ilya Sutskever", "Pieter Abbeel", "Wojciech Zaremba"], "category": "cs.AI", "comment": "", "img": "/static/thumbs/1703.07326v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.07326v2", "num_discussion": 0, "originally_published_time": "3/21/2017", "pid": "1703.07326v2", "published_time": "3/22/2017", "rawpid": "1703.07326", "tags": ["cs.AI", "cs.LG", "cs.NE", "cs.RO"], "title": "One-Shot Imitation Learning"}, {"abstract": "In this paper, we propose a novel generative model named Stacked Generative\nAdversarial Networks (SGAN), which is trained to invert the hierarchical\nrepresentations of a bottom-up discriminative network. Our model consists of a\ntop-down stack of GANs, each learned to generate lower-level representations\nconditioned on higher-level representations. A representation discriminator is\nintroduced at each feature hierarchy to encourage the representation manifold\nof the generator to align with that of the bottom-up discriminative network,\nleveraging the powerful discriminative representations to guide the generative\nmodel. In addition, we introduce a conditional loss that encourages the use of\nconditional information from the layer above, and a novel entropy loss that\nmaximizes a variational lower bound on the conditional entropy of generator\noutputs. We first train each stack independently, and then train the whole\nmodel end-to-end. Unlike the original GAN that uses a single noise vector to\nrepresent all the variations, our SGAN decomposes variations into multiple\nlevels and gradually resolves uncertainties in the top-down generative process.\nBased on visual inspection, Inception scores and visual Turing test, we\ndemonstrate that SGAN is able to generate images of much higher quality than\nGANs without stacking.", "authors": ["Xun Huang", "Yixuan Li", "Omid Poursaeed", "John Hopcroft", "Serge Belongie"], "category": "cs.CV", "comment": "CVPR 2017, camera-ready version", "img": "/static/thumbs/1612.04357v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1612.04357v4", "num_discussion": 0, "originally_published_time": "12/13/2016", "pid": "1612.04357v4", "published_time": "4/12/2017", "rawpid": "1612.04357", "tags": ["cs.CV", "cs.LG", "cs.NE", "stat.ML"], "title": "Stacked Generative Adversarial Networks"}, {"abstract": "We present an interpretation of Inception modules in convolutional neural\nnetworks as being an intermediate step in-between regular convolution and the\ndepthwise separable convolution operation (a depthwise convolution followed by\na pointwise convolution). In this light, a depthwise separable convolution can\nbe understood as an Inception module with a maximally large number of towers.\nThis observation leads us to propose a novel deep convolutional neural network\narchitecture inspired by Inception, where Inception modules have been replaced\nwith depthwise separable convolutions. We show that this architecture, dubbed\nXception, slightly outperforms Inception V3 on the ImageNet dataset (which\nInception V3 was designed for), and significantly outperforms Inception V3 on a\nlarger image classification dataset comprising 350 million images and 17,000\nclasses. Since the Xception architecture has the same number of parameters as\nInception V3, the performance gains are not due to increased capacity but\nrather to a more efficient use of model parameters.", "authors": ["Fran\u00e7ois Chollet"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1610.02357v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1610.02357v3", "num_discussion": 0, "originally_published_time": "10/7/2016", "pid": "1610.02357v3", "published_time": "4/4/2017", "rawpid": "1610.02357", "tags": ["cs.CV"], "title": "Xception: Deep Learning with Depthwise Separable Convolutions"}, {"abstract": "We show how the success of deep learning could depend not only on mathematics\nbut also on physics: although well-known mathematical theorems guarantee that\nneural networks can approximate arbitrary functions well, the class of\nfunctions of practical interest can frequently be approximated through \"cheap\nlearning\" with exponentially fewer parameters than generic ones. We explore how\nproperties frequently encountered in physics such as symmetry, locality,\ncompositionality, and polynomial log-probability translate into exceptionally\nsimple neural networks. We further argue that when the statistical process\ngenerating the data is of a certain hierarchical form prevalent in physics and\nmachine-learning, a deep neural network can be more efficient than a shallow\none. We formalize these claims using information theory and discuss the\nrelation to the renormalization group. We prove various \"no-flattening\ntheorems\" showing when efficient linear deep networks cannot be accurately\napproximated by shallow ones without efficiency loss, for example, we show that\n$n$ variables cannot be multiplied using fewer than 2^n neurons in a single\nhidden layer.", "authors": ["Henry W. Lin", "Max Tegmark", "David Rolnick"], "category": "cond-mat.dis-nn", "comment": "Provided shorter proof that n variables cannot be multiplied using\n  fewer than 2^n neurons in a sin...", "img": "/static/thumbs/1608.08225v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1608.08225v3", "num_discussion": 0, "originally_published_time": "8/29/2016", "pid": "1608.08225v3", "published_time": "5/2/2017", "rawpid": "1608.08225", "tags": ["cond-mat.dis-nn", "cs.LG", "cs.NE", "stat.ML"], "title": "Why does deep and cheap learning work so well?"}, {"abstract": "Synthesizing photo-realistic images from text descriptions is a challenging\nproblem in computer vision and has many practical applications. Samples\ngenerated by existing text-to-image approaches can roughly reflect the meaning\nof the given descriptions, but they fail to contain necessary details and vivid\nobject parts. In this paper, we propose stacked Generative Adversarial Networks\n(StackGAN) to generate photo-realistic images conditioned on text descriptions.\nThe Stage-I GAN sketches the primitive shape and basic colors of the object\nbased on the given text description, yielding Stage-I low resolution images.\nThe Stage-II GAN takes Stage-I results and text descriptions as inputs, and\ngenerates high resolution images with photo-realistic details. The Stage-II GAN\nis able to rectify defects and add compelling details with the refinement\nprocess. Samples generated by StackGAN are more plausible than those generated\nby existing approaches. Importantly, our StackGAN for the first time generates\nrealistic 256 x 256 images conditioned on only text descriptions, while\nstate-of-the-art methods can generate at most 128 x 128 images. To demonstrate\nthe effectiveness of the proposed StackGAN, extensive experiments are conducted\non CUB and Oxford-102 datasets, which contain enough object appearance\nvariations and are widely-used for text-to-image generation analysis.", "authors": ["Han Zhang", "Tao Xu", "Hongsheng Li", "Shaoting Zhang", "Xiaolei Huang", "Xiaogang Wang", "Dimitris Metaxas"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1612.03242v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1612.03242v1", "num_discussion": 0, "originally_published_time": "12/10/2016", "pid": "1612.03242v1", "published_time": "12/10/2016", "rawpid": "1612.03242", "tags": ["cs.CV", "cs.AI", "stat.ML"], "title": "StackGAN: Text to Photo-realistic Image Synthesis with Stacked\n  Generative Adversarial Networks"}, {"abstract": "A long-term goal of machine learning research is to build an intelligent\ndialog agent. Most research in natural language understanding has focused on\nlearning from fixed training sets of labeled data, with supervision either at\nthe word level (tagging, parsing tasks) or sentence level (question answering,\nmachine translation). This kind of supervision is not realistic of how humans\nlearn, where language is both learned by, and used for, communication. In this\nwork, we study dialog-based language learning, where supervision is given\nnaturally and implicitly in the response of the dialog partner during the\nconversation. We study this setup in two domains: the bAbI dataset of (Weston\net al., 2015) and large-scale question answering from (Dodge et al., 2015). We\nevaluate a set of baseline learning strategies on these tasks, and show that a\nnovel model incorporating predictive lookahead is a promising approach for\nlearning from a teacher\u0027s response. In particular, a surprising result is that\nit can learn to answer questions correctly without any reward-based supervision\nat all.", "authors": ["Jason Weston"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1604.06045v7.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1604.06045v7", "num_discussion": 0, "originally_published_time": "4/20/2016", "pid": "1604.06045v7", "published_time": "10/24/2016", "rawpid": "1604.06045", "tags": ["cs.CL"], "title": "Dialog-based Language Learning"}, {"abstract": "In fine art, especially painting, humans have mastered the skill to create\nunique visual experiences through composing a complex interplay between the\ncontent and style of an image. Thus far the algorithmic basis of this process\nis unknown and there exists no artificial system with similar capabilities.\nHowever, in other key areas of visual perception such as object and face\nrecognition near-human performance was recently demonstrated by a class of\nbiologically inspired vision models called Deep Neural Networks. Here we\nintroduce an artificial system based on a Deep Neural Network that creates\nartistic images of high perceptual quality. The system uses neural\nrepresentations to separate and recombine content and style of arbitrary\nimages, providing a neural algorithm for the creation of artistic images.\nMoreover, in light of the striking similarities between performance-optimised\nartificial neural networks and biological vision, our work offers a path\nforward to an algorithmic understanding of how humans create and perceive\nartistic imagery.", "authors": ["Leon A. Gatys", "Alexander S. Ecker", "Matthias Bethge"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1508.06576v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1508.06576v2", "num_discussion": 0, "originally_published_time": "8/26/2015", "pid": "1508.06576v2", "published_time": "9/2/2015", "rawpid": "1508.06576", "tags": ["cs.CV", "cs.NE", "q-bio.NC"], "title": "A Neural Algorithm of Artistic Style"}, {"abstract": "This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural\nnetwork architecture for image generation. DRAW networks combine a novel\nspatial attention mechanism that mimics the foveation of the human eye, with a\nsequential variational auto-encoding framework that allows for the iterative\nconstruction of complex images. The system substantially improves on the state\nof the art for generative models on MNIST, and, when trained on the Street View\nHouse Numbers dataset, it generates images that cannot be distinguished from\nreal data with the naked eye.", "authors": ["Karol Gregor", "Ivo Danihelka", "Alex Graves", "Danilo Jimenez Rezende", "Daan Wierstra"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1502.04623v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1502.04623v2", "num_discussion": 0, "originally_published_time": "2/16/2015", "pid": "1502.04623v2", "published_time": "5/20/2015", "rawpid": "1502.04623", "tags": ["cs.CV", "cs.LG", "cs.NE"], "title": "DRAW: A Recurrent Neural Network For Image Generation"}, {"abstract": "With a goal of understanding what drives generalization in deep networks, we\nconsider several recently suggested explanations, including norm-based control,\nsharpness and robustness. We study how these measures can ensure\ngeneralization, highlighting the importance of scale normalization, and making\na connection between sharpness and PAC-Bayes theory. We then investigate how\nwell the measures explain different observed phenomena.", "authors": ["Behnam Neyshabur", "Srinadh Bhojanapalli", "David McAllester", "Nathan Srebro"], "category": "cs.LG", "comment": "19 pages, 8 figures", "img": "/static/thumbs/1706.08947v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1706.08947v2", "num_discussion": 0, "originally_published_time": "6/27/2017", "pid": "1706.08947v2", "published_time": "7/6/2017", "rawpid": "1706.08947", "tags": ["cs.LG"], "title": "Exploring Generalization in Deep Learning"}, {"abstract": "This work explores conditional image generation with a new image density\nmodel based on the PixelCNN architecture. The model can be conditioned on any\nvector, including descriptive labels or tags, or latent embeddings created by\nother networks. When conditioned on class labels from the ImageNet database,\nthe model is able to generate diverse, realistic scenes representing distinct\nanimals, objects, landscapes and structures. When conditioned on an embedding\nproduced by a convolutional network given a single image of an unseen face, it\ngenerates a variety of new portraits of the same person with different facial\nexpressions, poses and lighting conditions. We also show that conditional\nPixelCNN can serve as a powerful decoder in an image autoencoder. Additionally,\nthe gated convolutional layers in the proposed model improve the log-likelihood\nof PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet,\nwith greatly reduced computational cost.", "authors": ["Aaron van den Oord", "Nal Kalchbrenner", "Oriol Vinyals", "Lasse Espeholt", "Alex Graves", "Koray Kavukcuoglu"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1606.05328v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1606.05328v2", "num_discussion": 0, "originally_published_time": "6/16/2016", "pid": "1606.05328v2", "published_time": "6/18/2016", "rawpid": "1606.05328", "tags": ["cs.CV", "cs.LG"], "title": "Conditional Image Generation with PixelCNN Decoders"}, {"abstract": "We present a novel and practical deep fully convolutional neural network\narchitecture for semantic pixel-wise segmentation termed SegNet. This core\ntrainable segmentation engine consists of an encoder network, a corresponding\ndecoder network followed by a pixel-wise classification layer. The architecture\nof the encoder network is topologically identical to the 13 convolutional\nlayers in the VGG16 network. The role of the decoder network is to map the low\nresolution encoder feature maps to full input resolution feature maps for\npixel-wise classification. The novelty of SegNet lies is in the manner in which\nthe decoder upsamples its lower resolution input feature map(s). Specifically,\nthe decoder uses pooling indices computed in the max-pooling step of the\ncorresponding encoder to perform non-linear upsampling. This eliminates the\nneed for learning to upsample. The upsampled maps are sparse and are then\nconvolved with trainable filters to produce dense feature maps. We compare our\nproposed architecture with the widely adopted FCN and also with the well known\nDeepLab-LargeFOV, DeconvNet architectures. This comparison reveals the memory\nversus accuracy trade-off involved in achieving good segmentation performance.\n  SegNet was primarily motivated by scene understanding applications. Hence, it\nis designed to be efficient both in terms of memory and computational time\nduring inference. It is also significantly smaller in the number of trainable\nparameters than other competing architectures. We also performed a controlled\nbenchmark of SegNet and other architectures on both road scenes and SUN RGB-D\nindoor scene segmentation tasks. We show that SegNet provides good performance\nwith competitive inference time and more efficient inference memory-wise as\ncompared to other architectures. We also provide a Caffe implementation of\nSegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.", "authors": ["Vijay Badrinarayanan", "Alex Kendall", "Roberto Cipolla"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1511.00561v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1511.00561v3", "num_discussion": 0, "originally_published_time": "11/2/2015", "pid": "1511.00561v3", "published_time": "10/10/2016", "rawpid": "1511.00561", "tags": ["cs.CV", "cs.LG", "cs.NE"], "title": "SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image\n  Segmentation"}, {"abstract": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent\nperformance on difficult learning tasks. Although DNNs work well whenever large\nlabeled training sets are available, they cannot be used to map sequences to\nsequences. In this paper, we present a general end-to-end approach to sequence\nlearning that makes minimal assumptions on the sequence structure. Our method\nuses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to\na vector of a fixed dimensionality, and then another deep LSTM to decode the\ntarget sequence from the vector. Our main result is that on an English to\nFrench translation task from the WMT\u002714 dataset, the translations produced by\nthe LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM\u0027s\nBLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did\nnot have difficulty on long sentences. For comparison, a phrase-based SMT\nsystem achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM\nto rerank the 1000 hypotheses produced by the aforementioned SMT system, its\nBLEU score increases to 36.5, which is close to the previous best result on\nthis task. The LSTM also learned sensible phrase and sentence representations\nthat are sensitive to word order and are relatively invariant to the active and\nthe passive voice. Finally, we found that reversing the order of the words in\nall source sentences (but not target sentences) improved the LSTM\u0027s performance\nmarkedly, because doing so introduced many short term dependencies between the\nsource and the target sentence which made the optimization problem easier.", "authors": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "category": "cs.CL", "comment": "9 pages", "img": "/static/thumbs/1409.3215v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1409.3215v3", "num_discussion": 0, "originally_published_time": "9/10/2014", "pid": "1409.3215v3", "published_time": "12/14/2014", "rawpid": "1409.3215", "tags": ["cs.CL", "cs.LG"], "title": "Sequence to Sequence Learning with Neural Networks"}, {"abstract": "We introduce an extremely computation efficient CNN architecture named\nShuffleNet, designed specially for mobile devices with very limited computing\npower (e.g., 10-150 MFLOPs). The new architecture utilizes two proposed\noperations, pointwise group convolution and channel shuffle, to greatly reduce\ncomputation cost while maintaining accuracy. Experiments on ImageNet\nclassification and MS COCO object detection demonstrate the superior\nperformance of ShuffleNet over other structures, e.g. lower top-1 error\n(absolute 6.7\\%) than the recent MobileNet system on ImageNet classification\nunder the computation budget of 40 MFLOPs. On an ARM-based mobile device,\nShuffleNet achieves \\textasciitilde 13$\\times$ actual speedup over AlexNet\nwhile maintaining comparable accuracy.", "authors": ["Xiangyu Zhang", "Xinyu Zhou", "Mengxiao Lin", "Jian Sun"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1707.01083v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.01083v1", "num_discussion": 0, "originally_published_time": "7/4/2017", "pid": "1707.01083v1", "published_time": "7/4/2017", "rawpid": "1707.01083", "tags": ["cs.CV"], "title": "ShuffleNet: An Extremely Efficient Convolutional Neural Network for\n  Mobile Devices"}, {"abstract": "Convolutional neural networks (CNNs) are inherently limited to model\ngeometric transformations due to the fixed geometric structures in its building\nmodules. In this work, we introduce two new modules to enhance the\ntransformation modeling capacity of CNNs, namely, deformable convolution and\ndeformable RoI pooling. Both are based on the idea of augmenting the spatial\nsampling locations in the modules with additional offsets and learning the\noffsets from target tasks, without additional supervision. The new modules can\nreadily replace their plain counterparts in existing CNNs and can be easily\ntrained end-to-end by standard back-propagation, giving rise to deformable\nconvolutional networks. Extensive experiments validate the effectiveness of our\napproach on sophisticated vision tasks of object detection and semantic\nsegmentation. The code would be released.", "authors": ["Jifeng Dai", "Haozhi Qi", "Yuwen Xiong", "Yi Li", "Guodong Zhang", "Han Hu", "Yichen Wei"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1703.06211v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.06211v3", "num_discussion": 0, "originally_published_time": "3/17/2017", "pid": "1703.06211v3", "published_time": "6/5/2017", "rawpid": "1703.06211", "tags": ["cs.CV"], "title": "Deformable Convolutional Networks"}, {"abstract": "Recent research in the deep learning field has produced a plethora of new\narchitectures. At the same time, a growing number of groups are applying deep\nlearning to new applications. Some of these groups are likely to be composed of\ninexperienced deep learning practitioners who are baffled by the dizzying array\nof architecture choices and therefore opt to use an older architecture (i.e.,\nAlexnet). Here we attempt to bridge this gap by mining the collective knowledge\ncontained in recent deep learning research to discover underlying principles\nfor designing neural network architectures. In addition, we describe several\narchitectural innovations, including Fractal of FractalNet network, Stagewise\nBoosting Networks, and Taylor Series Networks (our Caffe code and prototxt\nfiles is available at https://github.com/iPhysicist/CNNDesignPatterns). We hope\nothers are inspired to build on our preliminary work.", "authors": ["Leslie N. Smith", "Nicholay Topin"], "category": "cs.LG", "comment": "Submitted as a conference paper at ICLR 2017", "img": "/static/thumbs/1611.00847v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1611.00847v3", "num_discussion": 0, "originally_published_time": "11/2/2016", "pid": "1611.00847v3", "published_time": "11/14/2016", "rawpid": "1611.00847", "tags": ["cs.LG", "cs.CV", "cs.NE"], "title": "Deep Convolutional Neural Network Design Patterns"}, {"abstract": "We propose zoneout, a novel method for regularizing RNNs. At each timestep,\nzoneout stochastically forces some hidden units to maintain their previous\nvalues. Like dropout, zoneout uses random noise to train a pseudo-ensemble,\nimproving generalization. But by preserving instead of dropping hidden units,\ngradient information and state information are more readily propagated through\ntime, as in feedforward stochastic depth networks. We perform an empirical\ninvestigation of various RNN regularizers, and find that zoneout gives\nsignificant performance improvements across tasks. We achieve competitive\nresults with relatively simple models in character- and word-level language\nmodelling on the Penn Treebank and Text8 datasets, and combining with recurrent\nbatch normalization yields state-of-the-art results on permuted sequential\nMNIST.", "authors": ["David Krueger", "Tegan Maharaj", "J\u00e1nos Kram\u00e1r", "Mohammad Pezeshki", "Nicolas Ballas", "Nan Rosemary Ke", "Anirudh Goyal", "Yoshua Bengio", "Aaron Courville", "Chris Pal"], "category": "cs.NE", "comment": "", "img": "/static/thumbs/1606.01305v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1606.01305v3", "num_discussion": 0, "originally_published_time": "6/3/2016", "pid": "1606.01305v3", "published_time": "1/18/2017", "rawpid": "1606.01305", "tags": ["cs.NE", "cs.CL", "cs.LG"], "title": "Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations"}, {"abstract": "We introduce the dense captioning task, which requires a computer vision\nsystem to both localize and describe salient regions in images in natural\nlanguage. The dense captioning task generalizes object detection when the\ndescriptions consist of a single word, and Image Captioning when one predicted\nregion covers the full image. To address the localization and description task\njointly we propose a Fully Convolutional Localization Network (FCLN)\narchitecture that processes an image with a single, efficient forward pass,\nrequires no external regions proposals, and can be trained end-to-end with a\nsingle round of optimization. The architecture is composed of a Convolutional\nNetwork, a novel dense localization layer, and Recurrent Neural Network\nlanguage model that generates the label sequences. We evaluate our network on\nthe Visual Genome dataset, which comprises 94,000 images and 4,100,000\nregion-grounded captions. We observe both speed and accuracy improvements over\nbaselines based on current state of the art approaches in both generation and\nretrieval settings.", "authors": ["Justin Johnson", "Andrej Karpathy", "Li Fei-Fei"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1511.07571v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1511.07571v1", "num_discussion": 0, "originally_published_time": "11/24/2015", "pid": "1511.07571v1", "published_time": "11/24/2015", "rawpid": "1511.07571", "tags": ["cs.CV", "cs.LG"], "title": "DenseCap: Fully Convolutional Localization Networks for Dense Captioning"}, {"abstract": "We propose a general modeling and inference framework that composes\nprobabilistic graphical models with deep learning methods and combines their\nrespective strengths. Our model family augments graphical structure in latent\nvariables with neural network observation models. For inference, we extend\nvariational autoencoders to use graphical model approximating distributions\nwith recognition networks that output conjugate potentials. All components of\nthese models are learned simultaneously with a single objective, giving a\nscalable algorithm that leverages stochastic variational inference, natural\ngradients, graphical model message passing, and the reparameterization trick.\nWe illustrate this framework with several example models and an application to\nmouse behavioral phenotyping.", "authors": ["Matthew J. Johnson", "David Duvenaud", "Alexander B. Wiltschko", "Sandeep R. Datta", "Ryan P. Adams"], "category": "stat.ML", "comment": "v5 fixes tex compilation bugs and also a math bug in the statement\n  and proof of Prop. 4.1 (and D.3...", "img": "/static/thumbs/1603.06277v5.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1603.06277v5", "num_discussion": 0, "originally_published_time": "3/20/2016", "pid": "1603.06277v5", "published_time": "7/7/2017", "rawpid": "1603.06277", "tags": ["stat.ML"], "title": "Composing graphical models with neural networks for structured\n  representations and fast inference"}, {"abstract": "In many real-world scenarios, rewards extrinsic to the agent are extremely\nsparse, or absent altogether. In such cases, curiosity can serve as an\nintrinsic reward signal to enable the agent to explore its environment and\nlearn skills that might be useful later in its life. We formulate curiosity as\nthe error in an agent\u0027s ability to predict the consequence of its own actions\nin a visual feature space learned by a self-supervised inverse dynamics model.\nOur formulation scales to high-dimensional continuous state spaces like images,\nbypasses the difficulties of directly predicting pixels, and, critically,\nignores the aspects of the environment that cannot affect the agent. The\nproposed approach is evaluated in two environments: VizDoom and Super Mario\nBros. Three broad settings are investigated: 1) sparse extrinsic reward, where\ncuriosity allows for far fewer interactions with the environment to reach the\ngoal; 2) exploration with no extrinsic reward, where curiosity pushes the agent\nto explore more efficiently; and 3) generalization to unseen scenarios (e.g.\nnew levels of the same game) where the knowledge gained from earlier experience\nhelps the agent explore new places much faster than starting from scratch. Demo\nvideo and code available at https://pathak22.github.io/noreward-rl/", "authors": ["Deepak Pathak", "Pulkit Agrawal", "Alexei A. Efros", "Trevor Darrell"], "category": "cs.LG", "comment": "In ICML 2017. Website at https://pathak22.github.io/noreward-rl/", "img": "/static/thumbs/1705.05363v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1705.05363v1", "num_discussion": 0, "originally_published_time": "5/15/2017", "pid": "1705.05363v1", "published_time": "5/15/2017", "rawpid": "1705.05363", "tags": ["cs.LG", "cs.AI", "cs.CV", "cs.RO", "stat.ML"], "title": "Curiosity-driven Exploration by Self-supervised Prediction"}, {"abstract": "We present a novel neural network for processing sequences. The ByteNet is a\none-dimensional convolutional neural network that is composed of two parts, one\nto encode the source sequence and the other to decode the target sequence. The\ntwo network parts are connected by stacking the decoder on top of the encoder\nand preserving the temporal resolution of the sequences. To address the\ndiffering lengths of the source and the target, we introduce an efficient\nmechanism by which the decoder is dynamically unfolded over the representation\nof the encoder. The ByteNet uses dilation in the convolutional layers to\nincrease its receptive field. The resulting network has two core properties: it\nruns in time that is linear in the length of the sequences and it sidesteps the\nneed for excessive memorization. The ByteNet decoder attains state-of-the-art\nperformance on character-level language modelling and outperforms the previous\nbest results obtained with recurrent networks. The ByteNet also achieves\nstate-of-the-art performance on character-to-character machine translation on\nthe English-to-German WMT translation task, surpassing comparable neural\ntranslation models that are based on recurrent networks with attentional\npooling and run in quadratic time. We find that the latent alignment structure\ncontained in the representations reflects the expected alignment between the\ntokens.", "authors": ["Nal Kalchbrenner", "Lasse Espeholt", "Karen Simonyan", "Aaron van den Oord", "Alex Graves", "Koray Kavukcuoglu"], "category": "cs.CL", "comment": "9 pages", "img": "/static/thumbs/1610.10099v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1610.10099v2", "num_discussion": 0, "originally_published_time": "10/31/2016", "pid": "1610.10099v2", "published_time": "3/15/2017", "rawpid": "1610.10099", "tags": ["cs.CL", "cs.LG"], "title": "Neural Machine Translation in Linear Time"}, {"abstract": "In recent years there have been many successes of using deep representations\nin reinforcement learning. Still, many of these applications use conventional\narchitectures, such as convolutional networks, LSTMs, or auto-encoders. In this\npaper, we present a new neural network architecture for model-free\nreinforcement learning. Our dueling network represents two separate estimators:\none for the state value function and one for the state-dependent action\nadvantage function. The main benefit of this factoring is to generalize\nlearning across actions without imposing any change to the underlying\nreinforcement learning algorithm. Our results show that this architecture leads\nto better policy evaluation in the presence of many similar-valued actions.\nMoreover, the dueling architecture enables our RL agent to outperform the\nstate-of-the-art on the Atari 2600 domain.", "authors": ["Ziyu Wang", "Tom Schaul", "Matteo Hessel", "Hado van Hasselt", "Marc Lanctot", "Nando de Freitas"], "category": "cs.LG", "comment": "15 pages, 5 figures, and 5 tables", "img": "/static/thumbs/1511.06581v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1511.06581v3", "num_discussion": 0, "originally_published_time": "11/20/2015", "pid": "1511.06581v3", "published_time": "4/5/2016", "rawpid": "1511.06581", "tags": ["cs.LG"], "title": "Dueling Network Architectures for Deep Reinforcement Learning"}, {"abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization\nof stochastic objective functions, based on adaptive estimates of lower-order\nmoments. The method is straightforward to implement, is computationally\nefficient, has little memory requirements, is invariant to diagonal rescaling\nof the gradients, and is well suited for problems that are large in terms of\ndata and/or parameters. The method is also appropriate for non-stationary\nobjectives and problems with very noisy and/or sparse gradients. The\nhyper-parameters have intuitive interpretations and typically require little\ntuning. Some connections to related algorithms, on which Adam was inspired, are\ndiscussed. We also analyze the theoretical convergence properties of the\nalgorithm and provide a regret bound on the convergence rate that is comparable\nto the best known results under the online convex optimization framework.\nEmpirical results demonstrate that Adam works well in practice and compares\nfavorably to other stochastic optimization methods. Finally, we discuss AdaMax,\na variant of Adam based on the infinity norm.", "authors": ["Diederik P. Kingma", "Jimmy Ba"], "category": "cs.LG", "comment": "Published as a conference paper at the 3rd International Conference\n  for Learning Representations, ...", "img": "/static/thumbs/1412.6980v9.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1412.6980v9", "num_discussion": 0, "originally_published_time": "12/22/2014", "pid": "1412.6980v9", "published_time": "1/30/2017", "rawpid": "1412.6980", "tags": ["cs.LG"], "title": "Adam: A Method for Stochastic Optimization"}, {"abstract": "Gradient descent optimization algorithms, while increasingly popular, are\noften used as black-box optimizers, as practical explanations of their\nstrengths and weaknesses are hard to come by. This article aims to provide the\nreader with intuitions with regard to the behaviour of different algorithms\nthat will allow her to put them to use. In the course of this overview, we look\nat different variants of gradient descent, summarize challenges, introduce the\nmost common optimization algorithms, review architectures in a parallel and\ndistributed setting, and investigate additional strategies for optimizing\ngradient descent.", "authors": ["Sebastian Ruder"], "category": "cs.LG", "comment": "Added derivations of AdaMax and Nadam", "img": "/static/thumbs/1609.04747v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1609.04747v2", "num_discussion": 0, "originally_published_time": "9/15/2016", "pid": "1609.04747v2", "published_time": "6/15/2017", "rawpid": "1609.04747", "tags": ["cs.LG"], "title": "An overview of gradient descent optimization algorithms"}, {"abstract": "We propose a deep convolutional neural network architecture codenamed\n\"Inception\", which was responsible for setting the new state of the art for\nclassification and detection in the ImageNet Large-Scale Visual Recognition\nChallenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the\nimproved utilization of the computing resources inside the network. This was\nachieved by a carefully crafted design that allows for increasing the depth and\nwidth of the network while keeping the computational budget constant. To\noptimize quality, the architectural decisions were based on the Hebbian\nprinciple and the intuition of multi-scale processing. One particular\nincarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22\nlayers deep network, the quality of which is assessed in the context of\nclassification and detection.", "authors": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1409.4842v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1409.4842v1", "num_discussion": 0, "originally_published_time": "9/17/2014", "pid": "1409.4842v1", "published_time": "9/17/2014", "rawpid": "1409.4842", "tags": ["cs.CV"], "title": "Going Deeper with Convolutions"}, {"abstract": "We introduce a method to stabilize Generative Adversarial Networks (GANs) by\ndefining the generator objective with respect to an unrolled optimization of\nthe discriminator. This allows training to be adjusted between using the\noptimal discriminator in the generator\u0027s objective, which is ideal but\ninfeasible in practice, and using the current value of the discriminator, which\nis often unstable and leads to poor solutions. We show how this technique\nsolves the common problem of mode collapse, stabilizes training of GANs with\ncomplex recurrent generators, and increases diversity and coverage of the data\ndistribution by the generator.", "authors": ["Luke Metz", "Ben Poole", "David Pfau", "Jascha Sohl-Dickstein"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1611.02163v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1611.02163v4", "num_discussion": 0, "originally_published_time": "11/7/2016", "pid": "1611.02163v4", "published_time": "5/12/2017", "rawpid": "1611.02163", "tags": ["cs.LG", "stat.ML"], "title": "Unrolled Generative Adversarial Networks"}, {"abstract": "Generative adversarial networks (GANs) can implicitly learn rich\ndistributions over images, audio, and data which are hard to model with an\nexplicit likelihood. We present a practical Bayesian formulation for\nunsupervised and semi-supervised learning with GANs. Within this framework, we\nuse stochastic gradient Hamiltonian Monte Carlo to marginalize the weights of\nthe generator and discriminator networks. The resulting approach is\nstraightforward and obtains good performance without any standard interventions\nsuch as feature matching, or mini-batch discrimination. By exploring an\nexpressive posterior over the parameters of the generator, the Bayesian GAN\navoids mode-collapse, produces interpretable and diverse candidate samples, and\nprovides state-of-the-art quantitative results for semi-supervised learning on\nbenchmarks including SVHN, CelebA, and CIFAR-10, outperforming DCGAN,\nWasserstein GANs, and DCGAN ensembles.", "authors": ["Yunus Saatchi", "Andrew Gordon Wilson"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1705.09558v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1705.09558v2", "num_discussion": 0, "originally_published_time": "5/26/2017", "pid": "1705.09558v2", "published_time": "5/29/2017", "rawpid": "1705.09558", "tags": ["stat.ML", "cs.AI", "cs.CV", "cs.LG"], "title": "Bayesian GAN"}, {"abstract": "In this work we explore a straightforward variational Bayes scheme for\nRecurrent Neural Networks. Firstly, we show that a simple adaptation of\ntruncated backpropagation through time can yield good quality uncertainty\nestimates and superior regularisation at only a small extra computational cost\nduring training. Secondly, we demonstrate how a novel kind of posterior\napproximation yields further improvements to the performance of Bayesian RNNs.\nWe incorporate local gradient information into the approximate posterior to\nsharpen it around the current batch statistics. This technique is not exclusive\nto recurrent neural networks and can be applied more widely to train Bayesian\nneural networks. We also empirically demonstrate how Bayesian RNNs are superior\nto traditional RNNs on a language modelling benchmark and an image captioning\ntask, as well as showing how each of these methods improve our model over a\nvariety of other schemes for training them. We also introduce a new benchmark\nfor studying uncertainty for language models so future methods can be easily\ncompared.", "authors": ["Meire Fortunato", "Charles Blundell", "Oriol Vinyals"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1704.02798v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.02798v2", "num_discussion": 0, "originally_published_time": "4/10/2017", "pid": "1704.02798v2", "published_time": "4/11/2017", "rawpid": "1704.02798", "tags": ["cs.LG", "stat.ML"], "title": "Bayesian Recurrent Neural Networks"}, {"abstract": "We introduce a method to train Quantized Neural Networks (QNNs) --- neural\nnetworks with extremely low precision (e.g., 1-bit) weights and activations, at\nrun-time. At train-time the quantized weights and activations are used for\ncomputing the parameter gradients. During the forward pass, QNNs drastically\nreduce memory size and accesses, and replace most arithmetic operations with\nbit-wise operations. As a result, power consumption is expected to be\ndrastically reduced. We trained QNNs over the MNIST, CIFAR-10, SVHN and\nImageNet datasets. The resulting QNNs achieve prediction accuracy comparable to\ntheir 32-bit counterparts. For example, our quantized version of AlexNet with\n1-bit weights and 2-bit activations achieves $51\\%$ top-1 accuracy. Moreover,\nwe quantize the parameter gradients to 6-bits as well which enables gradients\ncomputation using only bit-wise operation. Quantized recurrent neural networks\nwere tested over the Penn Treebank dataset, and achieved comparable accuracy as\ntheir 32-bit counterparts using only 4-bits. Last but not least, we programmed\na binary matrix multiplication GPU kernel with which it is possible to run our\nMNIST QNN 7 times faster than with an unoptimized GPU kernel, without suffering\nany loss in classification accuracy. The QNN code is available online.", "authors": ["Itay Hubara", "Matthieu Courbariaux", "Daniel Soudry", "Ran El-Yaniv", "Yoshua Bengio"], "category": "cs.NE", "comment": "arXiv admin note: text overlap with arXiv:1602.02830", "img": "/static/thumbs/1609.07061v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1609.07061v1", "num_discussion": 0, "originally_published_time": "9/22/2016", "pid": "1609.07061v1", "published_time": "9/22/2016", "rawpid": "1609.07061", "tags": ["cs.NE", "cs.LG"], "title": "Quantized Neural Networks: Training Neural Networks with Low Precision\n  Weights and Activations"}, {"abstract": "Most existing machine learning classifiers are highly vulnerable to\nadversarial examples. An adversarial example is a sample of input data which\nhas been modified very slightly in a way that is intended to cause a machine\nlearning classifier to misclassify it. In many cases, these modifications can\nbe so subtle that a human observer does not even notice the modification at\nall, yet the classifier still makes a mistake. Adversarial examples pose\nsecurity concerns because they could be used to perform an attack on machine\nlearning systems, even if the adversary has no access to the underlying model.\nUp to now, all previous work have assumed a threat model in which the adversary\ncan feed data directly into the machine learning classifier. This is not always\nthe case for systems operating in the physical world, for example those which\nare using signals from cameras and other sensors as an input. This paper shows\nthat even in such physical world scenarios, machine learning systems are\nvulnerable to adversarial examples. We demonstrate this by feeding adversarial\nimages obtained from cell-phone camera to an ImageNet Inception classifier and\nmeasuring the classification accuracy of the system. We find that a large\nfraction of adversarial examples are classified incorrectly even when perceived\nthrough the camera.", "authors": ["Alexey Kurakin", "Ian Goodfellow", "Samy Bengio"], "category": "cs.CV", "comment": "14 pages, 6 figures. Demo available at https://youtu.be/zQ_uMenoBCk", "img": "/static/thumbs/1607.02533v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1607.02533v4", "num_discussion": 0, "originally_published_time": "7/8/2016", "pid": "1607.02533v4", "published_time": "2/11/2017", "rawpid": "1607.02533", "tags": ["cs.CV", "cs.CR", "cs.LG", "stat.ML"], "title": "Adversarial examples in the physical world"}, {"abstract": "The paper characterizes classes of functions for which deep learning can be\nexponentially better than shallow learning. Deep convolutional networks are a\nspecial case of these conditions, though weight sharing is not the main reason\nfor their exponential advantage.", "authors": ["Tomaso Poggio", "Hrushikesh Mhaskar", "Lorenzo Rosasco", "Brando Miranda", "Qianli Liao"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1611.00740v5.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1611.00740v5", "num_discussion": 0, "originally_published_time": "11/2/2016", "pid": "1611.00740v5", "published_time": "2/4/2017", "rawpid": "1611.00740", "tags": ["cs.LG"], "title": "Why and When Can Deep -- but Not Shallow -- Networks Avoid the Curse of\n  Dimensionality: a Review"}, {"abstract": "Major winning Convolutional Neural Networks (CNNs), such as AlexNet, VGGNet,\nResNet, GoogleNet, include tens to hundreds of millions of parameters, which\nimpose considerable computation and memory overhead. This limits their\npractical use for training, optimization and memory efficiency. On the\ncontrary, light-weight architectures, being proposed to address this issue,\nmainly suffer from low accuracy. These inefficiencies mostly stem from\nfollowing an ad hoc procedure. We propose a simple architecture, called\nSimpleNet, based on a set of designing principles and we empirically show that\nSimpleNet provides a good tradeoff between the computation/memory efficiency\nand the accuracy. Our simple 13-layer architecture outperforms most of the\ndeeper and complex architectures to date such as VGGNet, ResNet, and GoogleNet\non several well-known benchmarks while having 2 to 25 times fewer number of\nparameters and operations. This makes it very handy for embedded system or\nsystem with computational and memory limitations. We achieved state-of-the-art\nresult on standard data sets such as CIFAR10 outperforming several heavier\narchitectures including but not limited to AlexNet on ImageNet and very good\nresults on data sets such as CIFAR100, MNIST and SVHN. In our experiments we\nshow that SimpleNet is more efficient in terms of computation and memory\noverhead compared to state of the art. Models are made available at:\nhttps://github.com/Coderx7/SimpleNet", "authors": ["Seyyed Hossein Hasanpour", "Mohammad Rouhani", "Mohsen Fayyaz", "Mohammad Sabokrou"], "category": "cs.CV", "comment": "Third Draft: guideliness added, early imagenet results added, link to\n  models added", "img": "/static/thumbs/1608.06037v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1608.06037v4", "num_discussion": 0, "originally_published_time": "8/22/2016", "pid": "1608.06037v4", "published_time": "5/15/2017", "rawpid": "1608.06037", "tags": ["cs.CV", "cs.NE"], "title": "Lets keep it simple, Using simple architectures to outperform deeper and\n  more complex architectures"}, {"abstract": "In this work we propose a novel interpretation of residual networks showing\nthat they can be seen as a collection of many paths of differing length.\nMoreover, residual networks seem to enable very deep networks by leveraging\nonly the short paths during training. To support this observation, we rewrite\nresidual networks as an explicit collection of paths. Unlike traditional\nmodels, paths through residual networks vary in length. Further, a lesion study\nreveals that these paths show ensemble-like behavior in the sense that they do\nnot strongly depend on each other. Finally, and most surprising, most paths are\nshorter than one might expect, and only the short paths are needed during\ntraining, as longer paths do not contribute any gradient. For example, most of\nthe gradient in a residual network with 110 layers comes from paths that are\nonly 10-34 layers deep. Our results reveal one of the key characteristics that\nseem to enable the training of very deep networks: Residual networks avoid the\nvanishing gradient problem by introducing short paths which can carry gradient\nthroughout the extent of very deep networks.", "authors": ["Andreas Veit", "Michael Wilber", "Serge Belongie"], "category": "cs.CV", "comment": "NIPS 2016", "img": "/static/thumbs/1605.06431v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1605.06431v2", "num_discussion": 0, "originally_published_time": "5/20/2016", "pid": "1605.06431v2", "published_time": "10/27/2016", "rawpid": "1605.06431", "tags": ["cs.CV", "cs.AI", "cs.LG", "cs.NE"], "title": "Residual Networks Behave Like Ensembles of Relatively Shallow Networks"}, {"abstract": "Auto-encoding generative adversarial networks (GANs) combine the standard GAN\nalgorithm, which discriminates between real and model-generated data, with a\nreconstruction loss given by an auto-encoder. Such models aim to prevent mode\ncollapse in the learned generative model by ensuring that it is grounded in all\nthe available training data. In this paper, we develop a principle upon which\nauto-encoders can be combined with generative adversarial networks by\nexploiting the hierarchical structure of the generative model. The underlying\nprinciple shows that variational inference can be used a basic tool for\nlearning, but with the in- tractable likelihood replaced by a synthetic\nlikelihood, and the unknown posterior distribution replaced by an implicit\ndistribution; both synthetic likelihoods and implicit posterior distributions\ncan be learned using discriminators. This allows us to develop a natural fusion\nof variational auto-encoders and generative adversarial networks, combining the\nbest of both these methods. We describe a unified objective for optimization,\ndiscuss the constraints needed to guide learning, connect to the wide range of\nexisting work, and use a battery of tests to systematically and quantitatively\nassess the performance of our method.", "authors": ["Mihaela Rosca", "Balaji Lakshminarayanan", "David Warde-Farley", "Shakir Mohamed"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1706.04987v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1706.04987v1", "num_discussion": 0, "originally_published_time": "6/15/2017", "pid": "1706.04987v1", "published_time": "6/15/2017", "rawpid": "1706.04987", "tags": ["stat.ML", "cs.LG"], "title": "Variational Approaches for Auto-Encoding Generative Adversarial Networks"}, {"abstract": "Despite their great success, there is still no comprehensive theoretical\nunderstanding of learning with Deep Neural Networks (DNNs) or their inner\norganization. Previous work proposed to analyze DNNs in the \\textit{Information\nPlane}; i.e., the plane of the Mutual Information values that each layer\npreserves on the input and output variables. They suggested that the goal of\nthe network is to optimize the Information Bottleneck (IB) tradeoff between\ncompression and prediction, successively, for each layer.\n  In this work we follow up on this idea and demonstrate the effectiveness of\nthe Information-Plane visualization of DNNs. Our main results are: (i) most of\nthe training epochs in standard DL are spent on {\\emph compression} of the\ninput to efficient representation and not on fitting the training labels. (ii)\nThe representation compression phase begins when the training errors becomes\nsmall and the Stochastic Gradient Decent (SGD) epochs change from a fast drift\nto smaller training error into a stochastic relaxation, or random diffusion,\nconstrained by the training error value. (iii) The converged layers lie on or\nvery close to the Information Bottleneck (IB) theoretical bound, and the maps\nfrom the input to any hidden layer and from this hidden layer to the output\nsatisfy the IB self-consistent equations. This generalization through noise\nmechanism is unique to Deep Neural Networks and absent in one layer networks.\n(iv) The training time is dramatically reduced when adding more hidden layers.\nThus the main advantage of the hidden layers is computational. This can be\nexplained by the reduced relaxation time, as this it scales super-linearly\n(exponentially for simple diffusion) with the information compression from the\nprevious layer.", "authors": ["Ravid Shwartz-Ziv", "Naftali Tishby"], "category": "cs.LG", "comment": "19 pages, 8 figures", "img": "/static/thumbs/1703.00810v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.00810v3", "num_discussion": 2, "originally_published_time": "3/2/2017", "pid": "1703.00810v3", "published_time": "4/29/2017", "rawpid": "1703.00810", "tags": ["cs.LG"], "title": "Opening the Black Box of Deep Neural Networks via Information"}, {"abstract": "Neural networks are both computationally intensive and memory intensive,\nmaking them difficult to deploy on embedded systems with limited hardware\nresources. To address this limitation, we introduce \"deep compression\", a three\nstage pipeline: pruning, trained quantization and Huffman coding, that work\ntogether to reduce the storage requirement of neural networks by 35x to 49x\nwithout affecting their accuracy. Our method first prunes the network by\nlearning only the important connections. Next, we quantize the weights to\nenforce weight sharing, finally, we apply Huffman coding. After the first two\nsteps we retrain the network to fine tune the remaining connections and the\nquantized centroids. Pruning, reduces the number of connections by 9x to 13x;\nQuantization then reduces the number of bits that represent each connection\nfrom 32 to 5. On the ImageNet dataset, our method reduced the storage required\nby AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method\nreduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of\naccuracy. This allows fitting the model into on-chip SRAM cache rather than\noff-chip DRAM memory. Our compression method also facilitates the use of\ncomplex neural networks in mobile applications where application size and\ndownload bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU,\ncompressed network has 3x to 4x layerwise speedup and 3x to 7x better energy\nefficiency.", "authors": ["Song Han", "Huizi Mao", "William J. Dally"], "category": "cs.CV", "comment": "Published as a conference paper at ICLR 2016 (oral)", "img": "/static/thumbs/1510.00149v5.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1510.00149v5", "num_discussion": 0, "originally_published_time": "10/1/2015", "pid": "1510.00149v5", "published_time": "2/15/2016", "rawpid": "1510.00149", "tags": ["cs.CV", "cs.NE"], "title": "Deep Compression: Compressing Deep Neural Networks with Pruning, Trained\n  Quantization and Huffman Coding"}, {"abstract": "Deep learning tools have gained tremendous attention in applied machine\nlearning. However such tools for regression and classification do not capture\nmodel uncertainty. In comparison, Bayesian models offer a mathematically\ngrounded framework to reason about model uncertainty, but usually come with a\nprohibitive computational cost. In this paper we develop a new theoretical\nframework casting dropout training in deep neural networks (NNs) as approximate\nBayesian inference in deep Gaussian processes. A direct result of this theory\ngives us tools to model uncertainty with dropout NNs -- extracting information\nfrom existing models that has been thrown away so far. This mitigates the\nproblem of representing uncertainty in deep learning without sacrificing either\ncomputational complexity or test accuracy. We perform an extensive study of the\nproperties of dropout\u0027s uncertainty. Various network architectures and\nnon-linearities are assessed on tasks of regression and classification, using\nMNIST as an example. We show a considerable improvement in predictive\nlog-likelihood and RMSE compared to existing state-of-the-art methods, and\nfinish by using dropout\u0027s uncertainty in deep reinforcement learning.", "authors": ["Yarin Gal", "Zoubin Ghahramani"], "category": "stat.ML", "comment": "12 pages, 6 figures; fixed a mistake with standard error and added a\n  new table with updated result...", "img": "/static/thumbs/1506.02142v6.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1506.02142v6", "num_discussion": 0, "originally_published_time": "6/6/2015", "pid": "1506.02142v6", "published_time": "10/4/2016", "rawpid": "1506.02142", "tags": ["stat.ML", "cs.LG"], "title": "Dropout as a Bayesian Approximation: Representing Model Uncertainty in\n  Deep Learning"}, {"abstract": "This paper proposes a Fast Region-based Convolutional Network method (Fast\nR-CNN) for object detection. Fast R-CNN builds on previous work to efficiently\nclassify object proposals using deep convolutional networks. Compared to\nprevious work, Fast R-CNN employs several innovations to improve training and\ntesting speed while also increasing detection accuracy. Fast R-CNN trains the\nvery deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and\nachieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains\nVGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is\nimplemented in Python and C++ (using Caffe) and is available under the\nopen-source MIT License at https://github.com/rbgirshick/fast-rcnn.", "authors": ["Ross Girshick"], "category": "cs.CV", "comment": "To appear in ICCV 2015", "img": "/static/thumbs/1504.08083v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1504.08083v2", "num_discussion": 0, "originally_published_time": "4/30/2015", "pid": "1504.08083v2", "published_time": "9/27/2015", "rawpid": "1504.08083", "tags": ["cs.CV"], "title": "Fast R-CNN"}, {"abstract": "We propose an algorithm for meta-learning that is model-agnostic, in the\nsense that it is compatible with any model trained with gradient descent and\napplicable to a variety of different learning problems, including\nclassification, regression, and reinforcement learning. The goal of\nmeta-learning is to train a model on a variety of learning tasks, such that it\ncan solve new learning tasks using only a small number of training samples. In\nour approach, the parameters of the model are explicitly trained such that a\nsmall number of gradient steps with a small amount of training data from a new\ntask will produce good generalization performance on that task. In effect, our\nmethod trains the model to be easy to fine-tune. We demonstrate that this\napproach leads to state-of-the-art performance on two few-shot image\nclassification benchmarks, produces good results on few-shot regression, and\naccelerates fine-tuning for policy gradient reinforcement learning with neural\nnetwork policies.", "authors": ["Chelsea Finn", "Pieter Abbeel", "Sergey Levine"], "category": "cs.LG", "comment": "ICML 2017. Code at https://github.com/cbfinn/maml, Videos of RL\n  results at https://sites.google.co...", "img": "/static/thumbs/1703.03400v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.03400v3", "num_discussion": 1, "originally_published_time": "3/9/2017", "pid": "1703.03400v3", "published_time": "7/18/2017", "rawpid": "1703.03400", "tags": ["cs.LG", "cs.AI", "cs.CV", "cs.NE"], "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"}, {"abstract": "Despite recent advances, memory-augmented deep neural networks are still\nlimited when it comes to life-long and one-shot learning, especially in\nremembering rare events. We present a large-scale life-long memory module for\nuse in deep learning. The module exploits fast nearest-neighbor algorithms for\nefficiency and thus scales to large memory sizes. Except for the\nnearest-neighbor query, the module is fully differentiable and trained\nend-to-end with no extra supervision. It operates in a life-long manner, i.e.,\nwithout the need to reset it during training.\n  Our memory module can be easily added to any part of a supervised neural\nnetwork. To show its versatility we add it to a number of networks, from simple\nconvolutional ones tested on image classification to deep sequence-to-sequence\nand recurrent-convolutional models. In all cases, the enhanced network gains\nthe ability to remember and do life-long one-shot learning. Our module\nremembers training examples shown many thousands of steps in the past and it\ncan successfully generalize from them. We set new state-of-the-art for one-shot\nlearning on the Omniglot dataset and demonstrate, for the first time, life-long\none-shot learning in recurrent neural networks on a large-scale machine\ntranslation task.", "authors": ["\u0141ukasz Kaiser", "Ofir Nachum", "Aurko Roy", "Samy Bengio"], "category": "cs.LG", "comment": "Conference paper accepted for ICLR\u002717", "img": "/static/thumbs/1703.03129v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.03129v1", "num_discussion": 0, "originally_published_time": "3/9/2017", "pid": "1703.03129v1", "published_time": "3/9/2017", "rawpid": "1703.03129", "tags": ["cs.LG"], "title": "Learning to Remember Rare Events"}, {"abstract": "Variational Autoencoders (VAEs) are expressive latent variable models that\ncan be used to learn complex probability distributions from training data.\nHowever, the quality of the resulting model crucially relies on the\nexpressiveness of the inference model. We introduce Adversarial Variational\nBayes (AVB), a technique for training Variational Autoencoders with arbitrarily\nexpressive inference models. We achieve this by introducing an auxiliary\ndiscriminative network that allows to rephrase the maximum-likelihood-problem\nas a two-player game, hence establishing a principled connection between VAEs\nand Generative Adversarial Networks (GANs). We show that in the nonparametric\nlimit our method yields an exact maximum-likelihood assignment for the\nparameters of the generative model, as well as the exact posterior distribution\nover the latent variables given an observation. Contrary to competing\napproaches which combine VAEs with GANs, our approach has a clear theoretical\njustification, retains most advantages of standard Variational Autoencoders and\nis easy to implement.", "authors": ["Lars Mescheder", "Sebastian Nowozin", "Andreas Geiger"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1701.04722v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1701.04722v3", "num_discussion": 0, "originally_published_time": "1/17/2017", "pid": "1701.04722v3", "published_time": "8/2/2017", "rawpid": "1701.04722", "tags": ["cs.LG"], "title": "Adversarial Variational Bayes: Unifying Variational Autoencoders and\n  Generative Adversarial Networks"}, {"abstract": "We propose a new approach to the problem of neural network expressivity,\nwhich seeks to characterize how structural properties of a neural network\nfamily affect the functions it is able to compute. Our approach is based on an\ninterrelated set of measures of expressivity, unified by the novel notion of\ntrajectory length, which measures how the output of a network changes as the\ninput sweeps along a one-dimensional path. Our findings can be summarized as\nfollows:\n  (1) The complexity of the computed function grows exponentially with depth.\n  (2) All weights are not equal: trained networks are more sensitive to their\nlower (initial) layer weights.\n  (3) Regularizing on trajectory length (trajectory regularization) is a\nsimpler alternative to batch normalization, with the same performance.", "authors": ["Maithra Raghu", "Ben Poole", "Jon Kleinberg", "Surya Ganguli", "Jascha Sohl-Dickstein"], "category": "stat.ML", "comment": "Accepted to ICML 2017", "img": "/static/thumbs/1606.05336v6.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1606.05336v6", "num_discussion": 0, "originally_published_time": "6/16/2016", "pid": "1606.05336v6", "published_time": "6/18/2017", "rawpid": "1606.05336", "tags": ["stat.ML", "cs.AI", "cs.LG"], "title": "On the Expressive Power of Deep Neural Networks"}, {"abstract": "Recently, researchers have made significant progress combining the advances\nin deep learning for learning feature representations with reinforcement\nlearning. Some notable examples include training agents to play Atari games\nbased on raw pixel data and to acquire advanced manipulation skills using raw\nsensory inputs. However, it has been difficult to quantify progress in the\ndomain of continuous control due to the lack of a commonly adopted benchmark.\nIn this work, we present a benchmark suite of continuous control tasks,\nincluding classic tasks like cart-pole swing-up, tasks with very high state and\naction dimensionality such as 3D humanoid locomotion, tasks with partial\nobservations, and tasks with hierarchical structure. We report novel findings\nbased on the systematic evaluation of a range of implemented reinforcement\nlearning algorithms. Both the benchmark and reference implementations are\nreleased at https://github.com/rllab/rllab in order to facilitate experimental\nreproducibility and to encourage adoption by other researchers.", "authors": ["Yan Duan", "Xi Chen", "Rein Houthooft", "John Schulman", "Pieter Abbeel"], "category": "cs.LG", "comment": "14 pages, ICML 2016", "img": "/static/thumbs/1604.06778v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1604.06778v3", "num_discussion": 0, "originally_published_time": "4/22/2016", "pid": "1604.06778v3", "published_time": "5/27/2016", "rawpid": "1604.06778", "tags": ["cs.LG", "cs.AI", "cs.RO"], "title": "Benchmarking Deep Reinforcement Learning for Continuous Control"}, {"abstract": "Convolutional networks are at the core of most state-of-the-art computer\nvision solutions for a wide variety of tasks. Since 2014 very deep\nconvolutional networks started to become mainstream, yielding substantial gains\nin various benchmarks. Although increased model size and computational cost\ntend to translate to immediate quality gains for most tasks (as long as enough\nlabeled data is provided for training), computational efficiency and low\nparameter count are still enabling factors for various use cases such as mobile\nvision and big-data scenarios. Here we explore ways to scale up networks in\nways that aim at utilizing the added computation as efficiently as possible by\nsuitably factorized convolutions and aggressive regularization. We benchmark\nour methods on the ILSVRC 2012 classification challenge validation set\ndemonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6%\ntop-5 error for single frame evaluation using a network with a computational\ncost of 5 billion multiply-adds per inference and with using less than 25\nmillion parameters. With an ensemble of 4 models and multi-crop evaluation, we\nreport 3.5% top-5 error on the validation set (3.6% error on the test set) and\n17.3% top-1 error on the validation set.", "authors": ["Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jonathon Shlens", "Zbigniew Wojna"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1512.00567v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1512.00567v3", "num_discussion": 0, "originally_published_time": "12/2/2015", "pid": "1512.00567v3", "published_time": "12/11/2015", "rawpid": "1512.00567", "tags": ["cs.CV"], "title": "Rethinking the Inception Architecture for Computer Vision"}, {"abstract": "Deep Neural Networks (DNN) have achieved state-of-the-art results in a wide\nrange of tasks, with the best results obtained with large training sets and\nlarge models. In the past, GPUs enabled these breakthroughs because of their\ngreater computational speed. In the future, faster computation at both training\nand test time is likely to be crucial for further progress and for consumer\napplications on low-power devices. As a result, there is much interest in\nresearch and development of dedicated hardware for Deep Learning (DL). Binary\nweights, i.e., weights which are constrained to only two possible values (e.g.\n-1 or 1), would bring great benefits to specialized DL hardware by replacing\nmany multiply-accumulate operations by simple accumulations, as multipliers are\nthe most space and power-hungry components of the digital implementation of\nneural networks. We introduce BinaryConnect, a method which consists in\ntraining a DNN with binary weights during the forward and backward\npropagations, while retaining precision of the stored weights in which\ngradients are accumulated. Like other dropout schemes, we show that\nBinaryConnect acts as regularizer and we obtain near state-of-the-art results\nwith BinaryConnect on the permutation-invariant MNIST, CIFAR-10 and SVHN.", "authors": ["Matthieu Courbariaux", "Yoshua Bengio", "Jean-Pierre David"], "category": "cs.LG", "comment": "Accepted at NIPS 2015, 9 pages, 3 figures", "img": "/static/thumbs/1511.00363v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1511.00363v3", "num_discussion": 0, "originally_published_time": "11/2/2015", "pid": "1511.00363v3", "published_time": "4/18/2016", "rawpid": "1511.00363", "tags": ["cs.LG", "cs.CV", "cs.NE"], "title": "BinaryConnect: Training Deep Neural Networks with binary weights during\n  propagations"}, {"abstract": "Recent years have witnessed amazing progress in AI related fields such as\ncomputer vision, machine learning and autonomous vehicles. As with any rapidly\ngrowing field, however, it becomes increasingly difficult to stay up-to-date or\nenter the field as a beginner. While several topic specific survey papers have\nbeen written, to date no general survey on problems, datasets and methods in\ncomputer vision for autonomous vehicles exists. This paper attempts to narrow\nthis gap by providing a state-of-the-art survey on this topic. Our survey\nincludes both the historically most relevant literature as well as the current\nstate-of-the-art on several specific topics, including recognition,\nreconstruction, motion estimation, tracking, scene understanding and end-to-end\nlearning. Towards this goal, we first provide a taxonomy to classify each\napproach and then analyze the performance of the state-of-the-art on several\nchallenging benchmarking datasets including KITTI, ISPRS, MOT and Cityscapes.\nBesides, we discuss open problems and current research challenges. To ease\naccessibility and accommodate missing references, we will also provide an\ninteractive platform which allows to navigate topics and methods, and provides\nadditional information and project links for each paper.", "authors": ["Joel Janai", "Fatma G\u00fcney", "Aseem Behl", "Andreas Geiger"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1704.05519v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.05519v1", "num_discussion": 0, "originally_published_time": "4/18/2017", "pid": "1704.05519v1", "published_time": "4/18/2017", "rawpid": "1704.05519", "tags": ["cs.CV", "cs.RO"], "title": "Computer Vision for Autonomous Vehicles: Problems, Datasets and\n  State-of-the-Art"}, {"abstract": "One of the key challenges of artificial intelligence is to learn models that\nare effective in the context of planning. In this document we introduce the\npredictron architecture. The predictron consists of a fully abstract model,\nrepresented by a Markov reward process, that can be rolled forward multiple\n\"imagined\" planning steps. Each forward pass of the predictron accumulates\ninternal rewards and values over multiple planning depths. The predictron is\ntrained end-to-end so as to make these accumulated values accurately\napproximate the true value function. We applied the predictron to procedurally\ngenerated random mazes and a simulator for the game of pool. The predictron\nyielded significantly more accurate predictions than conventional deep neural\nnetwork architectures.", "authors": ["David Silver", "Hado van Hasselt", "Matteo Hessel", "Tom Schaul", "Arthur Guez", "Tim Harley", "Gabriel Dulac-Arnold", "David Reichert", "Neil Rabinowitz", "Andre Barreto", "Thomas Degris"], "category": "cs.LG", "comment": "Camera-ready version, ICML 2017, with supplement", "img": "/static/thumbs/1612.08810v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1612.08810v3", "num_discussion": 0, "originally_published_time": "12/28/2016", "pid": "1612.08810v3", "published_time": "7/20/2017", "rawpid": "1612.08810", "tags": ["cs.LG", "cs.AI", "cs.NE"], "title": "The Predictron: End-To-End Learning and Planning"}, {"abstract": "Feature pyramids are a basic component in recognition systems for detecting\nobjects at different scales. But recent deep learning object detectors have\navoided pyramid representations, in part because they are compute and memory\nintensive. In this paper, we exploit the inherent multi-scale, pyramidal\nhierarchy of deep convolutional networks to construct feature pyramids with\nmarginal extra cost. A top-down architecture with lateral connections is\ndeveloped for building high-level semantic feature maps at all scales. This\narchitecture, called a Feature Pyramid Network (FPN), shows significant\nimprovement as a generic feature extractor in several applications. Using FPN\nin a basic Faster R-CNN system, our method achieves state-of-the-art\nsingle-model results on the COCO detection benchmark without bells and\nwhistles, surpassing all existing single-model entries including those from the\nCOCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU\nand thus is a practical and accurate solution to multi-scale object detection.\nCode will be made publicly available.", "authors": ["Tsung-Yi Lin", "Piotr Doll\u00e1r", "Ross Girshick", "Kaiming He", "Bharath Hariharan", "Serge Belongie"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1612.03144v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1612.03144v2", "num_discussion": 0, "originally_published_time": "12/9/2016", "pid": "1612.03144v2", "published_time": "4/19/2017", "rawpid": "1612.03144", "tags": ["cs.CV"], "title": "Feature Pyramid Networks for Object Detection"}, {"abstract": "Some machine learning applications involve training data that is sensitive,\nsuch as the medical histories of patients in a clinical trial. A model may\ninadvertently and implicitly store some of its training data; careful analysis\nof the model may therefore reveal sensitive information.\n  To address this problem, we demonstrate a generally applicable approach to\nproviding strong privacy guarantees for training data: Private Aggregation of\nTeacher Ensembles (PATE). The approach combines, in a black-box fashion,\nmultiple models trained with disjoint datasets, such as records from different\nsubsets of users. Because they rely directly on sensitive data, these models\nare not published, but instead used as \"teachers\" for a \"student\" model. The\nstudent learns to predict an output chosen by noisy voting among all of the\nteachers, and cannot directly access an individual teacher or the underlying\ndata or parameters. The student\u0027s privacy properties can be understood both\nintuitively (since no single teacher and thus no single dataset dictates the\nstudent\u0027s training) and formally, in terms of differential privacy. These\nproperties hold even if an adversary can not only query the student but also\ninspect its internal workings.\n  Compared with previous work, the approach imposes only weak assumptions on\nhow teachers are trained: it applies to any model, including non-convex models\nlike DNNs. We achieve state-of-the-art privacy/utility trade-offs on MNIST and\nSVHN thanks to an improved privacy analysis and semi-supervised learning.", "authors": ["Nicolas Papernot", "Mart\u00edn Abadi", "\u00dalfar Erlingsson", "Ian Goodfellow", "Kunal Talwar"], "category": "stat.ML", "comment": "Accepted to ICLR 17 as an oral", "img": "/static/thumbs/1610.05755v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1610.05755v4", "num_discussion": 0, "originally_published_time": "10/18/2016", "pid": "1610.05755v4", "published_time": "3/3/2017", "rawpid": "1610.05755", "tags": ["stat.ML", "cs.CR", "cs.LG"], "title": "Semi-supervised Knowledge Transfer for Deep Learning from Private\n  Training Data"}, {"abstract": "While the authors of Batch Normalization (BN) identify and address an\nimportant problem involved in training deep networks-- Internal Covariate\nShift-- the current solution has certain drawbacks. Specifically, BN depends on\nbatch statistics for layerwise input normalization during training which makes\nthe estimates of mean and standard deviation of input (distribution) to hidden\nlayers inaccurate for validation due to shifting parameter values (especially\nduring initial training epochs). Also, BN cannot be used with batch-size 1\nduring training. We address these drawbacks by proposing a non-adaptive\nnormalization technique for removing internal covariate shift, that we call\nNormalization Propagation. Our approach does not depend on batch statistics,\nbut rather uses a data-independent parametric estimate of mean and\nstandard-deviation in every layer thus being computationally faster compared\nwith BN. We exploit the observation that the pre-activation before Rectified\nLinear Units follow Gaussian distribution in deep networks, and that once the\nfirst and second order statistics of any given dataset are normalized, we can\nforward propagate this normalization without the need for recalculating the\napproximate statistics for hidden layers.", "authors": ["Devansh Arpit", "Yingbo Zhou", "Bhargava U. Kota", "Venu Govindaraju"], "category": "stat.ML", "comment": "11 pages, ICML 2016, appendix added to the last version", "img": "/static/thumbs/1603.01431v6.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1603.01431v6", "num_discussion": 0, "originally_published_time": "3/4/2016", "pid": "1603.01431v6", "published_time": "7/12/2016", "rawpid": "1603.01431", "tags": ["stat.ML", "cs.LG"], "title": "Normalization Propagation: A Parametric Technique for Removing Internal\n  Covariate Shift in Deep Networks"}, {"abstract": "Generative Adversarial Networks (GANs) excel at creating realistic images\nwith complex models for which maximum likelihood is infeasible. However, the\nconvergence of GAN training has still not been proved. We propose a two\ntime-scale update rule (TTUR) for training GANs with stochastic gradient\ndescent that has an individual learning rate for both the discriminator and the\ngenerator. We prove that the TTUR converges under mild assumptions to a\nstationary Nash equilibrium. The convergence carries over to the popular Adam\noptimization, for which we prove that it follows the dynamics of a heavy ball\nwith friction and thus prefers flat minima in the objective landscape. For the\nevaluation of the performance of GANs at image generation, we introduce the\n\"Fr\\\u0027echet Inception Distance\" (FID) which captures the similarity of generated\nimages to real ones better than the Inception Score. In experiments, TTUR\nimproves learning for DCGANs, improved Wasserstein GANs, and BEGANs,\noutperforming conventional GAN training on CelebA, One Billion Word Benchmark,\nand LSUN bedrooms.", "authors": ["Martin Heusel", "Hubert Ramsauer", "Thomas Unterthiner", "Bernhard Nessler", "G\u00fcnter Klambauer", "Sepp Hochreiter"], "category": "cs.LG", "comment": "15 pages (+ 45 pages appendix) Implementations are available at:\n  https://github.com/bioinf-jku/TTU...", "img": "/static/thumbs/1706.08500v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1706.08500v4", "num_discussion": 0, "originally_published_time": "6/26/2017", "pid": "1706.08500v4", "published_time": "7/13/2017", "rawpid": "1706.08500", "tags": ["cs.LG", "stat.ML"], "title": "GANs Trained by a Two Time-Scale Update Rule Converge to a Nash\n  Equilibrium"}, {"abstract": "Generative Adversarial Networks (GANs) have gathered a lot of attention from\nthe computer vision community, yielding impressive results for image\ngeneration. Advances in the adversarial generation of natural language from\nnoise however are not commensurate with the progress made in generating images,\nand still lag far behind likelihood based methods. In this paper, we take a\nstep towards generating natural language with a GAN objective alone. We\nintroduce a simple baseline that addresses the discrete output space problem\nwithout relying on gradient estimators and show that it is able to achieve\nstate-of-the-art results on a Chinese poem generation dataset. We present\nquantitative results on generating sentences from context-free and\nprobabilistic context-free grammars, and qualitative language modeling results.\nA conditional version is also described that can generate sequences conditioned\non sentence characteristics.", "authors": ["Sai Rajeswar", "Sandeep Subramanian", "Francis Dutil", "Christopher Pal", "Aaron Courville"], "category": "cs.CL", "comment": "11 pages, 3 figures, 5 tables", "img": "/static/thumbs/1705.10929v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1705.10929v1", "num_discussion": 0, "originally_published_time": "5/31/2017", "pid": "1705.10929v1", "published_time": "5/31/2017", "rawpid": "1705.10929", "tags": ["cs.CL", "cs.AI", "cs.NE", "stat.ML"], "title": "Adversarial Generation of Natural Language"}, {"abstract": "Deep learning has been shown as a successful machine learning method for a\nvariety of tasks, and its popularity results in numerous open-source deep\nlearning software tools. Training a deep network is usually a very\ntime-consuming process. To address the computational challenge in deep\nlearning, many tools exploit hardware features such as multi-core CPUs and\nmany-core GPUs to shorten the training time. However, different tools exhibit\ndifferent features and running performance when training different types of\ndeep networks on different hardware platforms, which makes it difficult for end\nusers to select an appropriate pair of software and hardware. In this paper, we\naim to make a comparative study of the state-of-the-art GPU-accelerated deep\nlearning software tools, including Caffe, CNTK, MXNet, TensorFlow, and Torch.\nWe first benchmark the running performance of these tools with three popular\ntypes of neural networks on two CPU platforms and three GPU platforms. We then\nbenchmark some distributed versions on multiple GPUs. Our contribution is\ntwo-fold. First, for end users of deep learning tools, our benchmarking results\ncan serve as a guide to selecting appropriate hardware platforms and software\ntools. Second, for software developers of deep learning tools, our in-depth\nanalysis points out possible future directions to further optimize the running\nperformance.", "authors": ["Shaohuai Shi", "Qiang Wang", "Pengfei Xu", "Xiaowen Chu"], "category": "cs.DC", "comment": "Revision history: 1. Revise ResNet-50 configuration in MXNet. 2. Add\n  faster implementation of ResN...", "img": "/static/thumbs/1608.07249v7.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1608.07249v7", "num_discussion": 0, "originally_published_time": "8/25/2016", "pid": "1608.07249v7", "published_time": "2/17/2017", "rawpid": "1608.07249", "tags": ["cs.DC", "cs.LG"], "title": "Benchmarking State-of-the-Art Deep Learning Software Tools"}, {"abstract": "We introduce the adversarially learned inference (ALI) model, which jointly\nlearns a generation network and an inference network using an adversarial\nprocess. The generation network maps samples from stochastic latent variables\nto the data space while the inference network maps training examples in data\nspace to the space of latent variables. An adversarial game is cast between\nthese two networks and a discriminative network is trained to distinguish\nbetween joint latent/data-space samples from the generative network and joint\nsamples from the inference network. We illustrate the ability of the model to\nlearn mutually coherent inference and generation networks through the\ninspections of model samples and reconstructions and confirm the usefulness of\nthe learned representations by obtaining a performance competitive with\nstate-of-the-art on the semi-supervised SVHN and CIFAR10 tasks.", "authors": ["Vincent Dumoulin", "Ishmael Belghazi", "Ben Poole", "Olivier Mastropietro", "Alex Lamb", "Martin Arjovsky", "Aaron Courville"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1606.00704v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1606.00704v3", "num_discussion": 0, "originally_published_time": "6/2/2016", "pid": "1606.00704v3", "published_time": "2/21/2017", "rawpid": "1606.00704", "tags": ["stat.ML", "cs.LG"], "title": "Adversarially Learned Inference"}, {"abstract": "We introduce techniques for rapidly transferring the information stored in\none neural net into another neural net. The main purpose is to accelerate the\ntraining of a significantly larger neural net. During real-world workflows, one\noften trains very many different neural networks during the experimentation and\ndesign process. This is a wasteful process in which each new model is trained\nfrom scratch. Our Net2Net technique accelerates the experimentation process by\ninstantaneously transferring the knowledge from a previous network to each new\ndeeper or wider network. Our techniques are based on the concept of\nfunction-preserving transformations between neural network specifications. This\ndiffers from previous approaches to pre-training that altered the function\nrepresented by a neural net when adding layers to it. Using our knowledge\ntransfer mechanism to add depth to Inception modules, we demonstrate a new\nstate of the art accuracy rating on the ImageNet dataset.", "authors": ["Tianqi Chen", "Ian Goodfellow", "Jonathon Shlens"], "category": "cs.LG", "comment": "ICLR 2016 submission", "img": "/static/thumbs/1511.05641v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1511.05641v4", "num_discussion": 0, "originally_published_time": "11/18/2015", "pid": "1511.05641v4", "published_time": "4/23/2016", "rawpid": "1511.05641", "tags": ["cs.LG"], "title": "Net2Net: Accelerating Learning via Knowledge Transfer"}, {"abstract": "We propose SfM-Net, a geometry-aware neural network for motion estimation in\nvideos that decomposes frame-to-frame pixel motion in terms of scene and object\ndepth, camera motion and 3D object rotations and translations. Given a sequence\nof frames, SfM-Net predicts depth, segmentation, camera and rigid object\nmotions, converts those into a dense frame-to-frame motion field (optical\nflow), differentiably warps frames in time to match pixels and back-propagates.\nThe model can be trained with various degrees of supervision: 1)\nself-supervised by the re-projection photometric error (completely\nunsupervised), 2) supervised by ego-motion (camera motion), or 3) supervised by\ndepth (e.g., as provided by RGBD sensors). SfM-Net extracts meaningful depth\nestimates and successfully estimates frame-to-frame camera rotations and\ntranslations. It often successfully segments the moving objects in the scene,\neven though such supervision is never provided.", "authors": ["Sudheendra Vijayanarasimhan", "Susanna Ricco", "Cordelia Schmid", "Rahul Sukthankar", "Katerina Fragkiadaki"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1704.07804v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.07804v1", "num_discussion": 0, "originally_published_time": "4/25/2017", "pid": "1704.07804v1", "published_time": "4/25/2017", "rawpid": "1704.07804", "tags": ["cs.CV"], "title": "SfM-Net: Learning of Structure and Motion from Video"}, {"abstract": "Automatic synthesis of realistic images from text would be interesting and\nuseful, but current AI systems are still far from this goal. However, in recent\nyears generic and powerful recurrent neural network architectures have been\ndeveloped to learn discriminative text feature representations. Meanwhile, deep\nconvolutional generative adversarial networks (GANs) have begun to generate\nhighly compelling images of specific categories, such as faces, album covers,\nand room interiors. In this work, we develop a novel deep architecture and GAN\nformulation to effectively bridge these advances in text and image model- ing,\ntranslating visual concepts from characters to pixels. We demonstrate the\ncapability of our model to generate plausible images of birds and flowers from\ndetailed text descriptions.", "authors": ["Scott Reed", "Zeynep Akata", "Xinchen Yan", "Lajanugen Logeswaran", "Bernt Schiele", "Honglak Lee"], "category": "cs.NE", "comment": "ICML 2016", "img": "/static/thumbs/1605.05396v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1605.05396v2", "num_discussion": 0, "originally_published_time": "5/17/2016", "pid": "1605.05396v2", "published_time": "6/5/2016", "rawpid": "1605.05396", "tags": ["cs.NE", "cs.CV"], "title": "Generative Adversarial Text to Image Synthesis"}, {"abstract": "The popular Q-learning algorithm is known to overestimate action values under\ncertain conditions. It was not previously known whether, in practice, such\noverestimations are common, whether they harm performance, and whether they can\ngenerally be prevented. In this paper, we answer all these questions\naffirmatively. In particular, we first show that the recent DQN algorithm,\nwhich combines Q-learning with a deep neural network, suffers from substantial\noverestimations in some games in the Atari 2600 domain. We then show that the\nidea behind the Double Q-learning algorithm, which was introduced in a tabular\nsetting, can be generalized to work with large-scale function approximation. We\npropose a specific adaptation to the DQN algorithm and show that the resulting\nalgorithm not only reduces the observed overestimations, as hypothesized, but\nthat this also leads to much better performance on several games.", "authors": ["Hado van Hasselt", "Arthur Guez", "David Silver"], "category": "cs.LG", "comment": "AAAI 2016", "img": "/static/thumbs/1509.06461v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1509.06461v3", "num_discussion": 0, "originally_published_time": "9/22/2015", "pid": "1509.06461v3", "published_time": "12/8/2015", "rawpid": "1509.06461", "tags": ["cs.LG"], "title": "Deep Reinforcement Learning with Double Q-learning"}, {"abstract": "Deep generative models have achieved impressive success in recent years.\nGenerative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), as\npowerful frameworks for deep generative model learning, have largely been\nconsidered as two distinct paradigms and received extensive independent study\nrespectively. This paper establishes formal connections between deep generative\nmodeling approaches through a new formulation of GANs and VAEs. We show that\nGANs and VAEs are essentially minimizing KL divergences of respective posterior\nand inference distributions with opposite directions, extending the two\nlearning phases of classic wake-sleep algorithm, respectively. The unified view\nprovides a powerful tool to analyze a diverse set of existing model variants,\nand enables to exchange ideas across research lines in a principled way. For\nexample, we transfer the importance weighting method in VAE literatures for\nimproved GAN learning, and enhance VAEs with an adversarial mechanism for\nleveraging generated samples. Quantitative experiments show generality and\neffectiveness of the imported extensions.", "authors": ["Zhiting Hu", "Zichao Yang", "Ruslan Salakhutdinov", "Eric P. Xing"], "category": "cs.LG", "comment": "major revision: added more materials/discussions/figures, fixed typos", "img": "/static/thumbs/1706.00550v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1706.00550v3", "num_discussion": 0, "originally_published_time": "6/2/2017", "pid": "1706.00550v3", "published_time": "7/16/2017", "rawpid": "1706.00550", "tags": ["cs.LG", "stat.ML"], "title": "On Unifying Deep Generative Models"}, {"abstract": "We propose a new equilibrium enforcing method paired with a loss derived from\nthe Wasserstein distance for training auto-encoder based Generative Adversarial\nNetworks. This method balances the generator and discriminator during training.\nAdditionally, it provides a new approximate convergence measure, fast and\nstable training and high visual quality. We also derive a way of controlling\nthe trade-off between image diversity and visual quality. We focus on the image\ngeneration task, setting a new milestone in visual quality, even at higher\nresolutions. This is achieved while using a relatively simple model\narchitecture and a standard training procedure.", "authors": ["David Berthelot", "Thomas Schumm", "Luke Metz"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1703.10717v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.10717v4", "num_discussion": 0, "originally_published_time": "3/31/2017", "pid": "1703.10717v4", "published_time": "5/31/2017", "rawpid": "1703.10717", "tags": ["cs.LG", "stat.ML"], "title": "BEGAN: Boundary Equilibrium Generative Adversarial Networks"}, {"abstract": "The problem of arbitrary object tracking has traditionally been tackled by\nlearning a model of the object\u0027s appearance exclusively online, using as sole\ntraining data the video itself. Despite the success of these methods, their\nonline-only approach inherently limits the richness of the model they can\nlearn. Recently, several attempts have been made to exploit the expressive\npower of deep convolutional networks. However, when the object to track is not\nknown beforehand, it is necessary to perform Stochastic Gradient Descent online\nto adapt the weights of the network, severely compromising the speed of the\nsystem. In this paper we equip a basic tracking algorithm with a novel\nfully-convolutional Siamese network trained end-to-end on the ILSVRC15 dataset\nfor object detection in video. Our tracker operates at frame-rates beyond\nreal-time and, despite its extreme simplicity, achieves state-of-the-art\nperformance in multiple benchmarks.", "authors": ["Luca Bertinetto", "Jack Valmadre", "Jo\u00e3o F. Henriques", "Andrea Vedaldi", "Philip H. S. Torr"], "category": "cs.CV", "comment": "The first two authors contributed equally, and are listed in\n  alphabetical order. Code available at...", "img": "/static/thumbs/1606.09549v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1606.09549v2", "num_discussion": 0, "originally_published_time": "6/30/2016", "pid": "1606.09549v2", "published_time": "9/14/2016", "rawpid": "1606.09549", "tags": ["cs.CV"], "title": "Fully-Convolutional Siamese Networks for Object Tracking"}, {"abstract": "The development of intelligent machines is one of the biggest unsolved\nchallenges in computer science. In this paper, we propose some fundamental\nproperties these machines should have, focusing in particular on communication\nand learning. We discuss a simple environment that could be used to\nincrementally teach a machine the basics of natural-language-based\ncommunication, as a prerequisite to more complex interaction with human users.\nWe also present some conjectures on the sort of algorithms the machine should\nsupport in order to profitably learn from the environment.", "authors": ["Tomas Mikolov", "Armand Joulin", "Marco Baroni"], "category": "cs.AI", "comment": "", "img": "/static/thumbs/1511.08130v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1511.08130v2", "num_discussion": 0, "originally_published_time": "11/25/2015", "pid": "1511.08130v2", "published_time": "2/26/2016", "rawpid": "1511.08130", "tags": ["cs.AI", "cs.CL"], "title": "A Roadmap towards Machine Intelligence"}, {"abstract": "We introduce a neural network with a recurrent attention model over a\npossibly large external memory. The architecture is a form of Memory Network\n(Weston et al., 2015) but unlike the model in that work, it is trained\nend-to-end, and hence requires significantly less supervision during training,\nmaking it more generally applicable in realistic settings. It can also be seen\nas an extension of RNNsearch to the case where multiple computational steps\n(hops) are performed per output symbol. The flexibility of the model allows us\nto apply it to tasks as diverse as (synthetic) question answering and to\nlanguage modeling. For the former our approach is competitive with Memory\nNetworks, but with less supervision. For the latter, on the Penn TreeBank and\nText8 datasets our approach demonstrates comparable performance to RNNs and\nLSTMs. In both cases we show that the key concept of multiple computational\nhops yields improved results.", "authors": ["Sainbayar Sukhbaatar", "Arthur Szlam", "Jason Weston", "Rob Fergus"], "category": "cs.NE", "comment": "Accepted to NIPS 2015", "img": "/static/thumbs/1503.08895v5.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1503.08895v5", "num_discussion": 0, "originally_published_time": "3/31/2015", "pid": "1503.08895v5", "published_time": "11/24/2015", "rawpid": "1503.08895", "tags": ["cs.NE", "cs.CL"], "title": "End-To-End Memory Networks"}, {"abstract": "Several machine learning models, including neural networks, consistently\nmisclassify adversarial examples---inputs formed by applying small but\nintentionally worst-case perturbations to examples from the dataset, such that\nthe perturbed input results in the model outputting an incorrect answer with\nhigh confidence. Early attempts at explaining this phenomenon focused on\nnonlinearity and overfitting. We argue instead that the primary cause of neural\nnetworks\u0027 vulnerability to adversarial perturbation is their linear nature.\nThis explanation is supported by new quantitative results while giving the\nfirst explanation of the most intriguing fact about them: their generalization\nacross architectures and training sets. Moreover, this view yields a simple and\nfast method of generating adversarial examples. Using this approach to provide\nexamples for adversarial training, we reduce the test set error of a maxout\nnetwork on the MNIST dataset.", "authors": ["Ian J. Goodfellow", "Jonathon Shlens", "Christian Szegedy"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1412.6572v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1412.6572v3", "num_discussion": 0, "originally_published_time": "12/20/2014", "pid": "1412.6572v3", "published_time": "3/20/2015", "rawpid": "1412.6572", "tags": ["stat.ML", "cs.LG"], "title": "Explaining and Harnessing Adversarial Examples"}, {"abstract": "The Wasserstein probability metric has received much attention from the\nmachine learning community. Unlike the Kullback-Leibler divergence, which\nstrictly measures change in probability, the Wasserstein metric reflects the\nunderlying geometry between outcomes. The value of being sensitive to this\ngeometry has been demonstrated, among others, in ordinal regression and\ngenerative modelling. In this paper we describe three natural properties of\nprobability divergences that reflect requirements from machine learning: sum\ninvariance, scale sensitivity, and unbiased sample gradients. The Wasserstein\nmetric possesses the first two properties but, unlike the Kullback-Leibler\ndivergence, does not possess the third. We provide empirical evidence\nsuggesting that this is a serious issue in practice. Leveraging insights from\nprobabilistic forecasting we propose an alternative to the Wasserstein metric,\nthe Cram\\\u0027er distance. We show that the Cram\\\u0027er distance possesses all three\ndesired properties, combining the best of the Wasserstein and Kullback-Leibler\ndivergences. To illustrate the relevance of the Cram\\\u0027er distance in practice\nwe design a new algorithm, the Cram\\\u0027er Generative Adversarial Network (GAN),\nand show that it performs significantly better than the related Wasserstein\nGAN.", "authors": ["Marc G. Bellemare", "Ivo Danihelka", "Will Dabney", "Shakir Mohamed", "Balaji Lakshminarayanan", "Stephan Hoyer", "R\u00e9mi Munos"], "category": "cs.AI", "comment": "", "img": "/static/thumbs/1705.10743v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1705.10743v1", "num_discussion": 0, "originally_published_time": "5/30/2017", "pid": "1705.10743v1", "published_time": "5/30/2017", "rawpid": "1705.10743", "tags": ["cs.AI", "cs.LG", "stat.ML"], "title": "The Cramer Distance as a Solution to Biased Wasserstein Gradients"}, {"abstract": "While humans easily recognize relations between data from different domains\nwithout any supervision, learning to automatically discover them is in general\nvery challenging and needs many ground-truth pairs that illustrate the\nrelations. To avoid costly pairing, we address the task of discovering\ncross-domain relations given unpaired data. We propose a method based on\ngenerative adversarial networks that learns to discover relations between\ndifferent domains (DiscoGAN). Using the discovered relations, our proposed\nnetwork successfully transfers style from one domain to another while\npreserving key attributes such as orientation and face identity. Source code\nfor official implementation is publicly available\nhttps://github.com/SKTBrain/DiscoGAN", "authors": ["Taeksoo Kim", "Moonsu Cha", "Hyunsoo Kim", "Jung Kwon Lee", "Jiwon Kim"], "category": "cs.CV", "comment": "Accepted to International Conference on Machine Learning (ICML) 2017", "img": "/static/thumbs/1703.05192v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.05192v2", "num_discussion": 0, "originally_published_time": "3/15/2017", "pid": "1703.05192v2", "published_time": "5/15/2017", "rawpid": "1703.05192", "tags": ["cs.CV"], "title": "Learning to Discover Cross-Domain Relations with Generative Adversarial\n  Networks"}, {"abstract": "This paper presents OptNet, a network architecture that integrates\noptimization problems (here, specifically in the form of quadratic programs) as\nindividual layers in larger end-to-end trainable deep networks. These layers\nencode constraints and complex dependencies between the hidden states that\ntraditional convolutional and fully-connected layers often cannot capture. In\nthis paper, we explore the foundations for such an architecture: we show how\ntechniques from sensitivity analysis, bilevel optimization, and implicit\ndifferentiation can be used to exactly differentiate through these layers and\nwith respect to layer parameters; we develop a highly efficient solver for\nthese layers that exploits fast GPU-based batch solves within a primal-dual\ninterior point method, and which provides backpropagation gradients with\nvirtually no additional cost on top of the solve; and we highlight the\napplication of these approaches in several problems. In one notable example, we\nshow that the method is capable of learning to play mini-Sudoku (4x4) given\njust input and output games, with no a priori information about the rules of\nthe game; this highlights the ability of our architecture to learn hard\nconstraints better than other neural architectures.", "authors": ["Brandon Amos", "J. Zico Kolter"], "category": "cs.LG", "comment": "ICML 2017", "img": "/static/thumbs/1703.00443v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.00443v2", "num_discussion": 1, "originally_published_time": "3/1/2017", "pid": "1703.00443v2", "published_time": "6/14/2017", "rawpid": "1703.00443", "tags": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "title": "OptNet: Differentiable Optimization as a Layer in Neural Networks"}, {"abstract": "The goal of this paper is to serve as a guide for selecting a detection\narchitecture that achieves the right speed/memory/accuracy balance for a given\napplication and platform. To this end, we investigate various ways to trade\naccuracy for speed and memory usage in modern convolutional object detection\nsystems. A number of successful systems have been proposed in recent years, but\napples-to-apples comparisons are difficult due to different base feature\nextractors (e.g., VGG, Residual Networks), different default image resolutions,\nas well as different hardware and software platforms. We present a unified\nimplementation of the Faster R-CNN [Ren et al., 2015], R-FCN [Dai et al., 2016]\nand SSD [Liu et al., 2015] systems, which we view as \"meta-architectures\" and\ntrace out the speed/accuracy trade-off curve created by using alternative\nfeature extractors and varying other critical parameters such as image size\nwithin each of these meta-architectures. On one extreme end of this spectrum\nwhere speed and memory are critical, we present a detector that achieves real\ntime speeds and can be deployed on a mobile device. On the opposite end in\nwhich accuracy is critical, we present a detector that achieves\nstate-of-the-art performance measured on the COCO detection task.", "authors": ["Jonathan Huang", "Vivek Rathod", "Chen Sun", "Menglong Zhu", "Anoop Korattikara", "Alireza Fathi", "Ian Fischer", "Zbigniew Wojna", "Yang Song", "Sergio Guadarrama", "Kevin Murphy"], "category": "cs.CV", "comment": "Accepted to CVPR 2017", "img": "/static/thumbs/1611.10012v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1611.10012v3", "num_discussion": 0, "originally_published_time": "11/30/2016", "pid": "1611.10012v3", "published_time": "4/25/2017", "rawpid": "1611.10012", "tags": ["cs.CV"], "title": "Speed/accuracy trade-offs for modern convolutional object detectors"}, {"abstract": "Recent neural models of dialogue generation offer great promise for\ngenerating responses for conversational agents, but tend to be shortsighted,\npredicting utterances one at a time while ignoring their influence on future\noutcomes. Modeling the future direction of a dialogue is crucial to generating\ncoherent, interesting dialogues, a need which led traditional NLP models of\ndialogue to draw on reinforcement learning. In this paper, we show how to\nintegrate these goals, applying deep reinforcement learning to model future\nreward in chatbot dialogue. The model simulates dialogues between two virtual\nagents, using policy gradient methods to reward sequences that display three\nuseful conversational properties: informativity (non-repetitive turns),\ncoherence, and ease of answering (related to forward-looking function). We\nevaluate our model on diversity, length as well as with human judges, showing\nthat the proposed algorithm generates more interactive responses and manages to\nfoster a more sustained conversation in dialogue simulation. This work marks a\nfirst step towards learning a neural conversational model based on the\nlong-term success of dialogues.", "authors": ["Jiwei Li", "Will Monroe", "Alan Ritter", "Michel Galley", "Jianfeng Gao", "Dan Jurafsky"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1606.01541v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1606.01541v4", "num_discussion": 0, "originally_published_time": "6/5/2016", "pid": "1606.01541v4", "published_time": "9/29/2016", "rawpid": "1606.01541", "tags": ["cs.CL"], "title": "Deep Reinforcement Learning for Dialogue Generation"}, {"abstract": "A core challenge for an agent learning to interact with the world is to\npredict how its actions affect objects in its environment. Many existing\nmethods for learning the dynamics of physical interactions require labeled\nobject information. However, to scale real-world interaction learning to a\nvariety of scenes and objects, acquiring labeled data becomes increasingly\nimpractical. To learn about physical object motion without labels, we develop\nan action-conditioned video prediction model that explicitly models pixel\nmotion, by predicting a distribution over pixel motion from previous frames.\nBecause our model explicitly predicts motion, it is partially invariant to\nobject appearance, enabling it to generalize to previously unseen objects. To\nexplore video prediction for real-world interactive agents, we also introduce a\ndataset of 59,000 robot interactions involving pushing motions, including a\ntest set with novel objects. In this dataset, accurate prediction of videos\nconditioned on the robot\u0027s future actions amounts to learning a \"visual\nimagination\" of different futures based on different courses of action. Our\nexperiments show that our proposed method produces more accurate video\npredictions both quantitatively and qualitatively, when compared to prior\nmethods.", "authors": ["Chelsea Finn", "Ian Goodfellow", "Sergey Levine"], "category": "cs.LG", "comment": "To appear in NIPS \u002716; Video results, code, and data available at:\n  http://www.sites.google.com/sit...", "img": "/static/thumbs/1605.07157v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1605.07157v4", "num_discussion": 0, "originally_published_time": "5/23/2016", "pid": "1605.07157v4", "published_time": "10/17/2016", "rawpid": "1605.07157", "tags": ["cs.LG", "cs.AI", "cs.CV", "cs.RO"], "title": "Unsupervised Learning for Physical Interaction through Video Prediction"}, {"abstract": "Applying convolutional neural networks to large images is computationally\nexpensive because the amount of computation scales linearly with the number of\nimage pixels. We present a novel recurrent neural network model that is capable\nof extracting information from an image or video by adaptively selecting a\nsequence of regions or locations and only processing the selected regions at\nhigh resolution. Like convolutional neural networks, the proposed model has a\ndegree of translation invariance built-in, but the amount of computation it\nperforms can be controlled independently of the input image size. While the\nmodel is non-differentiable, it can be trained using reinforcement learning\nmethods to learn task-specific policies. We evaluate our model on several image\nclassification tasks, where it significantly outperforms a convolutional neural\nnetwork baseline on cluttered images, and on a dynamic visual control problem,\nwhere it learns to track a simple object without an explicit training signal\nfor doing so.", "authors": ["Volodymyr Mnih", "Nicolas Heess", "Alex Graves", "Koray Kavukcuoglu"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1406.6247v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1406.6247v1", "num_discussion": 0, "originally_published_time": "6/24/2014", "pid": "1406.6247v1", "published_time": "6/24/2014", "rawpid": "1406.6247", "tags": ["cs.LG", "cs.CV", "stat.ML"], "title": "Recurrent Models of Visual Attention"}, {"abstract": "For sophisticated reinforcement learning (RL) systems to interact usefully\nwith real-world environments, we need to communicate complex goals to these\nsystems. In this work, we explore goals defined in terms of (non-expert) human\npreferences between pairs of trajectory segments. We show that this approach\ncan effectively solve complex RL tasks without access to the reward function,\nincluding Atari games and simulated robot locomotion, while providing feedback\non less than one percent of our agent\u0027s interactions with the environment. This\nreduces the cost of human oversight far enough that it can be practically\napplied to state-of-the-art RL systems. To demonstrate the flexibility of our\napproach, we show that we can successfully train complex novel behaviors with\nabout an hour of human time. These behaviors and environments are considerably\nmore complex than any that have been previously learned from human feedback.", "authors": ["Paul Christiano", "Jan Leike", "Tom B. Brown", "Miljan Martic", "Shane Legg", "Dario Amodei"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1706.03741v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1706.03741v3", "num_discussion": 0, "originally_published_time": "6/12/2017", "pid": "1706.03741v3", "published_time": "7/13/2017", "rawpid": "1706.03741", "tags": ["stat.ML"], "title": "Deep reinforcement learning from human preferences"}, {"abstract": "We present a new autoencoder-type architecture that is trainable in an\nunsupervised mode, sustains both generation and inference, and has the quality\nof conditional and unconditional samples boosted by adversarial learning.\nUnlike previous hybrids of autoencoders and adversarial networks, the\nadversarial game in our approach is set up directly between the encoder and the\ngenerator, and no external mappings are trained in the process of learning. The\ngame objective compares the divergences of each of the real and the generated\ndata distributions with the prior distribution in the latent space. We show\nthat direct generator-vs-encoder game leads to a tight coupling of the two\ncomponents, resulting in samples and reconstructions of a comparable quality to\nsome recently-proposed more complex architectures.", "authors": ["Dmitry Ulyanov", "Andrea Vedaldi", "Victor Lempitsky"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1704.02304v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.02304v2", "num_discussion": 0, "originally_published_time": "4/7/2017", "pid": "1704.02304v2", "published_time": "6/13/2017", "rawpid": "1704.02304", "tags": ["cs.CV", "cs.LG", "stat.ML"], "title": "It Takes (Only) Two: Adversarial Generator-Encoder Networks"}, {"abstract": "We present Deep Voice, a production-quality text-to-speech system constructed\nentirely from deep neural networks. Deep Voice lays the groundwork for truly\nend-to-end neural speech synthesis. The system comprises five major building\nblocks: a segmentation model for locating phoneme boundaries, a\ngrapheme-to-phoneme conversion model, a phoneme duration prediction model, a\nfundamental frequency prediction model, and an audio synthesis model. For the\nsegmentation model, we propose a novel way of performing phoneme boundary\ndetection with deep neural networks using connectionist temporal classification\n(CTC) loss. For the audio synthesis model, we implement a variant of WaveNet\nthat requires fewer parameters and trains faster than the original. By using a\nneural network for each component, our system is simpler and more flexible than\ntraditional text-to-speech systems, where each component requires laborious\nfeature engineering and extensive domain expertise. Finally, we show that\ninference with our system can be performed faster than real time and describe\noptimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x\nspeedups over existing implementations.", "authors": ["Sercan O. Arik", "Mike Chrzanowski", "Adam Coates", "Gregory Diamos", "Andrew Gibiansky", "Yongguo Kang", "Xian Li", "John Miller", "Andrew Ng", "Jonathan Raiman", "Shubho Sengupta", "Mohammad Shoeybi"], "category": "cs.CL", "comment": "Submitted to ICML 2017", "img": "/static/thumbs/1702.07825v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1702.07825v2", "num_discussion": 0, "originally_published_time": "2/25/2017", "pid": "1702.07825v2", "published_time": "3/7/2017", "rawpid": "1702.07825", "tags": ["cs.CL", "cs.LG", "cs.NE", "cs.SD"], "title": "Deep Voice: Real-time Neural Text-to-Speech"}, {"abstract": "Attentional, RNN-based encoder-decoder models for abstractive summarization\nhave achieved good performance on short input and output sequences. However,\nfor longer documents and summaries, these models often include repetitive and\nincoherent phrases. We introduce a neural network model with intra-attention\nand a new training method. This method combines standard supervised word\nprediction and reinforcement learning (RL). Models trained only with the former\noften exhibit \"exposure bias\" -- they assume ground truth is provided at each\nstep during training. However, when standard word prediction is combined with\nthe global sequence prediction training of RL the resulting summaries become\nmore readable. We evaluate this model on the CNN/Daily Mail and New York Times\ndatasets. Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail\ndataset, a 5.7 absolute points improvement over previous state-of-the-art\nmodels. It also performs well as the first abstractive model on the New York\nTimes corpus. Human evaluation also shows that our model produces higher\nquality summaries.", "authors": ["Romain Paulus", "Caiming Xiong", "Richard Socher"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1705.04304v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1705.04304v2", "num_discussion": 0, "originally_published_time": "5/11/2017", "pid": "1705.04304v2", "published_time": "5/19/2017", "rawpid": "1705.04304", "tags": ["cs.CL"], "title": "A Deep Reinforced Model for Abstractive Summarization"}, {"abstract": "Recurrent neural networks are a powerful tool for modeling sequential data,\nbut the dependence of each timestep\u0027s computation on the previous timestep\u0027s\noutput limits parallelism and makes RNNs unwieldy for very long sequences. We\nintroduce quasi-recurrent neural networks (QRNNs), an approach to neural\nsequence modeling that alternates convolutional layers, which apply in parallel\nacross timesteps, and a minimalist recurrent pooling function that applies in\nparallel across channels. Despite lacking trainable recurrent layers, stacked\nQRNNs have better predictive accuracy than stacked LSTMs of the same hidden\nsize. Due to their increased parallelism, they are up to 16 times faster at\ntrain and test time. Experiments on language modeling, sentiment\nclassification, and character-level neural machine translation demonstrate\nthese advantages and underline the viability of QRNNs as a basic building block\nfor a variety of sequence tasks.", "authors": ["James Bradbury", "Stephen Merity", "Caiming Xiong", "Richard Socher"], "category": "cs.NE", "comment": "Submitted to conference track at ICLR 2017", "img": "/static/thumbs/1611.01576v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1611.01576v2", "num_discussion": 0, "originally_published_time": "11/5/2016", "pid": "1611.01576v2", "published_time": "11/21/2016", "rawpid": "1611.01576", "tags": ["cs.NE", "cs.AI", "cs.CL", "cs.LG"], "title": "Quasi-Recurrent Neural Networks"}, {"abstract": "We describe Swapout, a new stochastic training method, that outperforms\nResNets of identical network structure yielding impressive results on CIFAR-10\nand CIFAR-100. Swapout samples from a rich set of architectures including\ndropout, stochastic depth and residual architectures as special cases. When\nviewed as a regularization method swapout not only inhibits co-adaptation of\nunits in a layer, similar to dropout, but also across network layers. We\nconjecture that swapout achieves strong regularization by implicitly tying the\nparameters across layers. When viewed as an ensemble training method, it\nsamples a much richer set of architectures than existing methods such as\ndropout or stochastic depth. We propose a parameterization that reveals\nconnections to exiting architectures and suggests a much richer set of\narchitectures to be explored. We show that our formulation suggests an\nefficient training method and validate our conclusions on CIFAR-10 and\nCIFAR-100 matching state of the art accuracy. Remarkably, our 32 layer wider\nmodel performs similar to a 1001 layer ResNet model.", "authors": ["Saurabh Singh", "Derek Hoiem", "David Forsyth"], "category": "cs.CV", "comment": "Submitted to NIPS 2016", "img": "/static/thumbs/1605.06465v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1605.06465v1", "num_discussion": 0, "originally_published_time": "5/20/2016", "pid": "1605.06465v1", "published_time": "5/20/2016", "rawpid": "1605.06465", "tags": ["cs.CV", "cs.LG", "cs.NE"], "title": "Swapout: Learning an ensemble of deep architectures"}, {"abstract": "Current generative frameworks use end-to-end learning and generate images by\nsampling from uniform noise distribution. However, these approaches ignore the\nmost basic principle of image formation: images are product of: (a) Structure:\nthe underlying 3D model; (b) Style: the texture mapped onto structure. In this\npaper, we factorize the image generation process and propose Style and\nStructure Generative Adversarial Network (S^2-GAN). Our S^2-GAN has two\ncomponents: the Structure-GAN generates a surface normal map; the Style-GAN\ntakes the surface normal map as input and generates the 2D image. Apart from a\nreal vs. generated loss function, we use an additional loss with computed\nsurface normals from generated images. The two GANs are first trained\nindependently, and then merged together via joint learning. We show our S^2-GAN\nmodel is interpretable, generates more realistic images and can be used to\nlearn unsupervised RGBD representations.", "authors": ["Xiaolong Wang", "Abhinav Gupta"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1603.05631v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1603.05631v2", "num_discussion": 0, "originally_published_time": "3/17/2016", "pid": "1603.05631v2", "published_time": "7/26/2016", "rawpid": "1603.05631", "tags": ["cs.CV"], "title": "Generative Image Modeling using Style and Structure Adversarial Networks"}, {"abstract": "Although the latest high-end smartphone has powerful CPU and GPU, running\ndeeper convolutional neural networks (CNNs) for complex tasks such as ImageNet\nclassification on mobile devices is challenging. To deploy deep CNNs on mobile\ndevices, we present a simple and effective scheme to compress the entire CNN,\nwhich we call one-shot whole network compression. The proposed scheme consists\nof three steps: (1) rank selection with variational Bayesian matrix\nfactorization, (2) Tucker decomposition on kernel tensor, and (3) fine-tuning\nto recover accumulated loss of accuracy, and each step can be easily\nimplemented using publicly available tools. We demonstrate the effectiveness of\nthe proposed scheme by testing the performance of various compressed CNNs\n(AlexNet, VGGS, GoogLeNet, and VGG-16) on the smartphone. Significant\nreductions in model size, runtime, and energy consumption are obtained, at the\ncost of small loss in accuracy. In addition, we address the important\nimplementation level issue on 1?1 convolution, which is a key operation of\ninception module of GoogLeNet as well as CNNs compressed by our proposed\nscheme.", "authors": ["Yong-Deok Kim", "Eunhyeok Park", "Sungjoo Yoo", "Taelim Choi", "Lu Yang", "Dongjun Shin"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1511.06530v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1511.06530v2", "num_discussion": 0, "originally_published_time": "11/20/2015", "pid": "1511.06530v2", "published_time": "2/24/2016", "rawpid": "1511.06530", "tags": ["cs.CV", "cs.LG"], "title": "Compression of Deep Convolutional Neural Networks for Fast and Low Power\n  Mobile Applications"}, {"abstract": "Deep neural networks have achieved impressive supervised classification\nperformance in many tasks including image recognition, speech recognition, and\nsequence to sequence learning. However, this success has not been translated to\napplications like question answering that may involve complex arithmetic and\nlogic reasoning. A major limitation of these models is in their inability to\nlearn even simple arithmetic and logic operations. For example, it has been\nshown that neural networks fail to learn to add two binary numbers reliably. In\nthis work, we propose Neural Programmer, an end-to-end differentiable neural\nnetwork augmented with a small set of basic arithmetic and logic operations.\nNeural Programmer can call these augmented operations over several steps,\nthereby inducing compositional programs that are more complex than the built-in\noperations. The model learns from a weak supervision signal which is the result\nof execution of the correct program, hence it does not require expensive\nannotation of the correct program itself. The decisions of what operations to\ncall, and what data segments to apply to are inferred by Neural Programmer.\nSuch decisions, during training, are done in a differentiable fashion so that\nthe entire network can be trained jointly by gradient descent. We find that\ntraining the model is difficult, but it can be greatly improved by adding\nrandom noise to the gradient. On a fairly complex synthetic table-comprehension\ndataset, traditional recurrent networks and attentional models perform poorly\nwhile Neural Programmer typically obtains nearly perfect accuracy.", "authors": ["Arvind Neelakantan", "Quoc V. Le", "Ilya Sutskever"], "category": "cs.LG", "comment": "Accepted as a conference paper at ICLR 2015", "img": "/static/thumbs/1511.04834v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1511.04834v3", "num_discussion": 0, "originally_published_time": "11/16/2015", "pid": "1511.04834v3", "published_time": "8/4/2016", "rawpid": "1511.04834", "tags": ["cs.LG", "cs.CL", "stat.ML"], "title": "Neural Programmer: Inducing Latent Programs with Gradient Descent"}, {"abstract": "The success of deep learning in vision can be attributed to: (a) models with\nhigh capacity; (b) increased computational power; and (c) availability of\nlarge-scale labeled data. Since 2012, there have been significant advances in\nrepresentation capabilities of the models and computational capabilities of\nGPUs. But the size of the biggest dataset has surprisingly remained constant.\nWhat will happen if we increase the dataset size by 10x or 100x? This paper\ntakes a step towards clearing the clouds of mystery surrounding the\nrelationship between `enormous data\u0027 and deep learning. By exploiting the\nJFT-300M dataset which has more than 375M noisy labels for 300M images, we\ninvestigate how the performance of current vision tasks would change if this\ndata was used for representation learning. Our paper delivers some surprising\n(and some expected) findings. First, we find that the performance on vision\ntasks still increases linearly with orders of magnitude of training data size.\nSecond, we show that representation learning (or pre-training) still holds a\nlot of promise. One can improve performance on any vision tasks by just\ntraining a better base model. Finally, as expected, we present new\nstate-of-the-art results for different vision tasks including image\nclassification, object detection, semantic segmentation and human pose\nestimation. Our sincere hope is that this inspires vision community to not\nundervalue the data and develop collective efforts in building larger datasets.", "authors": ["Chen Sun", "Abhinav Shrivastava", "Saurabh Singh", "Abhinav Gupta"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1707.02968v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.02968v1", "num_discussion": 0, "originally_published_time": "7/10/2017", "pid": "1707.02968v1", "published_time": "7/10/2017", "rawpid": "1707.02968", "tags": ["cs.CV", "cs.AI"], "title": "Revisiting Unreasonable Effectiveness of Data in Deep Learning Era"}, {"abstract": "The framework of normalizing flows provides a general strategy for flexible\nvariational inference of posteriors over latent variables. We propose a new\ntype of normalizing flow, inverse autoregressive flow (IAF), that, in contrast\nto earlier published flows, scales well to high-dimensional latent spaces. The\nproposed flow consists of a chain of invertible transformations, where each\ntransformation is based on an autoregressive neural network. In experiments, we\nshow that IAF significantly improves upon diagonal Gaussian approximate\nposteriors. In addition, we demonstrate that a novel type of variational\nautoencoder, coupled with IAF, is competitive with neural autoregressive models\nin terms of attained log-likelihood on natural images, while allowing\nsignificantly faster synthesis.", "authors": ["Diederik P. Kingma", "Tim Salimans", "Rafal Jozefowicz", "Xi Chen", "Ilya Sutskever", "Max Welling"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1606.04934v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1606.04934v2", "num_discussion": 0, "originally_published_time": "6/15/2016", "pid": "1606.04934v2", "published_time": "1/30/2017", "rawpid": "1606.04934", "tags": ["cs.LG", "stat.ML"], "title": "Improving Variational Inference with Inverse Autoregressive Flow"}, {"abstract": "We consider image transformation problems, where an input image is\ntransformed into an output image. Recent methods for such problems typically\ntrain feed-forward convolutional neural networks using a \\emph{per-pixel} loss\nbetween the output and ground-truth images. Parallel work has shown that\nhigh-quality images can be generated by defining and optimizing\n\\emph{perceptual} loss functions based on high-level features extracted from\npretrained networks. We combine the benefits of both approaches, and propose\nthe use of perceptual loss functions for training feed-forward networks for\nimage transformation tasks. We show results on image style transfer, where a\nfeed-forward network is trained to solve the optimization problem proposed by\nGatys et al in real-time. Compared to the optimization-based method, our\nnetwork gives similar qualitative results but is three orders of magnitude\nfaster. We also experiment with single-image super-resolution, where replacing\na per-pixel loss with a perceptual loss gives visually pleasing results.", "authors": ["Justin Johnson", "Alexandre Alahi", "Li Fei-Fei"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1603.08155v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1603.08155v1", "num_discussion": 0, "originally_published_time": "3/27/2016", "pid": "1603.08155v1", "published_time": "3/27/2016", "rawpid": "1603.08155", "tags": ["cs.CV", "cs.LG"], "title": "Perceptual Losses for Real-Time Style Transfer and Super-Resolution"}, {"abstract": "While the costs of human violence have attracted a great deal of attention\nfrom the research community, the effects of the network-on-network (NoN)\nviolence popularised by Generative Adversarial Networks have yet to be\naddressed. In this work, we quantify the financial, social, spiritual,\ncultural, grammatical and dermatological impact of this aggression and address\nthe issue by proposing a more peaceful approach which we term Generative\nUnadversarial Networks (GUNs). Under this framework, we simultaneously train\ntwo models: a generator G that does its best to capture whichever data\ndistribution it feels it can manage, and a motivator M that helps G to achieve\nits dream. Fighting is strictly verboten and both models evolve by learning to\nrespect their differences. The framework is both theoretically and electrically\ngrounded in game theory, and can be viewed as a winner-shares-all two-player\ngame in which both players work as a team to achieve the best score.\nExperiments show that by working in harmony, the proposed model is able to\nclaim both the moral and log-likelihood high ground. Our work builds on a rich\nhistory of carefully argued position-papers, published as anonymous YouTube\ncomments, which prove that the optimal solution to NoN violence is more GUNs.", "authors": ["Samuel Albanie", "S\u00e9bastien Ehrhardt", "Jo\u00e3o F. Henriques"], "category": "stat.ML", "comment": "Under review as a conference paper at SIGBOVIK 2017", "img": "/static/thumbs/1703.02528v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.02528v1", "num_discussion": 4, "originally_published_time": "3/7/2017", "pid": "1703.02528v1", "published_time": "3/7/2017", "rawpid": "1703.02528", "tags": ["stat.ML", "cs.LG"], "title": "Stopping GAN Violence: Generative Unadversarial Networks"}, {"abstract": "We consider the general problem of modeling temporal data with long-range\ndependencies, wherein new observations are fully or partially predictable based\non temporally-distant, past observations. A sufficiently powerful temporal\nmodel should separate predictable elements of the sequence from unpredictable\nelements, express uncertainty about those unpredictable elements, and rapidly\nidentify novel elements that may help to predict the future. To create such\nmodels, we introduce Generative Temporal Models augmented with external memory\nsystems. They are developed within the variational inference framework, which\nprovides both a practical training methodology and methods to gain insight into\nthe models\u0027 operation. We show, on a range of problems with sparse, long-term\ntemporal dependencies, that these models store information from early in a\nsequence, and reuse this stored information efficiently. This allows them to\nperform substantially better than existing models based on well-known recurrent\nneural networks, like LSTMs.", "authors": ["Mevlana Gemici", "Chia-Chun Hung", "Adam Santoro", "Greg Wayne", "Shakir Mohamed", "Danilo J. Rezende", "David Amos", "Timothy Lillicrap"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1702.04649v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1702.04649v2", "num_discussion": 0, "originally_published_time": "2/15/2017", "pid": "1702.04649v2", "published_time": "2/21/2017", "rawpid": "1702.04649", "tags": ["cs.LG", "cs.NE", "stat.ML"], "title": "Generative Temporal Models with Memory"}, {"abstract": "In this paper, drawing intuition from the Turing test, we propose using\nadversarial training for open-domain dialogue generation: the system is trained\nto produce sequences that are indistinguishable from human-generated dialogue\nutterances. We cast the task as a reinforcement learning (RL) problem where we\njointly train two systems, a generative model to produce response sequences,\nand a discriminator---analagous to the human evaluator in the Turing test--- to\ndistinguish between the human-generated dialogues and the machine-generated\nones. The outputs from the discriminator are then used as rewards for the\ngenerative model, pushing the system to generate dialogues that mostly resemble\nhuman dialogues.\n  In addition to adversarial training we describe a model for adversarial {\\em\nevaluation} that uses success in fooling an adversary as a dialogue evaluation\nmetric, while avoiding a number of potential pitfalls. Experimental results on\nseveral metrics, including adversarial evaluation, demonstrate that the\nadversarially-trained system generates higher-quality responses than previous\nbaselines.", "authors": ["Jiwei Li", "Will Monroe", "Tianlin Shi", "S\u00e9bastien Jean", "Alan Ritter", "Dan Jurafsky"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1701.06547v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1701.06547v4", "num_discussion": 0, "originally_published_time": "1/23/2017", "pid": "1701.06547v4", "published_time": "2/22/2017", "rawpid": "1701.06547", "tags": ["cs.CL"], "title": "Adversarial Learning for Neural Dialogue Generation"}, {"abstract": "Deep reinforcement learning (deep RL) has been successful in learning\nsophisticated behaviors automatically; however, the learning process requires a\nhuge number of trials. In contrast, animals can learn new tasks in just a few\ntrials, benefiting from their prior knowledge about the world. This paper seeks\nto bridge this gap. Rather than designing a \"fast\" reinforcement learning\nalgorithm, we propose to represent it as a recurrent neural network (RNN) and\nlearn it from data. In our proposed method, RL$^2$, the algorithm is encoded in\nthe weights of the RNN, which are learned slowly through a general-purpose\n(\"slow\") RL algorithm. The RNN receives all information a typical RL algorithm\nwould receive, including observations, actions, rewards, and termination flags;\nand it retains its state across episodes in a given Markov Decision Process\n(MDP). The activations of the RNN store the state of the \"fast\" RL algorithm on\nthe current (previously unseen) MDP. We evaluate RL$^2$ experimentally on both\nsmall-scale and large-scale problems. On the small-scale side, we train it to\nsolve randomly generated multi-arm bandit problems and finite MDPs. After\nRL$^2$ is trained, its performance on new MDPs is close to human-designed\nalgorithms with optimality guarantees. On the large-scale side, we test RL$^2$\non a vision-based navigation task and show that it scales up to\nhigh-dimensional problems.", "authors": ["Yan Duan", "John Schulman", "Xi Chen", "Peter L. Bartlett", "Ilya Sutskever", "Pieter Abbeel"], "category": "cs.AI", "comment": "14 pages. Under review as a conference paper at ICLR 2017", "img": "/static/thumbs/1611.02779v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1611.02779v2", "num_discussion": 0, "originally_published_time": "11/9/2016", "pid": "1611.02779v2", "published_time": "11/10/2016", "rawpid": "1611.02779", "tags": ["cs.AI", "cs.LG", "cs.NE", "stat.ML"], "title": "RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning"}, {"abstract": "Very deep convolutional neural networks (CNNs) yield state of the art results\non a wide variety of visual recognition problems. A number of state of the the\nart methods for image recognition are based on networks with well over 100\nlayers and the performance vs. depth trend is moving towards networks in excess\nof 1000 layers. In such extremely deep architectures the vanishing or exploding\ngradient problem becomes a key issue. Recent evidence also indicates that\nconvolutional networks could benefit from an interface to explicitly\nconstructed memory mechanisms interacting with a CNN feature processing\nhierarchy. Correspondingly, we propose and evaluate a memory mechanism enhanced\nconvolutional neural network architecture based on augmenting convolutional\nresidual networks with a long short term memory mechanism. We refer to this as\na convolutional residual memory network. To the best of our knowledge this\napproach can yield state of the art performance on the CIFAR-100 benchmark and\ncompares well with other state of the art techniques on the CIFAR-10 and SVHN\nbenchmarks. This is achieved using networks with more breadth, much less depth\nand much less overall computation relative to comparable deep ResNets without\nthe memory mechanism. Our experiments and analysis explore the importance of\nthe memory mechanism, network depth, breadth, and predictive performance.", "authors": ["Joel Moniz", "Christopher Pal"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1606.05262v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1606.05262v3", "num_discussion": 0, "originally_published_time": "6/16/2016", "pid": "1606.05262v3", "published_time": "7/14/2016", "rawpid": "1606.05262", "tags": ["cs.CV"], "title": "Convolutional Residual Memory Networks"}, {"abstract": "In this paper, we propose a principled deep reinforcement learning (RL)\napproach that is able to accelerate the convergence rate of general deep neural\nnetworks (DNNs). With our approach, a deep RL agent (synonym for optimizer in\nthis work) is used to automatically learn policies about how to schedule\nlearning rates during the optimization of a DNN. The state features of the\nagent are learned from the weight statistics of the optimizee during training.\nThe reward function of this agent is designed to learn policies that minimize\nthe optimizee\u0027s training time given a certain performance goal. The actions of\nthe agent correspond to changing the learning rate for the optimizee during\ntraining. As far as we know, this is the first attempt to use deep RL to learn\nhow to optimize a large-sized DNN. We perform extensive experiments on a\nstandard benchmark dataset and demonstrate the effectiveness of the policies\nlearned by our approach.", "authors": ["Jie Fu"], "category": "cs.LG", "comment": "We choose to withdraw this paper. The DQN itself has too many\n  hyperparameters, which makes it almo...", "img": "/static/thumbs/1606.01467v10.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1606.01467v10", "num_discussion": 0, "originally_published_time": "6/5/2016", "pid": "1606.01467v10", "published_time": "7/13/2017", "rawpid": "1606.01467", "tags": ["cs.LG", "cs.NE"], "title": "Deep Q-Networks for Accelerating the Training of Deep Neural Networks"}, {"abstract": "We discuss relations between Residual Networks (ResNet), Recurrent Neural\nNetworks (RNNs) and the primate visual cortex. We begin with the observation\nthat a shallow RNN is exactly equivalent to a very deep ResNet with weight\nsharing among the layers. A direct implementation of such a RNN, although\nhaving orders of magnitude fewer parameters, leads to a performance similar to\nthe corresponding ResNet. We propose 1) a generalization of both RNN and ResNet\narchitectures and 2) the conjecture that a class of moderately deep RNNs is a\nbiologically-plausible model of the ventral stream in visual cortex. We\ndemonstrate the effectiveness of the architectures by testing them on the\nCIFAR-10 dataset.", "authors": ["Qianli Liao", "Tomaso Poggio"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1604.03640v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1604.03640v1", "num_discussion": 0, "originally_published_time": "4/13/2016", "pid": "1604.03640v1", "published_time": "4/13/2016", "rawpid": "1604.03640", "tags": ["cs.LG", "cs.NE"], "title": "Bridging the Gaps Between Residual Learning, Recurrent Neural Networks\n  and Visual Cortex"}, {"abstract": "Neural network architectures with memory and attention mechanisms exhibit\ncertain reasoning capabilities required for question answering. One such\narchitecture, the dynamic memory network (DMN), obtained high accuracy on a\nvariety of language tasks. However, it was not shown whether the architecture\nachieves strong results for question answering when supporting facts are not\nmarked during training or whether it could be applied to other modalities such\nas images. Based on an analysis of the DMN, we propose several improvements to\nits memory and input modules. Together with these changes we introduce a novel\ninput module for images in order to be able to answer visual questions. Our new\nDMN+ model improves the state of the art on both the Visual Question Answering\ndataset and the \\babi-10k text question-answering dataset without supporting\nfact supervision.", "authors": ["Caiming Xiong", "Stephen Merity", "Richard Socher"], "category": "cs.NE", "comment": "", "img": "/static/thumbs/1603.01417v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1603.01417v1", "num_discussion": 0, "originally_published_time": "3/4/2016", "pid": "1603.01417v1", "published_time": "3/4/2016", "rawpid": "1603.01417", "tags": ["cs.NE", "cs.CL", "cs.CV"], "title": "Dynamic Memory Networks for Visual and Textual Question Answering"}, {"abstract": "Despite widespread adoption, machine learning models remain mostly black\nboxes. Understanding the reasons behind predictions is, however, quite\nimportant in assessing trust, which is fundamental if one plans to take action\nbased on a prediction, or when choosing whether to deploy a new model. Such\nunderstanding also provides insights into the model, which can be used to\ntransform an untrustworthy model or prediction into a trustworthy one. In this\nwork, we propose LIME, a novel explanation technique that explains the\npredictions of any classifier in an interpretable and faithful manner, by\nlearning an interpretable model locally around the prediction. We also propose\na method to explain models by presenting representative individual predictions\nand their explanations in a non-redundant way, framing the task as a submodular\noptimization problem. We demonstrate the flexibility of these methods by\nexplaining different models for text (e.g. random forests) and image\nclassification (e.g. neural networks). We show the utility of explanations via\nnovel experiments, both simulated and with human subjects, on various scenarios\nthat require trust: deciding if one should trust a prediction, choosing between\nmodels, improving an untrustworthy classifier, and identifying why a classifier\nshould not be trusted.", "authors": ["Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1602.04938v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1602.04938v3", "num_discussion": 0, "originally_published_time": "2/16/2016", "pid": "1602.04938v3", "published_time": "8/9/2016", "rawpid": "1602.04938", "tags": ["cs.LG", "cs.AI", "stat.ML"], "title": "\"Why Should I Trust You?\": Explaining the Predictions of Any Classifier"}, {"abstract": "We introduce the value iteration network (VIN): a fully differentiable neural\nnetwork with a `planning module\u0027 embedded within. VINs can learn to plan, and\nare suitable for predicting outcomes that involve planning-based reasoning,\nsuch as policies for reinforcement learning. Key to our approach is a novel\ndifferentiable approximation of the value-iteration algorithm, which can be\nrepresented as a convolutional neural network, and trained end-to-end using\nstandard backpropagation. We evaluate VIN based policies on discrete and\ncontinuous path-planning domains, and on a natural-language based search task.\nWe show that by learning an explicit planning computation, VIN policies\ngeneralize better to new, unseen domains.", "authors": ["Aviv Tamar", "Yi Wu", "Garrett Thomas", "Sergey Levine", "Pieter Abbeel"], "category": "cs.AI", "comment": "Fixed missing table values", "img": "/static/thumbs/1602.02867v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1602.02867v4", "num_discussion": 0, "originally_published_time": "2/9/2016", "pid": "1602.02867v4", "published_time": "3/20/2017", "rawpid": "1602.02867", "tags": ["cs.AI", "cs.LG", "cs.NE", "stat.ML"], "title": "Value Iteration Networks"}, {"abstract": "Advances in artificial intelligence (AI) will transform modern life by\nreshaping transportation, health, science, finance, and the military. To adapt\npublic policy, we need to better anticipate these advances. Here we report the\nresults from a large survey of machine learning researchers on their beliefs\nabout progress in AI. Researchers predict AI will outperform humans in many\nactivities in the next ten years, such as translating languages (by 2024),\nwriting high-school essays (by 2026), driving a truck (by 2027), working in\nretail (by 2031), writing a bestselling book (by 2049), and working as a\nsurgeon (by 2053). Researchers believe there is a 50% chance of AI\noutperforming humans in all tasks in 45 years and of automating all human jobs\nin 120 years, with Asian respondents expecting these dates much sooner than\nNorth Americans. These results will inform discussion amongst researchers and\npolicymakers about anticipating and managing trends in AI.", "authors": ["Katja Grace", "John Salvatier", "Allan Dafoe", "Baobao Zhang", "Owain Evans"], "category": "cs.AI", "comment": "", "img": "/static/thumbs/1705.08807v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1705.08807v2", "num_discussion": 0, "originally_published_time": "5/24/2017", "pid": "1705.08807v2", "published_time": "5/30/2017", "rawpid": "1705.08807", "tags": ["cs.AI", "cs.CY"], "title": "When Will AI Exceed Human Performance? Evidence from AI Experts"}, {"abstract": "The dominant approach for many NLP tasks are recurrent neural networks, in\nparticular LSTMs, and convolutional neural networks. However, these\narchitectures are rather shallow in comparison to the deep convolutional\nnetworks which have pushed the state-of-the-art in computer vision. We present\na new architecture (VDCNN) for text processing which operates directly at the\ncharacter level and uses only small convolutions and pooling operations. We are\nable to show that the performance of this model increases with depth: using up\nto 29 convolutional layers, we report improvements over the state-of-the-art on\nseveral public text classification tasks. To the best of our knowledge, this is\nthe first time that very deep convolutional nets have been applied to text\nprocessing.", "authors": ["Alexis Conneau", "Holger Schwenk", "Lo\u00efc Barrault", "Yann Lecun"], "category": "cs.CL", "comment": "10 pages, EACL 2017, camera-ready", "img": "/static/thumbs/1606.01781v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1606.01781v2", "num_discussion": 0, "originally_published_time": "6/6/2016", "pid": "1606.01781v2", "published_time": "1/27/2017", "rawpid": "1606.01781", "tags": ["cs.CL", "cs.LG", "cs.NE"], "title": "Very Deep Convolutional Networks for Text Classification"}];
var pid_to_users = {};
var msg = "Top papers based on people&#39;s libraries:";
var render_format = "top";
var username = "";
var numresults = "16399";
var show_prompt = "no";

var urlq = ''; // global will be read in to QueryString when load is done

// when page loads...
$(document).ready(function(){

	urlq = QueryString.q;

  // display message, if any
  if(msg !== '') { d3.select("#rtable").append('div').classed('msg', true).html(msg); }

  // add papers to #rtable
	var done = addPapers(10, false);
  if(done) { $("#loadmorebtn").hide(); }

  // set up inifinite scrolling for adding more papers
  $(window).on('scroll', function(){
    var scrollTop = $(document).scrollTop();
    var windowHeight = $(window).height();
    var bodyHeight = $(document).height() - windowHeight;
    var scrollPercentage = (scrollTop / bodyHeight);
    if(scrollPercentage > 0.9) {
      var done = addPapers(5, true);
      if(done) { $("#loadmorebtn").hide(); }
    }
  });

  // just in case scrolling is broken somehow, provide a button handler explicit
  $("#loadmorebtn").on('click', function(){
    var done = addPapers(5, true);
    if(done) { $("#loadmorebtn").hide(); }
  });

  if(papers.length === 0) { $("#loadmorebtn").hide(); }

	if(!(typeof urlq == 'undefined')) {
		d3.select("#qfield").attr('value', urlq.replace(/\+/g, " "));
	}

  var vf = QueryString.vfilter; if(typeof vf === 'undefined') { vf = 'all'; }
  var tf = QueryString.timefilter; if(typeof tf === 'undefined') { tf = 'week'; }
  var link_endpoint = '/';
  if(render_format === 'recent') { link_endpoint = ''; }
  if(render_format === 'top') { link_endpoint = 'top'; }
  if(render_format === 'recommend') { link_endpoint = 'recommend'; }
  if(render_format === 'friends') { link_endpoint = 'friends'; }
  if(render_format === 'toptwtr') { link_endpoint = 'toptwtr'; }
  if(render_format === 'discussions') { link_endpoint = 'discussions'; }

  var time_ranges = ['day', '3days', 'week', 'month', 'year', 'alltime'];
  var time_txt = {'day':'Last day', '3days': 'Last 3 days', 'week': 'Last week', 'month': 'Last month', 'year': 'Last year', 'alltime': 'All time'}
  var time_range = tf;

  // set up time filtering options
  if(render_format === 'recommend' || render_format === 'top' || render_format === 'recent' || render_format === 'friends') {
    // insert version filtering options for these views
    var elt = d3.select('#recommend-time-choice');
    var vflink = vf === 'all' ? '1' : 'all'; // toggle only showing v1 or not
    if(render_format === 'recent') {
      var aelt = elt.append('a').attr('href', '/'+link_endpoint+'?'+'&vfilter='+vflink); // leave out timefilter from this page
    } else {
      var aelt = elt.append('a').attr('href', '/'+link_endpoint+'?'+'timefilter='+time_range+'&vfilter='+vflink);
    }
    var delt = aelt.append('div').classed('vchoice', true).html('Only show v1');
    if(vf === '1') { delt.classed('vchoice-selected', true); }
  }

  // time choices for recommend/top
  if(render_format === 'recommend' || render_format === 'top' || render_format === 'friends') {
    // insert time filtering options for these two views
    var elt = d3.select('#recommend-time-choice');
    elt.append('div').classed('fdivider', true).html('|');
    for(var i=0;i<time_ranges.length;i++) {
      var time_range = time_ranges[i];
      var aelt = elt.append('a').attr('href', '/'+link_endpoint+'?'+'timefilter='+time_range+'&vfilter='+vf);
      var delt = aelt.append('div').classed('timechoice', true).html(time_txt[time_range]);
      if(tf == time_range) { delt.classed('timechoice-selected', true); } // also render as chosen
    }
  }

  // time choices for top tweets
  if(render_format === 'toptwtr') {
    var tf = QueryString.timefilter; if(typeof tf === 'undefined') { tf = 'day'; } // default here is day
    var time_ranges = ['day', 'week', 'month'];
    var elt = d3.select('#recommend-time-choice');
    for(var i=0;i<time_ranges.length;i++) {
      var time_range = time_ranges[i];
      var aelt = elt.append('a').attr('href', '/'+link_endpoint+'?'+'timefilter='+time_range);
      var delt = aelt.append('div').classed('timechoice', true).html(time_txt[time_range]);
      if(tf == time_range) { delt.classed('timechoice-selected', true); } // also render as chosen
    }
  }

  var xb = $("#xbanner");
  if(xb.length !== 0) {
    xb.click(function(){ $("#banner").slideUp('fast'); })
  }

  // in top tab: color current choice
  if( render_format === 'recent') { d3.select('#tabrecent').classed('tab-selected', true); }
  if( render_format === 'top') { d3.select('#tabtop').classed('tab-selected', true); }
  if( render_format === 'toptwtr') { d3.select('#tabtwtr').classed('tab-selected', true); }
  if( render_format === 'friends') { d3.select('#tabfriends').classed('tab-selected', true); }
  if( render_format === 'discussions') { d3.select('#tabdiscussions').classed('tab-selected', true); }
  if( render_format === 'recommend') { d3.select('#tabrec').classed('tab-selected', true); }
  if( render_format === 'library') { d3.select('#tablib').classed('tab-selected', true); }

  $("#goaway").on('click', function(){
    $("#prompt").slideUp('fast');
    $.post("/goaway", {}).done(function(data){ });
  });
});

</script>
</head>

<body>
<a href="https://github.com/karpathy/arxiv-sanity-preserver"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://camo.githubusercontent.com/38ef81f8aca64bb9a64448d0d70f1308ef5341ab/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f72696768745f6461726b626c75655f3132313632312e706e67" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png"></a>

<div id ="titdiv">

  <!-- User account information on top right -->
  <div id="userinfo">
    
    <form action="/login" method="post">
      User:
      <input type="text" name="username" class="input-no-border">
      Pass:
      <input type="password" name="password" class="input-no-border">
      <input type="submit" value="Login or Create" class="btn-fancy">
    </form>
    
  </div>

  <!-- Site information/banner on top left -->
	<a href="/">
	<div id="tittxt">
		<h1>Arxiv Sanity Preserver</h1>
		Built in spare time by <a href="https://twitter.com/karpathy">@karpathy</a> to accelerate research.<br>
		Serving last 33342 papers from cs.[CV|CL|LG|AI|NE]/stat.ML
	</div>
	</a>
</div>

<div id="flashesdiv">

    

</div>



<div id="sbox">
  <form action="/search" method="get">
  	<input name="q" type="text" id="qfield">
  </form>
  <div id="search_hint"></div>
</div>



<div id="pagebar">
  <div class="pagelink" id="tabrecent"><a href="/">most recent</a></div>
  <div class="pagelink" id="tabtop"><a href="/top">top recent</a></div>
  <div class="pagelink" id="tabtwtr"><a href="/toptwtr">top hype</a></div>
  <div class="pagelink" id="tabfriends"><a href="/friends">friends</a></div>
  <div class="pagelink" id="tabdiscussions"><a href="/discussions">discussions</a></div>
  <div class="pagelink" id="tabrec"><a href="/recommend">recommended</a></div>
  <div class="pagelink" id="tablib"><a href="/library">library</a></div>
</div>

<!-- this div will be rendered into dynamcially at init with JS -->
<div id="recommend-time-choice" class="centerdiv"></div>

<div id="maindiv">

<div id="rtable"></div>

<div id="loadmore">
  <button id="loadmorebtn">Load more</button>
</div>

</div>

<br><br><br><br><br><br>
</body>

</html>